{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEBUG 3 - Copy of PyTorch/XLA ResNet18/CIFAR10 Training",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/DEBUG_3_Copy_of_PyTorch_XLA_ResNet18_CIFAR10_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1hxqUQn47M"
      },
      "source": [
        "## PyTorch/XLA ResNet18/CIFAR10 (GPU or TPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQPoJ6Fn8wF"
      },
      "source": [
        "### [RUNME] Install Colab compatible PyTorch/XLA wheels and dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59821ac-035f-4e76-f28b-fbae5f646f5b"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.7/dist-packages (0.10)\n",
            "Requirement already satisfied: torch-xla==1.8.1 from https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.30.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (56.1.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (20.9)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ALIOtsgSczi"
      },
      "source": [
        "#VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyG1Dv3XWAlq"
      },
      "source": [
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6H47dPVMAIJ"
      },
      "source": [
        "import gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHKXfPULVP1"
      },
      "source": [
        "def download(data, url):\n",
        "    # Download dataset\n",
        "    import zipfile\n",
        "    url = url\n",
        "    output = \"{}.zip\".format(data)\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Uncompress dataset\n",
        "    local_zip = '{}.zip'.format(data)\n",
        "    zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhpxrcKLX7W"
      },
      "source": [
        "data_samples = {\n",
        "    \"sample@200\": \"https://drive.google.com/uc?id=1FfV7YyDJvNUCDP5r3-8iQfZ2-xJp_pgb\",\n",
        "    \"sample@500\": \"https://drive.google.com/uc?id=1dHwUqpmSogEdjAB9rwDUL-OKFRUcVXte\",\n",
        "    \"sample@1000\": \"https://drive.google.com/uc?id=1DPZrHrj3Bdte5Dc6NCZ33CAqMG-Oipa2\",\n",
        "    \"sample@2000\": \"https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\",\n",
        "    \"sample@3000\": \"https://drive.google.com/uc?id=1_yre5K9YYvJgSrT4xvrI8eD_htucIywA\",\n",
        "    \"sample@4000_images\": \"https://drive.google.com/uc?id=1dqVB8EozEpwWzyuU80AauoQmsiw3Gtm2\",\n",
        "    \"sample@20000\": \"https://drive.google.com/uc?id=1MTDpLzpmhSiZq2jSdmHx2UDPn9FC8gzO\",\n",
        "    \"val-voets-tf\": \"https://drive.google.com/uc?id=1VzVgMGTkBBPG2qbzLunD9HvLzH6tcyrv\",\n",
        "    \"train_voets\": \"https://drive.google.com/uc?id=1AmcFh1MOOZ6aqKm2eO7XEdgmIEqHKTZ5\",\n",
        "    \"voets_test_images\": \"https://drive.google.com/uc?id=15S_V3B_Z3BOjCT3AbO2c887FyS5B0Lyd\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1J6gqYLZC7"
      },
      "source": [
        "UNLABELED = 'sample@1000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-BZSTOLaFm",
        "outputId": "5dd6a826-3c28-470f-892d-605d02ed1234"
      },
      "source": [
        "URL_UNLABELED = data_samples[UNLABELED]\n",
        "download(UNLABELED, URL_UNLABELED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DPZrHrj3Bdte5Dc6NCZ33CAqMG-Oipa2\n",
            "To: /content/sample@1000.zip\n",
            "108MB [00:00, 308MB/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IednejwkIW-K"
      },
      "source": [
        "Only run the below commented cell if you would like a nightly release"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFzLg5gy7l6"
      },
      "source": [
        "# PyTorch/XLA GPU Setup (only if GPU runtime)\n",
        "import os\n",
        "if os.environ.get('COLAB_GPU', '0') == '1':\n",
        "  os.environ['GPU_NUM_DEVICES'] = '1'\n",
        "  os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acr15gwFZXPK"
      },
      "source": [
        "import os\n",
        "os.environ['XLA_USE_BF16'] = \"1\"\n",
        "os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '10000000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5CwYW4ZbJA"
      },
      "source": [
        "!#python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rroH9yiAn-XE"
      },
      "source": [
        "### Define Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMojPWZUqr2s"
      },
      "source": [
        "# Result Visualization Helper\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "M, N = 4, 6\n",
        "RESULT_IMG_PATH = '/tmp/test_result.jpg'\n",
        "CIFAR10_LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def plot_results(images, labels, preds):\n",
        "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
        "  inv_norm = transforms.Normalize(\n",
        "      mean=(-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010),\n",
        "      std=(1/0.2023, 1/0.1994, 1/0.2010))\n",
        "\n",
        "  num_images = images.shape[0]\n",
        "  fig, axes = plt.subplots(M, N, figsize=(16, 9))\n",
        "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.axis('off')\n",
        "    if i >= num_images:\n",
        "      continue\n",
        "    img, label, prediction = images[i], labels[i], preds[i]\n",
        "    img = inv_norm(img)\n",
        "    img = img.permute(1, 2, 0) # (C, M, N) -> (M, N, C)\n",
        "    label, prediction = label.item(), prediction.item()\n",
        "    if label == prediction:\n",
        "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "    else:\n",
        "      ax.set_title(\n",
        "          'X {}/{}'.format(CIFAR10_LABELS[label],\n",
        "                          CIFAR10_LABELS[prediction]), color='red')\n",
        "    ax.imshow(img)\n",
        "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdPRFXIn_jH"
      },
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
        "FLAGS['batch_size'] = 4\n",
        "FLAGS['num_workers'] = 2\n",
        "FLAGS['learning_rate'] = 0.02\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 20\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd3xZvoA-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc6d8a5-a669-453c-e85f-34b70ff97521"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion * planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_planes,\n",
        "              self.expansion * planes,\n",
        "              kernel_size=1,\n",
        "              stride=stride,\n",
        "              bias=False), nn.BatchNorm2d(self.expansion * planes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, num_blocks, num_classes=2):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.linear(out)\n",
        "    return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "  return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.8.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msn_dk2b8WcM"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "969H-9W4-Lie"
      },
      "source": [
        "#device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwqD__ER8Q9c"
      },
      "source": [
        "#model = models.resnet50(pretrained=False, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EqiEWm6-BIP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "588f0a6d-c218-4c33-8919-c31e0a04b341"
      },
      "source": [
        "\"\"\"\n",
        "model = torchvision.models.resnet50(pretrained=False, num_classes=512).to(device)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(2048, 512),\n",
        "    nn.Linear(512, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel = torchvision.models.resnet50(pretrained=False, num_classes=512).to(device)\\nmodel.fc = nn.Sequential(\\n    nn.Linear(2048, 512),\\n    nn.Linear(512, 1),\\n    nn.Sigmoid()\\n)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh18kwNf86V2"
      },
      "source": [
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "normal_mean = (0.5, 0.5, 0.5)\n",
        "normal_std = (0.5, 0.5, 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf6iS50e8G5X"
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "# Only instantiate model weights once in memory.\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
        "#WRAPPED_MODEL = xmp.MpModelWrapper(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY4-Fd7QHjNc",
        "outputId": "d405c950-c811-4f5a-c004-2497d74cfbae"
      },
      "source": [
        "xm.xrt_world_size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rNzX92nYFss"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMl96KLoCq8"
      },
      "source": [
        "def train_resnet18():\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  norm = transforms.Normalize(\n",
        "    mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
        "\n",
        "  transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    norm,\n",
        "     ])\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(root=\"{}\".format(UNLABELED),\n",
        "    transform=transform_train)\n",
        "\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True)\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=FLAGS['batch_size'],\n",
        "        sampler=train_sampler,\n",
        "        num_workers=1,\n",
        "        drop_last=True)\n",
        "\n",
        "  # Scale learning rate to num cores\n",
        "  learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "  # Get loss function, optimizer, and model\n",
        "  device = xm.xla_device()\n",
        "  model = WRAPPED_MODEL.to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                        momentum=FLAGS['momentum'], weight_decay=5e-4)\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_loop_fn(loader):\n",
        "    tracker = xm.RateTracker()\n",
        "    model.train()\n",
        "    for x, (data, target) in enumerate(loader):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      xm.optimizer_step(optimizer)\n",
        "      tracker.add(FLAGS['batch_size'])\n",
        "      if x % FLAGS['log_steps'] == 0:\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
        "            tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "\n",
        "\n",
        "    accuracy = 0\n",
        "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "        xm.get_ordinal(), accuracy), flush=True)\n",
        "    return accuracy, data, pred, target\n",
        "\n",
        "  # Train and eval loops\n",
        "  accuracy = 0.0\n",
        "  data, pred, target = None, None, None\n",
        "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    train_loop_fn(para_loader.per_device_loader(device))\n",
        "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "\n",
        "  return accuracy, data, pred, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXIVNrvwMt_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4f72a13d-4da7-4a91-82f3-70361434e8b5"
      },
      "source": [
        "\"\"\"\n",
        "PILAS AQUI: \n",
        " - Parece que el problema que tenia era que 500 no era un numero que se distribuia en \n",
        " los 8 cores entonces no entrenada, pero no daba error, simplemente no funcionaban\n",
        " los dataloaders paralelos\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nPILAS AQUI: \\n - Parece que el problema que tenia era que 500 no era un numero que se distribuia en \\n los 8 cores entonces no entrenada, pero no daba error, simplemente no funcionaban\\n los dataloaders paralelos\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d133103b-6840-4514-d089-01cfce23bcbd"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    global FLAGS\n",
        "\n",
        "    FLAGS = flags \n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "    accuracy, data, pred, target = train_resnet18()\n",
        "    if rank == 0:\n",
        "        # Retrieve tensors that are on TPU core 0 and plot.\n",
        "        print(\"Hello\")\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[xla:0](0) Loss=0.84766 Rate=23.72 GlobalRate=23.72 Time=Sun May 30 23:38:33 2021\n",
            "[xla:5](0) Loss=0.82812 Rate=22.96 GlobalRate=22.96 Time=Sun May 30 23:38:37 2021\n",
            "[xla:3](0) Loss=0.84375 Rate=26.20 GlobalRate=26.20 Time=Sun May 30 23:38:37 2021\n",
            "[xla:4](0) Loss=0.82812 Rate=23.83 GlobalRate=23.83 Time=Sun May 30 23:38:41 2021\n",
            "[xla:6](0) Loss=0.81250 Rate=23.78 GlobalRate=23.78 Time=Sun May 30 23:38:42 2021\n",
            "[xla:7](0) Loss=0.86328 Rate=23.98 GlobalRate=23.98 Time=Sun May 30 23:38:42 2021\n",
            "[xla:1](0) Loss=0.86719 Rate=23.08 GlobalRate=23.08 Time=Sun May 30 23:38:43 2021\n",
            "[xla:2](0) Loss=0.82812 Rate=24.93 GlobalRate=24.93 Time=Sun May 30 23:38:43 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=109.13 GlobalRate=130.33 Time=Sun May 30 23:38:44 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=13.84 GlobalRate=7.50 Time=Sun May 30 23:38:44 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=18.15 GlobalRate=13.11 Time=Sun May 30 23:38:44 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=16.31 GlobalRate=12.16 Time=Sun May 30 23:38:44 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=28.72 GlobalRate=31.46 Time=Sun May 30 23:38:44 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=56.74 GlobalRate=70.97 Time=Sun May 30 23:38:44 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=41.07 GlobalRate=49.65 Time=Sun May 30 23:38:44 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=33.29 GlobalRate=38.41 Time=Sun May 30 23:38:44 2021\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "Finished training epoch 1\n",
            "[xla:2](0) Loss=0.00000 Rate=19.84 GlobalRate=19.84 Time=Sun May 30 23:38:44 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=18.94 GlobalRate=18.93 Time=Sun May 30 23:38:44 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=18.74 GlobalRate=18.74 Time=Sun May 30 23:38:44 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=17.95 GlobalRate=17.95 Time=Sun May 30 23:38:44 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=17.90 GlobalRate=17.90 Time=Sun May 30 23:38:44 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=17.91 GlobalRate=17.91 Time=Sun May 30 23:38:44 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=17.62 GlobalRate=17.62 Time=Sun May 30 23:38:44 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=17.34 GlobalRate=17.34 Time=Sun May 30 23:38:44 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=135.97 GlobalRate=143.09 Time=Sun May 30 23:38:44 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=138.49 GlobalRate=141.05 Time=Sun May 30 23:38:44 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=135.07 GlobalRate=140.31 Time=Sun May 30 23:38:44 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=136.37 GlobalRate=140.42 Time=Sun May 30 23:38:44 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=131.86 GlobalRate=142.63 Time=Sun May 30 23:38:44 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=129.75 GlobalRate=136.57 Time=Sun May 30 23:38:44 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=126.87 GlobalRate=136.89 Time=Sun May 30 23:38:44 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=129.50 GlobalRate=136.54 Time=Sun May 30 23:38:44 2021\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "Finished training epoch 2\n",
            "[xla:3](0) Loss=0.00000 Rate=21.05 GlobalRate=21.05 Time=Sun May 30 23:38:45 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=21.03 GlobalRate=21.02 Time=Sun May 30 23:38:45 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=19.68 GlobalRate=19.68 Time=Sun May 30 23:38:45 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=19.58 GlobalRate=19.58 Time=Sun May 30 23:38:45 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=18.95 GlobalRate=18.95 Time=Sun May 30 23:38:45 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=18.12 GlobalRate=18.12 Time=Sun May 30 23:38:45 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=17.03 GlobalRate=17.03 Time=Sun May 30 23:38:45 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=17.03 GlobalRate=17.03 Time=Sun May 30 23:38:45 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=155.78 GlobalRate=150.77 Time=Sun May 30 23:38:45 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=142.81 GlobalRate=150.28 Time=Sun May 30 23:38:45 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=154.83 GlobalRate=150.22 Time=Sun May 30 23:38:45 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=137.09 GlobalRate=149.13 Time=Sun May 30 23:38:45 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=147.48 GlobalRate=149.20 Time=Sun May 30 23:38:45 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=144.46 GlobalRate=149.53 Time=Sun May 30 23:38:45 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=138.99 GlobalRate=147.29 Time=Sun May 30 23:38:45 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=131.89 GlobalRate=145.13 Time=Sun May 30 23:38:45 2021\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "Finished training epoch 3\n",
            "[xla:0](0) Loss=0.00000 Rate=21.69 GlobalRate=21.69 Time=Sun May 30 23:38:46 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=20.26 GlobalRate=20.26 Time=Sun May 30 23:38:46 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=20.99 GlobalRate=20.99 Time=Sun May 30 23:38:46 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=20.16 GlobalRate=20.16 Time=Sun May 30 23:38:46 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=19.83 GlobalRate=19.83 Time=Sun May 30 23:38:46 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=19.84 GlobalRate=19.84 Time=Sun May 30 23:38:46 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=18.15 GlobalRate=18.15 Time=Sun May 30 23:38:46 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=18.15 GlobalRate=18.15 Time=Sun May 30 23:38:46 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=143.83 GlobalRate=151.38 Time=Sun May 30 23:38:46 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=142.10 GlobalRate=152.85 Time=Sun May 30 23:38:46 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=141.79 GlobalRate=150.71 Time=Sun May 30 23:38:46 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=149.54 GlobalRate=150.59 Time=Sun May 30 23:38:46 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=141.12 GlobalRate=150.46 Time=Sun May 30 23:38:46 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=137.55 GlobalRate=150.83 Time=Sun May 30 23:38:46 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=142.32 GlobalRate=150.31 Time=Sun May 30 23:38:46 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=148.13 GlobalRate=149.69 Time=Sun May 30 23:38:46 2021\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "Finished training epoch 4\n",
            "[xla:5](0) Loss=0.00000 Rate=20.29 GlobalRate=20.29 Time=Sun May 30 23:38:46 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=19.86 GlobalRate=19.86 Time=Sun May 30 23:38:46 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=19.72 GlobalRate=19.72 Time=Sun May 30 23:38:46 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=19.92 GlobalRate=19.92 Time=Sun May 30 23:38:46 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=19.78 GlobalRate=19.77 Time=Sun May 30 23:38:46 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=20.08 GlobalRate=20.08 Time=Sun May 30 23:38:46 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=19.77 GlobalRate=19.77 Time=Sun May 30 23:38:46 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=18.88 GlobalRate=18.88 Time=Sun May 30 23:38:46 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=141.77 GlobalRate=150.99 Time=Sun May 30 23:38:47 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=143.29 GlobalRate=151.07 Time=Sun May 30 23:38:47 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=146.26 GlobalRate=150.57 Time=Sun May 30 23:38:47 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=144.93 GlobalRate=152.76 Time=Sun May 30 23:38:47 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=143.01 GlobalRate=150.65 Time=Sun May 30 23:38:47 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=143.09 GlobalRate=151.07 Time=Sun May 30 23:38:47 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=146.10 GlobalRate=152.81 Time=Sun May 30 23:38:47 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=141.83 GlobalRate=149.69 Time=Sun May 30 23:38:47 2021\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "Finished training epoch 5\n",
            "[xla:3](0) Loss=0.00000 Rate=21.33 GlobalRate=21.33 Time=Sun May 30 23:38:47 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=20.93 GlobalRate=20.93 Time=Sun May 30 23:38:47 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=20.38 GlobalRate=20.38 Time=Sun May 30 23:38:47 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=21.23 GlobalRate=21.22 Time=Sun May 30 23:38:47 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=20.06 GlobalRate=20.06 Time=Sun May 30 23:38:47 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=20.04 GlobalRate=20.04 Time=Sun May 30 23:38:47 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=20.88 GlobalRate=20.88 Time=Sun May 30 23:38:47 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=19.75 GlobalRate=19.75 Time=Sun May 30 23:38:47 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=143.89 GlobalRate=154.70 Time=Sun May 30 23:38:47 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=140.63 GlobalRate=152.48 Time=Sun May 30 23:38:47 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=141.75 GlobalRate=152.45 Time=Sun May 30 23:38:47 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=144.89 GlobalRate=154.67 Time=Sun May 30 23:38:47 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=145.44 GlobalRate=152.30 Time=Sun May 30 23:38:47 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=143.92 GlobalRate=152.00 Time=Sun May 30 23:38:47 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=142.87 GlobalRate=152.02 Time=Sun May 30 23:38:47 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=144.14 GlobalRate=152.12 Time=Sun May 30 23:38:47 2021\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "Finished training epoch 6\n",
            "[xla:7](0) Loss=0.00000 Rate=20.59 GlobalRate=20.59 Time=Sun May 30 23:38:48 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=19.84 GlobalRate=19.84 Time=Sun May 30 23:38:48 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=19.42 GlobalRate=19.42 Time=Sun May 30 23:38:48 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=19.99 GlobalRate=19.99 Time=Sun May 30 23:38:48 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=19.95 GlobalRate=19.95 Time=Sun May 30 23:38:48 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=19.31 GlobalRate=19.31 Time=Sun May 30 23:38:48 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=19.27 GlobalRate=19.27 Time=Sun May 30 23:38:48 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=18.83 GlobalRate=18.83 Time=Sun May 30 23:38:48 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=145.98 GlobalRate=150.23 Time=Sun May 30 23:38:48 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=144.22 GlobalRate=150.22 Time=Sun May 30 23:38:48 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=142.54 GlobalRate=152.27 Time=Sun May 30 23:38:48 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=144.33 GlobalRate=152.01 Time=Sun May 30 23:38:48 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=144.11 GlobalRate=150.25 Time=Sun May 30 23:38:48 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=142.13 GlobalRate=150.18 Time=Sun May 30 23:38:48 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=142.00 GlobalRate=149.06 Time=Sun May 30 23:38:48 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=142.93 GlobalRate=151.12 Time=Sun May 30 23:38:48 2021\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "Finished training epoch 7\n",
            "[xla:1](0) Loss=0.00000 Rate=21.41 GlobalRate=21.41 Time=Sun May 30 23:38:48 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=21.02 GlobalRate=21.02 Time=Sun May 30 23:38:48 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=21.60 GlobalRate=21.60 Time=Sun May 30 23:38:48 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=20.47 GlobalRate=20.47 Time=Sun May 30 23:38:48 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=20.32 GlobalRate=20.32 Time=Sun May 30 23:38:48 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=20.23 GlobalRate=20.23 Time=Sun May 30 23:38:48 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=19.61 GlobalRate=19.61 Time=Sun May 30 23:38:48 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=20.32 GlobalRate=20.32 Time=Sun May 30 23:38:48 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=147.96 GlobalRate=155.29 Time=Sun May 30 23:38:49 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=144.11 GlobalRate=155.28 Time=Sun May 30 23:38:49 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=145.38 GlobalRate=155.35 Time=Sun May 30 23:38:49 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=150.40 GlobalRate=157.23 Time=Sun May 30 23:38:49 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=149.80 GlobalRate=154.95 Time=Sun May 30 23:38:49 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=146.75 GlobalRate=155.04 Time=Sun May 30 23:38:49 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=146.53 GlobalRate=157.50 Time=Sun May 30 23:38:49 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=146.85 GlobalRate=154.72 Time=Sun May 30 23:38:49 2021\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "Finished training epoch 8\n",
            "[xla:0](0) Loss=0.00000 Rate=22.27 GlobalRate=22.27 Time=Sun May 30 23:38:49 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=21.29 GlobalRate=21.29 Time=Sun May 30 23:38:49 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=21.28 GlobalRate=21.28 Time=Sun May 30 23:38:49 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=21.16 GlobalRate=21.16 Time=Sun May 30 23:38:49 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=20.97 GlobalRate=20.97 Time=Sun May 30 23:38:49 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=21.06 GlobalRate=21.05 Time=Sun May 30 23:38:49 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=20.74 GlobalRate=20.74 Time=Sun May 30 23:38:49 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=20.49 GlobalRate=20.49 Time=Sun May 30 23:38:49 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=147.06 GlobalRate=156.66 Time=Sun May 30 23:38:49 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=146.95 GlobalRate=156.37 Time=Sun May 30 23:38:49 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=146.51 GlobalRate=156.50 Time=Sun May 30 23:38:49 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=148.46 GlobalRate=156.28 Time=Sun May 30 23:38:49 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=146.13 GlobalRate=156.49 Time=Sun May 30 23:38:49 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=147.71 GlobalRate=156.38 Time=Sun May 30 23:38:49 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=143.08 GlobalRate=156.31 Time=Sun May 30 23:38:49 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=144.74 GlobalRate=155.48 Time=Sun May 30 23:38:49 2021\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "Finished training epoch 9\n",
            "[xla:6](0) Loss=0.00000 Rate=20.74 GlobalRate=20.74 Time=Sun May 30 23:38:50 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=20.64 GlobalRate=20.64 Time=Sun May 30 23:38:50 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=20.01 GlobalRate=20.01 Time=Sun May 30 23:38:50 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=20.06 GlobalRate=20.06 Time=Sun May 30 23:38:50 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=20.00 GlobalRate=20.00 Time=Sun May 30 23:38:50 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=19.92 GlobalRate=19.92 Time=Sun May 30 23:38:50 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=19.90 GlobalRate=19.90 Time=Sun May 30 23:38:50 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=19.78 GlobalRate=19.77 Time=Sun May 30 23:38:50 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=144.82 GlobalRate=151.93 Time=Sun May 30 23:38:50 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=143.65 GlobalRate=151.48 Time=Sun May 30 23:38:50 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=142.31 GlobalRate=152.20 Time=Sun May 30 23:38:50 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=143.67 GlobalRate=151.68 Time=Sun May 30 23:38:50 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=143.57 GlobalRate=151.62 Time=Sun May 30 23:38:50 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=144.53 GlobalRate=152.05 Time=Sun May 30 23:38:50 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=141.17 GlobalRate=151.59 Time=Sun May 30 23:38:50 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=143.41 GlobalRate=151.64 Time=Sun May 30 23:38:50 2021\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "Finished training epoch 10\n",
            "[xla:0](0) Loss=0.00000 Rate=21.47 GlobalRate=21.47 Time=Sun May 30 23:38:51 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=20.53 GlobalRate=20.53 Time=Sun May 30 23:38:51 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=20.48 GlobalRate=20.48 Time=Sun May 30 23:38:51 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=20.24 GlobalRate=20.24 Time=Sun May 30 23:38:51 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=19.91 GlobalRate=19.91 Time=Sun May 30 23:38:51 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=19.65 GlobalRate=19.65 Time=Sun May 30 23:38:51 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=19.56 GlobalRate=19.55 Time=Sun May 30 23:38:51 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=19.53 GlobalRate=19.52 Time=Sun May 30 23:38:51 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=147.69 GlobalRate=155.84 Time=Sun May 30 23:38:51 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=149.91 GlobalRate=155.83 Time=Sun May 30 23:38:51 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=150.99 GlobalRate=155.87 Time=Sun May 30 23:38:51 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=145.01 GlobalRate=156.09 Time=Sun May 30 23:38:51 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=151.11 GlobalRate=155.61 Time=Sun May 30 23:38:51 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=148.65 GlobalRate=155.81 Time=Sun May 30 23:38:51 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=147.17 GlobalRate=155.36 Time=Sun May 30 23:38:51 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=149.05 GlobalRate=154.29 Time=Sun May 30 23:38:51 2021\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "Finished training epoch 11\n",
            "[xla:6](0) Loss=0.00000 Rate=21.10 GlobalRate=21.10 Time=Sun May 30 23:38:51 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=19.88 GlobalRate=19.88 Time=Sun May 30 23:38:51 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=19.44 GlobalRate=19.44 Time=Sun May 30 23:38:51 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=19.35 GlobalRate=19.35 Time=Sun May 30 23:38:51 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=19.86 GlobalRate=19.86 Time=Sun May 30 23:38:51 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=19.10 GlobalRate=19.10 Time=Sun May 30 23:38:51 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=17.71 GlobalRate=17.71 Time=Sun May 30 23:38:51 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=18.22 GlobalRate=18.22 Time=Sun May 30 23:38:51 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=141.38 GlobalRate=148.44 Time=Sun May 30 23:38:52 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=136.02 GlobalRate=148.46 Time=Sun May 30 23:38:52 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=142.56 GlobalRate=150.54 Time=Sun May 30 23:38:52 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=139.56 GlobalRate=148.42 Time=Sun May 30 23:38:52 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=149.45 GlobalRate=150.77 Time=Sun May 30 23:38:52 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=147.64 GlobalRate=148.04 Time=Sun May 30 23:38:52 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=140.95 GlobalRate=148.36 Time=Sun May 30 23:38:52 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=140.78 GlobalRate=147.38 Time=Sun May 30 23:38:52 2021\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "Finished training epoch 12\n",
            "[xla:5](0) Loss=0.00000 Rate=21.24 GlobalRate=21.24 Time=Sun May 30 23:38:52 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=20.97 GlobalRate=20.97 Time=Sun May 30 23:38:52 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=20.03 GlobalRate=20.03 Time=Sun May 30 23:38:52 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=19.78 GlobalRate=19.77 Time=Sun May 30 23:38:52 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=19.89 GlobalRate=19.88 Time=Sun May 30 23:38:52 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=18.82 GlobalRate=18.82 Time=Sun May 30 23:38:52 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=18.33 GlobalRate=18.33 Time=Sun May 30 23:38:52 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=18.24 GlobalRate=18.24 Time=Sun May 30 23:38:52 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=146.13 GlobalRate=153.13 Time=Sun May 30 23:38:52 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=141.45 GlobalRate=152.90 Time=Sun May 30 23:38:52 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=146.10 GlobalRate=152.83 Time=Sun May 30 23:38:52 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=142.14 GlobalRate=152.82 Time=Sun May 30 23:38:52 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=152.27 GlobalRate=152.62 Time=Sun May 30 23:38:52 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=145.07 GlobalRate=152.76 Time=Sun May 30 23:38:52 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=151.91 GlobalRate=152.67 Time=Sun May 30 23:38:52 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=149.31 GlobalRate=152.42 Time=Sun May 30 23:38:52 2021\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "Finished training epoch 13\n",
            "[xla:3](0) Loss=0.00000 Rate=21.53 GlobalRate=21.52 Time=Sun May 30 23:38:53 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=21.19 GlobalRate=21.19 Time=Sun May 30 23:38:53 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=20.40 GlobalRate=20.40 Time=Sun May 30 23:38:53 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=20.18 GlobalRate=20.17 Time=Sun May 30 23:38:53 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=20.19 GlobalRate=20.18 Time=Sun May 30 23:38:53 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=20.01 GlobalRate=20.01 Time=Sun May 30 23:38:53 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=20.87 GlobalRate=20.87 Time=Sun May 30 23:38:53 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=19.85 GlobalRate=19.85 Time=Sun May 30 23:38:53 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=153.92 GlobalRate=159.30 Time=Sun May 30 23:38:53 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=153.28 GlobalRate=158.84 Time=Sun May 30 23:38:53 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=154.06 GlobalRate=161.21 Time=Sun May 30 23:38:53 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=154.41 GlobalRate=158.71 Time=Sun May 30 23:38:53 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=153.47 GlobalRate=158.52 Time=Sun May 30 23:38:53 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=151.71 GlobalRate=158.35 Time=Sun May 30 23:38:53 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=148.12 GlobalRate=158.52 Time=Sun May 30 23:38:53 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=149.19 GlobalRate=158.53 Time=Sun May 30 23:38:53 2021\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "Finished training epoch 14\n",
            "[xla:6](0) Loss=0.00000 Rate=22.15 GlobalRate=22.15 Time=Sun May 30 23:38:53 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=21.63 GlobalRate=21.63 Time=Sun May 30 23:38:53 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=21.10 GlobalRate=21.10 Time=Sun May 30 23:38:53 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=21.03 GlobalRate=21.03 Time=Sun May 30 23:38:53 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=19.91 GlobalRate=19.91 Time=Sun May 30 23:38:53 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=19.20 GlobalRate=19.20 Time=Sun May 30 23:38:53 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=18.90 GlobalRate=18.90 Time=Sun May 30 23:38:53 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=18.56 GlobalRate=18.56 Time=Sun May 30 23:38:53 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=137.16 GlobalRate=149.34 Time=Sun May 30 23:38:54 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=145.95 GlobalRate=149.45 Time=Sun May 30 23:38:54 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=137.50 GlobalRate=149.45 Time=Sun May 30 23:38:54 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=134.60 GlobalRate=149.37 Time=Sun May 30 23:38:54 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=141.14 GlobalRate=149.65 Time=Sun May 30 23:38:54 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=144.06 GlobalRate=149.12 Time=Sun May 30 23:38:54 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=142.90 GlobalRate=149.12 Time=Sun May 30 23:38:54 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=135.26 GlobalRate=148.92 Time=Sun May 30 23:38:54 2021\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "Finished training epoch 15\n",
            "[xla:6](0) Loss=0.00000 Rate=19.15 GlobalRate=19.14 Time=Sun May 30 23:38:54 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=19.02 GlobalRate=19.02 Time=Sun May 30 23:38:54 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=18.98 GlobalRate=18.97 Time=Sun May 30 23:38:54 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=18.82 GlobalRate=18.81 Time=Sun May 30 23:38:54 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=18.76 GlobalRate=18.76 Time=Sun May 30 23:38:54 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=18.21 GlobalRate=18.21 Time=Sun May 30 23:38:54 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=17.07 GlobalRate=17.07 Time=Sun May 30 23:38:54 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=16.27 GlobalRate=16.27 Time=Sun May 30 23:38:54 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=131.56 GlobalRate=140.86 Time=Sun May 30 23:38:54 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=132.19 GlobalRate=140.93 Time=Sun May 30 23:38:54 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=143.66 GlobalRate=140.97 Time=Sun May 30 23:38:54 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=132.08 GlobalRate=140.95 Time=Sun May 30 23:38:54 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=133.21 GlobalRate=141.16 Time=Sun May 30 23:38:54 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=134.76 GlobalRate=140.89 Time=Sun May 30 23:38:54 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=132.38 GlobalRate=140.69 Time=Sun May 30 23:38:54 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=140.95 GlobalRate=141.85 Time=Sun May 30 23:38:54 2021\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "Finished training epoch 16\n",
            "[xla:0](0) Loss=0.00000 Rate=20.00 GlobalRate=20.00 Time=Sun May 30 23:38:55 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=19.67 GlobalRate=19.67 Time=Sun May 30 23:38:55 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=18.95 GlobalRate=18.95 Time=Sun May 30 23:38:55 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=18.16 GlobalRate=18.16 Time=Sun May 30 23:38:55 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=18.01 GlobalRate=18.01 Time=Sun May 30 23:38:55 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=17.94 GlobalRate=17.94 Time=Sun May 30 23:38:55 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=17.58 GlobalRate=17.58 Time=Sun May 30 23:38:55 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=17.71 GlobalRate=17.71 Time=Sun May 30 23:38:55 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=148.73 GlobalRate=150.10 Time=Sun May 30 23:38:55 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=148.84 GlobalRate=152.49 Time=Sun May 30 23:38:55 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=149.74 GlobalRate=150.08 Time=Sun May 30 23:38:55 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=141.68 GlobalRate=150.25 Time=Sun May 30 23:38:55 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=142.58 GlobalRate=150.08 Time=Sun May 30 23:38:55 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=154.05 GlobalRate=152.04 Time=Sun May 30 23:38:55 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=151.28 GlobalRate=149.91 Time=Sun May 30 23:38:55 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=149.02 GlobalRate=149.86 Time=Sun May 30 23:38:55 2021\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "Finished training epoch 17\n",
            "[xla:0](0) Loss=0.00000 Rate=20.27 GlobalRate=20.27 Time=Sun May 30 23:38:56 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=19.58 GlobalRate=19.58 Time=Sun May 30 23:38:56 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=19.43 GlobalRate=19.42 Time=Sun May 30 23:38:56 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=18.96 GlobalRate=18.96 Time=Sun May 30 23:38:56 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=18.93 GlobalRate=18.93 Time=Sun May 30 23:38:56 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=18.80 GlobalRate=18.79 Time=Sun May 30 23:38:56 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=17.99 GlobalRate=17.99 Time=Sun May 30 23:38:56 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=17.95 GlobalRate=17.95 Time=Sun May 30 23:38:56 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=146.28 GlobalRate=150.78 Time=Sun May 30 23:38:56 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=150.56 GlobalRate=150.64 Time=Sun May 30 23:38:56 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=150.12 GlobalRate=152.89 Time=Sun May 30 23:38:56 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=146.12 GlobalRate=150.61 Time=Sun May 30 23:38:56 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=143.94 GlobalRate=150.42 Time=Sun May 30 23:38:56 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=143.77 GlobalRate=150.71 Time=Sun May 30 23:38:56 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=141.63 GlobalRate=150.86 Time=Sun May 30 23:38:56 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=148.67 GlobalRate=149.55 Time=Sun May 30 23:38:56 2021\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "Finished training epoch 18\n",
            "[xla:6](0) Loss=0.00000 Rate=21.08 GlobalRate=21.08 Time=Sun May 30 23:38:56 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=20.71 GlobalRate=20.71 Time=Sun May 30 23:38:56 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=20.50 GlobalRate=20.50 Time=Sun May 30 23:38:56 2021\n",
            "[xla:3](0) Loss=0.00000 Rate=20.20 GlobalRate=20.20 Time=Sun May 30 23:38:56 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=17.84 GlobalRate=17.84 Time=Sun May 30 23:38:56 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=17.88 GlobalRate=17.88 Time=Sun May 30 23:38:56 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=17.84 GlobalRate=17.84 Time=Sun May 30 23:38:56 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=17.51 GlobalRate=17.51 Time=Sun May 30 23:38:56 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=135.11 GlobalRate=140.17 Time=Sun May 30 23:38:57 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=134.96 GlobalRate=140.17 Time=Sun May 30 23:38:57 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=127.53 GlobalRate=140.02 Time=Sun May 30 23:38:57 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=136.47 GlobalRate=140.18 Time=Sun May 30 23:38:57 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=135.41 GlobalRate=140.36 Time=Sun May 30 23:38:57 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=125.39 GlobalRate=139.96 Time=Sun May 30 23:38:57 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=126.14 GlobalRate=139.91 Time=Sun May 30 23:38:57 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=126.62 GlobalRate=139.89 Time=Sun May 30 23:38:57 2021\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "Finished training epoch 19\n",
            "[xla:3](0) Loss=0.00000 Rate=21.59 GlobalRate=21.59 Time=Sun May 30 23:38:57 2021\n",
            "[xla:0](0) Loss=0.00000 Rate=22.23 GlobalRate=22.23 Time=Sun May 30 23:38:57 2021\n",
            "[xla:5](0) Loss=0.00000 Rate=20.68 GlobalRate=20.68 Time=Sun May 30 23:38:57 2021\n",
            "[xla:2](0) Loss=0.00000 Rate=19.77 GlobalRate=19.77 Time=Sun May 30 23:38:57 2021\n",
            "[xla:7](0) Loss=0.00000 Rate=19.57 GlobalRate=19.57 Time=Sun May 30 23:38:57 2021\n",
            "[xla:6](0) Loss=0.00000 Rate=19.54 GlobalRate=19.54 Time=Sun May 30 23:38:57 2021\n",
            "[xla:1](0) Loss=0.00000 Rate=19.35 GlobalRate=19.35 Time=Sun May 30 23:38:57 2021\n",
            "[xla:4](0) Loss=0.00000 Rate=18.71 GlobalRate=18.71 Time=Sun May 30 23:38:57 2021\n",
            "[xla:6](20) Loss=0.00000 Rate=145.77 GlobalRate=151.99 Time=Sun May 30 23:38:57 2021\n",
            "[xla:7](20) Loss=0.00000 Rate=145.69 GlobalRate=152.02 Time=Sun May 30 23:38:57 2021\n",
            "[xla:5](20) Loss=0.00000 Rate=141.95 GlobalRate=152.04 Time=Sun May 30 23:38:57 2021\n",
            "[xla:4](20) Loss=0.00000 Rate=149.06 GlobalRate=151.93 Time=Sun May 30 23:38:57 2021\n",
            "[xla:1](20) Loss=0.00000 Rate=146.55 GlobalRate=152.03 Time=Sun May 30 23:38:57 2021\n",
            "[xla:3](20) Loss=0.00000 Rate=139.12 GlobalRate=151.86 Time=Sun May 30 23:38:57 2021\n",
            "[xla:0](20) Loss=0.00000 Rate=140.26 GlobalRate=154.03 Time=Sun May 30 23:38:57 2021\n",
            "[xla:2](20) Loss=0.00000 Rate=144.65 GlobalRate=151.79 Time=Sun May 30 23:38:57 2021\n",
            "[xla:7] Accuracy=0.00%\n",
            "[xla:2] Accuracy=0.00%\n",
            "[xla:4] Accuracy=0.00%\n",
            "[xla:0] Accuracy=0.00%\n",
            "[xla:1] Accuracy=0.00%\n",
            "[xla:6] Accuracy=0.00%\n",
            "[xla:5] Accuracy=0.00%\n",
            "[xla:3] Accuracy=0.00%\n",
            "Finished training epoch 20\n",
            "Hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-XHdIq2VLsF",
        "outputId": "5cccfd68-6a05-4bc0-b336-3b3e05cdfd6a"
      },
      "source": [
        "!cat /proc/cpuinfo | grep processor | wc -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ9LZSH8U-oh",
        "outputId": "afceaf5a-5b8f-4faf-c486-71e5237819c6"
      },
      "source": [
        "!free -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            35G        1.2G         26G        1.2M        7.9G         33G\n",
            "Swap:            0B          0B          0B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h1fEcGkU8VM",
        "outputId": "7f41e32a-cb8e-494b-b3d4-103fd057edc5"
      },
      "source": [
        "!free -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            35G        1.2G         26G        1.2M        7.9G         33G\n",
            "Swap:            0B          0B          0B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wt7wEVJoFmf"
      },
      "source": [
        "## Visualize Predictions"
      ]
    }
  ]
}