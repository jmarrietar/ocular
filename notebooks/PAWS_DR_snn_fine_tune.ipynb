{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PAWS_DR_snn_fine_tune.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOilFu2fPzt9tfU0WUyxQwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/PAWS_DR_snn_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV1_C0uBTHea",
        "outputId": "95c30184-0a27-411a-ec3f-92f91f4cdfff"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install --quiet -v --no-cache-dir ./"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8054, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 8054 (delta 68), reused 101 (delta 47), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8054/8054), 14.11 MiB | 28.51 MiB/s, done.\n",
            "Resolving deltas: 100% (5467/5467), done.\n",
            "/content/apex\n",
            "Processing /content/apex\n",
            "Building wheels for collected packages: apex\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=204709 sha256=c684f6bd7272b58b14dd3235a9bdd0907e4c72c58cf13d65e2e91af2d908f14a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gd93qwcv/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl-vU4L_WX5C",
        "outputId": "ca3d90d8-8cf9-4551-bcb1-80d29c9105ae"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soM7NoBHWdQh",
        "outputId": "9d6109d2-3821-4827-d531-5e6adcce439c"
      },
      "source": [
        "!pip install --quiet -U PyYAML"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 10.6MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 6.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 6.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 225kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 307kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 368kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 419kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 450kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 481kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 501kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 532kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 563kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 593kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 614kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 6.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5qfDJJtWm6w",
        "outputId": "d69a5ef3-68e8-49c0-9a77-b55e0afc0982"
      },
      "source": [
        "!git clone -b feature/DR-images-v2 https://github.com/jmarrietar/suncet.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'suncet'...\n",
            "remote: Enumerating objects: 247, done.\u001b[K\n",
            "remote: Counting objects: 100% (247/247), done.\u001b[K\n",
            "remote: Compressing objects: 100% (155/155), done.\u001b[K\n",
            "remote: Total 247 (delta 140), reused 190 (delta 89), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (247/247), 1.10 MiB | 14.76 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgpjSUqqWrsD",
        "outputId": "7d812ddb-fcd8-40b5-8dbd-0e421db324f4"
      },
      "source": [
        "cd suncet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/suncet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fblle7-IW10Q",
        "outputId": "a79637c8-886f-4e68-f198-8c4f2451d5b9"
      },
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Check whether the file is already in the desired path or if it needs to be downloaded\n",
        "# File downloaded from source : https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
        "\n",
        "base_path = '/content/suncet/datasets/dr/'\n",
        "file_path = 'sample@2000.zip'\n",
        "\n",
        "if not os.path.isfile(base_path + file_path):\n",
        "    subprocess.run(['mkdir', '-p', base_path])\n",
        "    subprocess.run(['mkdir', '-p', 'logs'])\n",
        "    subprocess.call(['python', 'download.py', '-d', file_path.split('.')[0]])\n",
        "else:\n",
        "    print('File already downloaded!')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File already downloaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgTyanFiZkjp"
      },
      "source": [
        "###################################\n",
        "########### DEBUG #################\n",
        "###################################"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n43wHu3VZll0"
      },
      "source": [
        "`main.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSWpicZZnOQ"
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "import pprint\n",
        "import yaml\n",
        "\n",
        "from src.paws_train import main as paws\n",
        "from src.suncet_train import main as suncet\n",
        "from src.fine_tune import main as fine_tune\n",
        "from src.snn_fine_tune import main as snn_fine_tune\n",
        "\n",
        "from src.utils import init_distributed\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--fname\", type=str, help=\"name of config file to load\", default=\"configs.yaml\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--devices\",\n",
        "    type=str,\n",
        "    nargs=\"+\",\n",
        "    default=[\"cuda:0\"],\n",
        "    help=\"which devices to use on local machine\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--sel\",\n",
        "    type=str,\n",
        "    help=\"which script to run\",\n",
        "    choices=[\"paws_train\", \"suncet_train\", \"fine_tune\", \"snn_fine_tune\"],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VchIpoj2asUE"
      },
      "source": [
        "args = parser.parse_args(['--sel', 'snn_fine_tune',\n",
        "                          '--fname', 'configs/paws/dr_snn.yaml'\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwx-W1YYashV"
      },
      "source": [
        "fname = args.fname\n",
        "sel = args.sel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCqcTFjWasoZ"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "\n",
        "logger.info(f'called-params {sel} {fname}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1PMAId8asvo"
      },
      "source": [
        "# -- load script params\n",
        "params = None\n",
        "with open(fname, 'r') as y_file:\n",
        "    params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
        "    logger.info('loaded params...')\n",
        "    if rank == 0:\n",
        "        pp = pprint.PrettyPrinter(indent=4)\n",
        "        pp.pprint(params)\n",
        "\n",
        "if rank == 0:\n",
        "    dump = os.path.join(params['logging']['folder'], f'params-{sel}.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(params, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3GrQQcas4p"
      },
      "source": [
        "args = params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-tY973TdTpX"
      },
      "source": [
        "# FINE TUNE TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuRVjbmedzQF"
      },
      "source": [
        "`snn_fine_tune.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWbSYOpGd4AG"
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import logging\n",
        "import contextlib\n",
        "import io\n",
        "import sys\n",
        "import copy\n",
        "\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import src.resnet as resnet\n",
        "import src.wide_resnet as wide_resnet\n",
        "from src.utils import (\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule\n",
        ")\n",
        "from src.losses import (\n",
        "    init_suncet_loss,\n",
        "    make_labels_matrix\n",
        ")\n",
        "from src.data_manager import (\n",
        "    init_data,\n",
        "    make_transforms\n",
        ")\n",
        "\n",
        "from src.sgd import SGD\n",
        "from src.lars import LARS\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from snn_eval import main as val_run\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhLWc51Ce7QU"
      },
      "source": [
        "def load_pretrained(\n",
        "    r_path,\n",
        "    encoder,\n",
        "    device,\n",
        "    ckp=False\n",
        "):\n",
        "    checkpoint = torch.load(r_path, map_location=device)\n",
        "    if ckp:\n",
        "        pretrained_dict = {k: v for k, v in checkpoint['encoder'].items()}\n",
        "    else:\n",
        "        pretrained_dict = {k.replace('module.', ''): v for k, v in checkpoint['encoder'].items()}\n",
        "    for k, v in encoder.state_dict().items():\n",
        "        if k not in pretrained_dict:\n",
        "            logger.info(f'key \"{k}\" could not be found in loaded state dict')\n",
        "        elif pretrained_dict[k].shape != v.shape:\n",
        "            logger.info(f'key \"{k}\" is of different shape in model and loaded state dict')\n",
        "            pretrained_dict[k] = v\n",
        "    msg = encoder.load_state_dict(pretrained_dict, strict=False)\n",
        "    logger.info(f'loaded pretrained model with msg: {msg}')\n",
        "    logger.info(f'loaded pretrained encoder from epoch: {checkpoint[\"epoch\"]} '\n",
        "                f'path: {r_path}')\n",
        "    del checkpoint\n",
        "    return encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wv30voke9Th"
      },
      "source": [
        "def load_from_path(\n",
        "    r_path,\n",
        "    encoder,\n",
        "    opt,\n",
        "    sched,\n",
        "    scaler,\n",
        "    device,\n",
        "    use_fp16=False,\n",
        "    ckp=False\n",
        "):\n",
        "    encoder = load_pretrained(r_path, encoder, device, ckp)\n",
        "    checkpoint = torch.load(r_path, map_location=device)\n",
        "    epoch = checkpoint['epoch']\n",
        "    best_acc = None\n",
        "    if 'best_top1_acc' in checkpoint:\n",
        "        best_acc = checkpoint['best_top1_acc']\n",
        "    if opt is not None:\n",
        "        if use_fp16:\n",
        "            scaler.load_state_dict(checkpoint['amp'])\n",
        "        opt.load_state_dict(checkpoint['opt'])\n",
        "        sched.load_state_dict(checkpoint['sched'])\n",
        "        logger.info(f'loaded optimizers from epoch {epoch}')\n",
        "    logger.info(f'read-path: {r_path}')\n",
        "    del checkpoint\n",
        "    return encoder, opt, scaler, sched, epoch, best_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lKkvECRdWUl"
      },
      "source": [
        "def init_model(\n",
        "    device,\n",
        "    training,\n",
        "    use_fp16,\n",
        "    r_enc_path,\n",
        "    iterations_per_epoch,\n",
        "    world_size,\n",
        "    start_lr,\n",
        "    ref_lr,\n",
        "    num_epochs,\n",
        "    output_dim=128,\n",
        "    model_name='resnet50',\n",
        "    warmup_epochs=0,\n",
        "    use_pred_head=False,\n",
        "    use_lars=False,\n",
        "    wd=1e-6,\n",
        "    final_lr=0.,\n",
        "    momentum=0.9,\n",
        "    nesterov=False\n",
        "):\n",
        "    if 'wide_resnet' in model_name:\n",
        "        encoder = wide_resnet.__dict__[model_name](dropout_rate=0.0)\n",
        "        hidden_dim = 128\n",
        "    else:\n",
        "        encoder = resnet.__dict__[model_name]()\n",
        "        hidden_dim = 2048\n",
        "        if 'w2' in model_name:\n",
        "            hidden_dim *= 2\n",
        "        elif 'w4' in model_name:\n",
        "            hidden_dim *= 4\n",
        "\n",
        "    # -- projection head\n",
        "    encoder.fc = torch.nn.Sequential(OrderedDict([\n",
        "        ('fc1', torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "        ('bn1', torch.nn.BatchNorm1d(hidden_dim)),\n",
        "        ('relu1', torch.nn.ReLU(inplace=True)),\n",
        "        ('fc2', torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "        ('bn2', torch.nn.BatchNorm1d(hidden_dim)),\n",
        "        ('relu2', torch.nn.ReLU(inplace=True)),\n",
        "        ('fc3', torch.nn.Linear(hidden_dim, output_dim))\n",
        "    ]))\n",
        "\n",
        "    # -- prediction head\n",
        "    encoder.pred = None\n",
        "    if use_pred_head:\n",
        "        mx = 4  # 4x bottleneck prediction head\n",
        "        pred_head = OrderedDict([])\n",
        "        pred_head['bn1'] = torch.nn.BatchNorm1d(output_dim)\n",
        "        pred_head['fc1'] = torch.nn.Linear(output_dim, output_dim//mx)\n",
        "        pred_head['bn2'] = torch.nn.BatchNorm1d(output_dim//mx)\n",
        "        pred_head['relu'] = torch.nn.ReLU(inplace=True)\n",
        "        pred_head['fc2'] = torch.nn.Linear(output_dim//mx, output_dim)\n",
        "        encoder.pred = torch.nn.Sequential(pred_head)\n",
        "\n",
        "    for m in encoder.modules():\n",
        "        if isinstance(m, torch.nn.BatchNorm1d) or isinstance(m, torch.nn.BatchNorm2d):\n",
        "            m.eval()\n",
        "\n",
        "    encoder.to(device)\n",
        "    encoder = load_pretrained(\n",
        "        r_path=r_enc_path,\n",
        "        encoder=encoder,\n",
        "        device=device)\n",
        "\n",
        "    # -- init optimizer\n",
        "    optimizer, scheduler = None, None\n",
        "    if training:\n",
        "        param_groups = [\n",
        "            {'params': (p for n, p in encoder.named_parameters()\n",
        "                        if ('bias' not in n) and ('bn' not in n))},\n",
        "            {'params': (p for n, p in encoder.named_parameters()\n",
        "                        if ('bias' in n) or ('bn' in n)),\n",
        "                'LARS_exclude': True,\n",
        "                'weight_decay': 0}\n",
        "        ]\n",
        "        optimizer = SGD(\n",
        "            param_groups,\n",
        "            momentum=momentum,\n",
        "            nesterov=nesterov,\n",
        "            weight_decay=wd,\n",
        "            lr=ref_lr)\n",
        "        scheduler = WarmupCosineSchedule(\n",
        "            optimizer,\n",
        "            warmup_steps=warmup_epochs*iterations_per_epoch,\n",
        "            start_lr=start_lr,\n",
        "            ref_lr=ref_lr,\n",
        "            final_lr=final_lr,\n",
        "            T_max=num_epochs*iterations_per_epoch)\n",
        "        if use_lars:\n",
        "            optimizer = LARS(optimizer, trust_coefficient=0.001)\n",
        "    if world_size > 1:\n",
        "        encoder = DistributedDataParallel(encoder)\n",
        "\n",
        "    return encoder, optimizer, scheduler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGO_VgrMfB6k"
      },
      "source": [
        "@contextlib.contextmanager\n",
        "def nostdout():\n",
        "    logger.disabled = True\n",
        "    save_stdout = sys.stdout\n",
        "    sys.stdout = io.BytesIO()\n",
        "    yield\n",
        "    sys.stdout = save_stdout\n",
        "    logger.disabled = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARlTURmadlgD"
      },
      "source": [
        "\n",
        "# -- META\n",
        "model_name = args['meta']['model_name']\n",
        "load_checkpoint = args['meta']['load_checkpoint']\n",
        "copy_data = args['meta']['copy_data']\n",
        "output_dim = args['meta']['output_dim']\n",
        "use_pred_head = args['meta']['use_pred_head']\n",
        "use_fp16 = args['meta']['use_fp16']\n",
        "device = torch.device(args['meta']['device'])\n",
        "torch.cuda.set_device(device)\n",
        "\n",
        "# -- DATA\n",
        "unlabeled_frac = args['data']['unlabeled_frac']\n",
        "label_smoothing = args['data']['label_smoothing']\n",
        "normalize = args['data']['normalize']\n",
        "root_path = args['data']['root_path']\n",
        "image_folder = args['data']['image_folder']\n",
        "dataset_name = args['data']['dataset']\n",
        "subset_path = args['data']['subset_path']\n",
        "unique_classes = args['data']['unique_classes_per_rank']\n",
        "data_seed = args['data']['data_seed']\n",
        "\n",
        "# -- CRITERTION\n",
        "classes_per_batch = args['criterion']['classes_per_batch']\n",
        "supervised_views = args['criterion']['supervised_views']\n",
        "batch_size = args['criterion']['supervised_batch_size']\n",
        "temperature = args['criterion']['temperature']\n",
        "\n",
        "# -- OPTIMIZATION\n",
        "wd = float(args['optimization']['weight_decay'])\n",
        "num_epochs = args['optimization']['epochs']\n",
        "use_lars = args['optimization']['use_lars']\n",
        "warmup = args['optimization']['warmup']\n",
        "start_lr = args['optimization']['start_lr']\n",
        "ref_lr = args['optimization']['lr']\n",
        "final_lr = args['optimization']['final_lr']\n",
        "momentum = args['optimization']['momentum']\n",
        "nesterov = args['optimization']['nesterov']\n",
        "\n",
        "# -- LOGGING\n",
        "folder = args['logging']['folder']\n",
        "tag = args['logging']['write_tag']\n",
        "r_file_enc = args['logging']['pretrain_path']\n",
        "\n",
        "# -- log/checkpointing paths\n",
        "r_enc_path = os.path.join(folder, r_file_enc)\n",
        "w_enc_path = os.path.join(folder, f'{tag}-fine-tune-SNN.pth.tar')\n",
        "\n",
        "# -- init distributed\n",
        "world_size, rank = init_distributed()\n",
        "logger.info(f'initialized rank/world-size: {rank}/{world_size}')\n",
        "\n",
        "# -- init loss\n",
        "suncet = init_suncet_loss(\n",
        "    num_classes=classes_per_batch,\n",
        "    batch_size=batch_size*supervised_views,\n",
        "    world_size=world_size,\n",
        "    rank=rank,\n",
        "    temperature=temperature,\n",
        "    device=device)\n",
        "labels_matrix = make_labels_matrix(\n",
        "    num_classes=classes_per_batch,\n",
        "    s_batch_size=batch_size,\n",
        "    world_size=world_size,\n",
        "    device=device,\n",
        "    unique_classes=unique_classes,\n",
        "    smoothing=label_smoothing)\n",
        "\n",
        "# -- make data transforms\n",
        "transform, init_transform = make_transforms(\n",
        "    dataset_name=dataset_name,\n",
        "    subset_path=subset_path,\n",
        "    unlabeled_frac=unlabeled_frac,\n",
        "    training=True,\n",
        "    split_seed=data_seed,\n",
        "    basic_augmentations=True,\n",
        "    normalize=normalize)\n",
        "(data_loader,\n",
        "    dist_sampler) = init_data(\n",
        "        dataset_name=dataset_name,\n",
        "        transform=transform,\n",
        "        init_transform=init_transform,\n",
        "        supervised_views=supervised_views,\n",
        "        u_batch_size=None,\n",
        "        stratify=True,\n",
        "        s_batch_size=batch_size,\n",
        "        classes_per_batch=classes_per_batch,\n",
        "        unique_classes=unique_classes,\n",
        "        world_size=world_size,\n",
        "        rank=rank,\n",
        "        root_path=root_path,\n",
        "        image_folder=image_folder,\n",
        "        training=True,\n",
        "        copy_data=copy_data)\n",
        "\n",
        "# -- rough estimate of labeled imgs per class used to set the number of\n",
        "#    fine-tuning iterations\n",
        "imgs_per_class = int(1300*(1.-unlabeled_frac)) if 'imagenet' in dataset_name else int(5000*(1.-unlabeled_frac))\n",
        "dist_sampler.set_inner_epochs(imgs_per_class//batch_size)\n",
        "\n",
        "ipe = len(data_loader)\n",
        "logger.info(f'initialized data-loader (ipe {ipe})')\n",
        "\n",
        "# -- init model and optimizer\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "encoder, optimizer, scheduler = init_model(\n",
        "    device=device,\n",
        "    training=True,\n",
        "    r_enc_path=r_enc_path,\n",
        "    iterations_per_epoch=ipe,\n",
        "    world_size=world_size,\n",
        "    start_lr=start_lr,\n",
        "    ref_lr=ref_lr,\n",
        "    num_epochs=num_epochs,\n",
        "    output_dim=output_dim,\n",
        "    model_name=model_name,\n",
        "    warmup_epochs=warmup,\n",
        "    use_pred_head=use_pred_head,\n",
        "    use_fp16=use_fp16,\n",
        "    wd=wd,\n",
        "    final_lr=final_lr,\n",
        "    momentum=momentum,\n",
        "    nesterov=nesterov,\n",
        "    use_lars=use_lars)\n",
        "\n",
        "best_acc, val_top1 = None, None\n",
        "start_epoch = 0\n",
        "# -- load checkpoint\n",
        "if load_checkpoint:\n",
        "    encoder, optimizer, scaler, scheduler, start_epoch, best_acc = load_from_path(\n",
        "        r_path=w_enc_path,\n",
        "        encoder=encoder,\n",
        "        opt=optimizer,\n",
        "        scaler=scaler,\n",
        "        sched=scheduler,\n",
        "        device=device,\n",
        "        use_fp16=use_fp16,\n",
        "        ckp=True)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "    def train_step():\n",
        "        # -- update distributed-data-loader epoch\n",
        "        dist_sampler.set_epoch(epoch)\n",
        "\n",
        "        for i, data in enumerate(data_loader):\n",
        "            imgs = torch.cat([s.to(device) for s in data[:-1]], 0)\n",
        "            labels = torch.cat([labels_matrix for _ in range(supervised_views)])\n",
        "            with torch.cuda.amp.autocast(enabled=use_fp16):\n",
        "                optimizer.zero_grad()\n",
        "                z = encoder(imgs)\n",
        "                loss = suncet(z, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            if i % log_freq == 0:\n",
        "                logger.info('[%d, %5d] (loss: %.3f)' % (epoch + 1, i, loss))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with nostdout():\n",
        "            val_top1, _ = val_run(\n",
        "                pretrained=copy.deepcopy(encoder),\n",
        "                subset_path=subset_path,\n",
        "                unlabeled_frac=unlabeled_frac,\n",
        "                dataset_name=dataset_name,\n",
        "                root_path=root_path,\n",
        "                image_folder=image_folder,\n",
        "                use_pred=use_pred_head,\n",
        "                normalize=normalize,\n",
        "                split_seed=data_seed)\n",
        "    logger.info('[%d] (val: %.3f%%)' % (epoch + 1, val_top1))\n",
        "    train_step()\n",
        "\n",
        "    # -- logging/checkpointing\n",
        "    if (rank == 0) and ((best_acc is None) or (best_acc < val_top1)):\n",
        "        best_acc = val_top1\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'sched': scheduler.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'unlabel_prob': unlabeled_frac,\n",
        "            'world_size': world_size,\n",
        "            'batch_size': batch_size,\n",
        "            'best_top1_acc': best_acc,\n",
        "            'lr': ref_lr,\n",
        "            'amp': scaler.state_dict()\n",
        "        }\n",
        "        torch.save(save_dict, w_enc_path)\n",
        "\n",
        "logger.info('[%d] (best-val: %.3f%%)' % (epoch + 1, best_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7F8gQsqdl6G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sKu2idwdmBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a81dLi_RdmJS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emCH0rLvdmQm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}