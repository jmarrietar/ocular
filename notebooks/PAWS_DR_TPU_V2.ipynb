{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PAWS-DR-TPU-V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOykTjluZzrb1AEp1Da/dKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/PAWS_DR_TPU_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxN8OFauZt1a"
      },
      "source": [
        "from google.colab import auth, drive\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYzFY6t7a951",
        "outputId": "5fedcf7c-2c66-4997-ae55-e7ff6725a126"
      },
      "source": [
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7__oQqEeGc1m",
        "outputId": "163f7817-7736-4186-d3de-7e3dfe30d959"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-xla==1.8.1\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl (145.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 145.0 MB 43 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.32.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.2.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.272 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhTNyuKBIojX",
        "outputId": "ad532bcb-7f5d-40a7-b99b-4df5420ec443"
      },
      "source": [
        "!pip uninstall torch -y\n",
        "!pip install torch==1.8.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.9.0+cu102\n",
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG-mynzHPG5k",
        "outputId": "02906b25-fdc0-4bb6-ebf9-c760646f6240"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install --quiet -v --no-cache-dir ./"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8102, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 8102 (delta 92), reused 124 (delta 63), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8102/8102), 14.15 MiB | 22.02 MiB/s, done.\n",
            "Resolving deltas: 100% (5493/5493), done.\n",
            "/content/apex\n",
            "Processing /content/apex\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: apex\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=205204 sha256=17f887f1fef2bcf5a99653bfd7036b461a1772892c2ef0db840e17d66c3ff66c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jrzgx8w7/wheels/02/1d/54/16beaa489b73437cc70f3f4ef0bbaa36f0ac443dd94834df91\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "Successfully installed apex-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qSEpwYDPJcM",
        "outputId": "521bfe5c-6317-4e57-f87a-f3d5d5775402"
      },
      "source": [
        "pip install PyYAML"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (5.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeB-8AtePL4A",
        "outputId": "5ddc135b-f07f-4eef-c9f4-8b4bdd3b9fa4"
      },
      "source": [
        "pip install -U PyYAML"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (5.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxJ56dbBPPf7",
        "outputId": "f5fade02-a471-4722-a692-d7fcc30d8b9f"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSAriVP8PSFg",
        "outputId": "4390cb4a-9722-4ab6-cf0a-da8312ff6263"
      },
      "source": [
        "!git clone -b feature/DR-images-v2 https://github.com/jmarrietar/suncet.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'suncet'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Counting objects: 100% (336/336), done.\u001b[K\n",
            "remote: Compressing objects: 100% (214/214), done.\u001b[K\n",
            "remote: Total 336 (delta 199), reused 250 (delta 119), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (336/336), 1.11 MiB | 8.65 MiB/s, done.\n",
            "Resolving deltas: 100% (199/199), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBuHyojFPn-a",
        "outputId": "3e750f2c-b10b-4b46-c5d7-1f5c89f620af"
      },
      "source": [
        "cd suncet"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/suncet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuGu7UYlQWfL"
      },
      "source": [
        "!mkdir datasets\n",
        "!mkdir datasets/dr\n",
        "!mkdir logs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJUpmIirQWkS",
        "outputId": "c4afc05d-0c50-4639-f572-0135dfedd1ce"
      },
      "source": [
        "!python download.py -d sample@1000"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DPZrHrj3Bdte5Dc6NCZ33CAqMG-Oipa2\n",
            "To: /content/suncet/datasets/dr/sample@1000.zip\n",
            "108MB [00:00, 113MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA0_ZQBMQWrP"
      },
      "source": [
        "import argparse\n",
        "import yaml"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPEEocg_QWw6",
        "outputId": "6c235e0c-fb09-4ce1-ad73-fd2ece3516fa"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--fname', type=str,\n",
        "    help='name of config file to load',\n",
        "    default='configs.yaml')\n",
        "parser.add_argument(\n",
        "    '--devices', type=str, nargs='+', default=['cuda:0'],\n",
        "    help='which devices to use on local machine')\n",
        "parser.add_argument(\n",
        "    '--sel', type=str,\n",
        "    help='which script to run',\n",
        "    choices=[\n",
        "        'paws_train',\n",
        "        'suncet_train',\n",
        "        'fine_tune',\n",
        "        'snn_fine_tune'\n",
        "    ])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--sel'], dest='sel', nargs=None, const=None, default=None, type=<class 'str'>, choices=['paws_train', 'suncet_train', 'fine_tune', 'snn_fine_tune'], help='which script to run', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JOY6LhGQWzA"
      },
      "source": [
        "args = parser.parse_args(['--sel', 'paws_train',\n",
        "                            '--fname', 'configs/paws/dr_train.yaml'\n",
        "])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13k7yK1cQW3j",
        "outputId": "6cbafd67-f31d-4ee7-db71-3554897bf43c"
      },
      "source": [
        "args"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(devices=['cuda:0'], fname='configs/paws/dr_train.yaml', sel='paws_train')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tk7_KPEQkog",
        "outputId": "d35368e8-3f17-4f57-9aa5-77dac9044cf7"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.8.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DElWMyYNK-Tw"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7lYYw-ZQkuV",
        "outputId": "30b0846a-c7f5-4c32-96f6-e148ba07f46e"
      },
      "source": [
        "print(torch_xla.__version__)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5hFmaVJVewD"
      },
      "source": [
        "def load_checkpoint(r_path, encoder, opt, scaler, use_fp16=False):\n",
        "    checkpoint = torch.load(r_path, map_location=\"cpu\")\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "    # -- loading encoder\n",
        "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
        "    logger.info(f\"loaded encoder from epoch {epoch}\")\n",
        "\n",
        "    # -- loading optimizer\n",
        "    opt.load_state_dict(checkpoint[\"opt\"])\n",
        "    if use_fp16:\n",
        "        scaler.load_state_dict(checkpoint[\"amp\"])\n",
        "    logger.info(f\"loaded optimizers from epoch {epoch}\")\n",
        "    logger.info(f\"read-path: {r_path}\")\n",
        "    del checkpoint\n",
        "    return encoder, opt, epoch\n",
        "\n",
        "\n",
        "def init_model(device, model_name=\"resnet50\", use_pred=False, output_dim=128):\n",
        "    if \"wide_resnet\" in model_name:\n",
        "        encoder = wide_resnet.__dict__[model_name](dropout_rate=0.0)\n",
        "        hidden_dim = 128\n",
        "    else:\n",
        "        encoder = resnet.__dict__[model_name]() \n",
        "\n",
        "        # Load pre-trained ResNetImagenNet\n",
        "        #logger.info(\"Load pre-trained ResNet ImagenNet weigths ...\")\n",
        "        #state_dict = load_state_dict_from_url('https://download.pytorch.org/models/resnet50-0676ba61.pth',progress=True)\n",
        "        #log = encoder.load_state_dict(state_dict, strict=False)\n",
        "        #logger.info(log)\n",
        "    \n",
        "        hidden_dim = 2048\n",
        "        if \"w2\" in model_name:\n",
        "            hidden_dim *= 2\n",
        "        elif \"w4\" in model_name:\n",
        "            hidden_dim *= 4\n",
        "\n",
        "    # -- projection head\n",
        "    encoder.fc = torch.nn.Sequential(\n",
        "        OrderedDict(\n",
        "            [\n",
        "                (\"fc1\", torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "                (\"bn1\", torch.nn.BatchNorm1d(hidden_dim)),\n",
        "                (\"relu1\", torch.nn.ReLU(inplace=True)),\n",
        "                (\"fc2\", torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "                (\"bn2\", torch.nn.BatchNorm1d(hidden_dim)),\n",
        "                (\"relu2\", torch.nn.ReLU(inplace=True)),\n",
        "                (\"fc3\", torch.nn.Linear(hidden_dim, output_dim)),\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # -- prediction head\n",
        "    encoder.pred = None\n",
        "    if use_pred:\n",
        "        mx = 4  # 4x bottleneck prediction head\n",
        "        pred_head = OrderedDict([])\n",
        "        pred_head[\"bn1\"] = torch.nn.BatchNorm1d(output_dim)\n",
        "        pred_head[\"fc1\"] = torch.nn.Linear(output_dim, output_dim // mx)\n",
        "        pred_head[\"bn2\"] = torch.nn.BatchNorm1d(output_dim // mx)\n",
        "        pred_head[\"relu\"] = torch.nn.ReLU(inplace=True)\n",
        "        pred_head[\"fc2\"] = torch.nn.Linear(output_dim // mx, output_dim)\n",
        "        encoder.pred = torch.nn.Sequential(pred_head)\n",
        "\n",
        "    encoder.to(device)\n",
        "    logger.info(encoder)\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def init_opt(\n",
        "    encoder,\n",
        "    iterations_per_epoch,\n",
        "    start_lr,\n",
        "    ref_lr,\n",
        "    ref_mom,\n",
        "    nesterov,\n",
        "    warmup,\n",
        "    num_epochs,\n",
        "    weight_decay=1e-6,\n",
        "    final_lr=0.0,\n",
        "):\n",
        "    param_groups = [\n",
        "        {\n",
        "            \"params\": (\n",
        "                p\n",
        "                for n, p in encoder.named_parameters()\n",
        "                if (\"bias\" not in n) and (\"bn\" not in n)\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"params\": (\n",
        "                p for n, p in encoder.named_parameters() if (\"bias\" in n) or (\"bn\" in n)\n",
        "            ),\n",
        "            \"LARS_exclude\": True,\n",
        "            \"weight_decay\": 0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = SGD(\n",
        "        param_groups,\n",
        "        weight_decay=weight_decay,\n",
        "        momentum=0.9,\n",
        "        nesterov=nesterov,\n",
        "        lr=ref_lr,\n",
        "    )\n",
        "    scheduler = WarmupCosineSchedule(\n",
        "        optimizer,\n",
        "        warmup_steps=warmup * iterations_per_epoch,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=ref_lr,\n",
        "        final_lr=final_lr,\n",
        "        T_max=num_epochs * iterations_per_epoch,\n",
        "    )\n",
        "    optimizer = LARS(optimizer, trust_coefficient=0.001)\n",
        "    return encoder, optimizer, scheduler"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_u7XpR4dXs6"
      },
      "source": [
        "fname = args.fname\n",
        "sel = args.sel"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCIcRanJdbkX"
      },
      "source": [
        "import pprint\n",
        "import logging\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "\n",
        "logger.info(f'called-params {sel} {fname}')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N6nW_InddEy",
        "outputId": "c9e4553e-1387-4460-e10c-8725370aea21"
      },
      "source": [
        "# -- load script params\n",
        "params = None\n",
        "with open(fname, 'r') as y_file:\n",
        "    #params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
        "    params = yaml.load(y_file)\n",
        "    logger.info('loaded params...')\n",
        "    #if rank == 0:\n",
        "    pp = pprint.PrettyPrinter(indent=4)\n",
        "    pp.pprint(params)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{   'criterion': {   'classes_per_batch': 2,\n",
            "                     'me_max': True,\n",
            "                     'sharpen': 0.25,\n",
            "                     'supervised_imgs_per_class': 32,\n",
            "                     'supervised_views': 1,\n",
            "                     'temperature': 0.1,\n",
            "                     'unsupervised_batch_size': 32},\n",
            "    'data': {   'color_jitter_strength': 1.0,\n",
            "                'data_seed': None,\n",
            "                'dataset': 'dr',\n",
            "                'label_smoothing': 0.1,\n",
            "                'multicrop': 6,\n",
            "                'normalize': True,\n",
            "                'root_path': 'datasets/',\n",
            "                's_image_folder': 'dr/sample@1000/',\n",
            "                'subset_path': 'dr_subsets',\n",
            "                'u_image_folder': 'dr/train_voets/',\n",
            "                'unique_classes_per_rank': False,\n",
            "                'unlabeled_frac': 0.9},\n",
            "    'logging': {'folder': 'logs/', 'write_tag': 'paws'},\n",
            "    'meta': {   'copy_data': True,\n",
            "                'device': 'cuda:0',\n",
            "                'load_checkpoint': False,\n",
            "                'model_name': 'resnet50',\n",
            "                'output_dim': 2048,\n",
            "                'read_checkpoint': None,\n",
            "                'use_fp16': True,\n",
            "                'use_pred_head': True},\n",
            "    'optimization': {   'epochs': 100,\n",
            "                        'final_lr': 1e-05,\n",
            "                        'lr': 0.001,\n",
            "                        'momentum': 0.9,\n",
            "                        'nesterov': False,\n",
            "                        'start_lr': 0.001,\n",
            "                        'warmup': 10,\n",
            "                        'weight_decay': 1e-06}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKra8C8vd1ep"
      },
      "source": [
        "args = params"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j0-J-PaXJK3"
      },
      "source": [
        "# ----------------------------------------------------------------------- #\n",
        "#  PASSED IN PARAMS FROM CONFIG FILE\n",
        "# ----------------------------------------------------------------------- #\n",
        "# -- META\n",
        "model_name = args[\"meta\"][\"model_name\"]\n",
        "output_dim = args[\"meta\"][\"output_dim\"]\n",
        "load_model = args[\"meta\"][\"load_checkpoint\"]\n",
        "r_file = args[\"meta\"][\"read_checkpoint\"]\n",
        "copy_data = args[\"meta\"][\"copy_data\"]\n",
        "use_fp16 = args[\"meta\"][\"use_fp16\"]\n",
        "use_pred_head = args[\"meta\"][\"use_pred_head\"]\n",
        "#device = torch.device(args[\"meta\"][\"device\"])\n",
        "device = xm.xla_device()\n",
        "\n",
        "# -- CRITERTION\n",
        "reg = args[\"criterion\"][\"me_max\"]\n",
        "supervised_views = args[\"criterion\"][\"supervised_views\"]\n",
        "classes_per_batch = args[\"criterion\"][\"classes_per_batch\"]\n",
        "s_batch_size = args[\"criterion\"][\"supervised_imgs_per_class\"]\n",
        "u_batch_size = args[\"criterion\"][\"unsupervised_batch_size\"]\n",
        "temperature = args[\"criterion\"][\"temperature\"]\n",
        "sharpen = args[\"criterion\"][\"sharpen\"]\n",
        "\n",
        "# -- DATA\n",
        "unlabeled_frac = args[\"data\"][\"unlabeled_frac\"]\n",
        "color_jitter = args[\"data\"][\"color_jitter_strength\"]\n",
        "normalize = args[\"data\"][\"normalize\"]\n",
        "root_path = args[\"data\"][\"root_path\"]\n",
        "s_image_folder = args[\"data\"][\"s_image_folder\"]\n",
        "u_image_folder = args[\"data\"][\"u_image_folder\"]\n",
        "dataset_name = args[\"data\"][\"dataset\"]\n",
        "subset_path = args[\"data\"][\"subset_path\"]\n",
        "unique_classes = args[\"data\"][\"unique_classes_per_rank\"]\n",
        "multicrop = args[\"data\"][\"multicrop\"]\n",
        "label_smoothing = args[\"data\"][\"label_smoothing\"]\n",
        "data_seed = None\n",
        "if \"cifar10\" in dataset_name:\n",
        "    data_seed = args[\"data\"][\"data_seed\"]\n",
        "    crop_scale = (0.75, 1.0) if multicrop > 0 else (0.5, 1.0)\n",
        "    mc_scale = (0.3, 0.75)\n",
        "    mc_size = 18\n",
        "else:\n",
        "    crop_scale = (0.14, 1.0) if multicrop > 0 else (0.08, 1.0)\n",
        "    mc_scale = (0.05, 0.14)\n",
        "    mc_size = 96\n",
        "\n",
        "# -- OPTIMIZATION\n",
        "wd = float(args[\"optimization\"][\"weight_decay\"])\n",
        "num_epochs = args[\"optimization\"][\"epochs\"]\n",
        "warmup = args[\"optimization\"][\"warmup\"]\n",
        "start_lr = args[\"optimization\"][\"start_lr\"]\n",
        "lr = args[\"optimization\"][\"lr\"]\n",
        "final_lr = args[\"optimization\"][\"final_lr\"]\n",
        "mom = args[\"optimization\"][\"momentum\"]\n",
        "nesterov = args[\"optimization\"][\"nesterov\"]\n",
        "\n",
        "# -- LOGGING\n",
        "folder = args[\"logging\"][\"folder\"]\n",
        "tag = args[\"logging\"][\"write_tag\"]\n",
        "# ----------------------------------------------------------------------- #\n",
        "\n",
        "# -- init torch distributed backend\n",
        "#world_size, rank = init_distributed()\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjWLczExkHJi",
        "outputId": "266684db-b774-48a3-8068-0648941f733c"
      },
      "source": [
        "xm.xrt_world_size()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkhcakuxVLe5"
      },
      "source": [
        "def train_resnet():\n",
        "\n",
        "    \"\"\"\n",
        "    TO DO: \n",
        "        1. Dataset y Dataloader: \n",
        "            - Creo que el init ya lo esta haciendo de la forma que Necesito y \n",
        "              me esta devolviendo lo que necesito, pero no estoy del todo seguro. \n",
        "              Por lo que deberia ver bien esto. Double Check!.\n",
        "        2. La parte del Loss Function  paws( , mirar si tengo que modificarle algo.\n",
        "        3. El update del optimizer, no estoy muy seguro donde va `xm.optimizer_step(optimizer)`\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if load_model:\n",
        "        load_path = os.path.join(folder, r_file) if r_file is not None else latest_path\n",
        "\n",
        "    # -- make csv_logger\n",
        "    csv_logger = CSVLogger(\n",
        "        log_file,\n",
        "        (\"%d\", \"epoch\"),\n",
        "        (\"%d\", \"itr\"),\n",
        "        (\"%.5f\", \"paws-xent-loss\"),\n",
        "        (\"%.5f\", \"paws-me_max-reg\"),\n",
        "        (\"%d\", \"time (ms)\"),\n",
        "    )\n",
        "\n",
        "    # -- init model\n",
        "    encoder = init_model(\n",
        "        device=device,\n",
        "        model_name=model_name,\n",
        "        use_pred=use_pred_head,\n",
        "        output_dim=output_dim,\n",
        "    )\n",
        "    if xm.xrt_world_size() > 1:\n",
        "        process_group = apex.parallel.create_syncbn_process_group(0)\n",
        "        encoder = apex.parallel.convert_syncbn_model(\n",
        "            encoder, process_group=process_group\n",
        "        )\n",
        "\n",
        "    # -- init losses\n",
        "    paws = init_paws_loss(multicrop=multicrop, tau=temperature, T=sharpen, me_max=reg)\n",
        "    # -- assume support images are sampled with ClassStratifiedSampler\n",
        "    labels_matrix = make_labels_matrix(\n",
        "        num_classes=classes_per_batch,\n",
        "        s_batch_size=s_batch_size,\n",
        "        world_size=world_size,\n",
        "        device=device,\n",
        "        unique_classes=unique_classes,\n",
        "        smoothing=label_smoothing,\n",
        "    )\n",
        "\n",
        "    print(\"normalize {}\".format(normalize))\n",
        "\n",
        "    # -- make data transforms\n",
        "    transform, init_transform = make_transforms(\n",
        "        dataset_name=dataset_name,\n",
        "        subset_path=subset_path,\n",
        "        unlabeled_frac=unlabeled_frac,\n",
        "        training=True,\n",
        "        split_seed=data_seed,\n",
        "        crop_scale=crop_scale,\n",
        "        basic_augmentations=False,\n",
        "        color_jitter=color_jitter,\n",
        "        normalize=normalize,\n",
        "    )\n",
        "    multicrop_transform = (multicrop, None)\n",
        "    if multicrop > 0:\n",
        "        multicrop_transform = make_multicrop_transform(\n",
        "            dataset_name=dataset_name,\n",
        "            num_crops=multicrop,\n",
        "            size=mc_size,\n",
        "            crop_scale=mc_scale,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_jitter,\n",
        "        )\n",
        "\n",
        "    # -- init data-loaders/samplers\n",
        "    (\n",
        "        unsupervised_loader,\n",
        "        unsupervised_sampler,\n",
        "        supervised_loader,\n",
        "        supervised_sampler,\n",
        "    ) = init_data(\n",
        "        dataset_name=dataset_name,\n",
        "        transform=transform,\n",
        "        init_transform=init_transform,\n",
        "        supervised_views=supervised_views,\n",
        "        u_batch_size=u_batch_size,\n",
        "        s_batch_size=s_batch_size,\n",
        "        unique_classes=unique_classes,\n",
        "        classes_per_batch=classes_per_batch,\n",
        "        multicrop_transform=multicrop_transform,\n",
        "        world_size=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        root_path=root_path,\n",
        "        s_image_folder=s_image_folder,\n",
        "        u_image_folder=u_image_folder,\n",
        "        training=True,\n",
        "        copy_data=copy_data,\n",
        "    )\n",
        "    iter_supervised = None\n",
        "    ipe = len(unsupervised_loader)\n",
        "    logger.info(f\"iterations per epoch: {ipe}\")\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "    encoder, optimizer, scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        weight_decay=wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        ref_mom=mom,\n",
        "        nesterov=nesterov,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "    if world_size > 1:\n",
        "        encoder = DistributedDataParallel(encoder, broadcast_buffers=False)\n",
        "\n",
        "    start_epoch = 0\n",
        "    # -- load training checkpoint\n",
        "    if load_model:\n",
        "        encoder, optimizer, start_epoch = load_checkpoint(\n",
        "            r_path=load_path,\n",
        "            encoder=encoder,\n",
        "            opt=optimizer,\n",
        "            scaler=scaler,\n",
        "            use_fp16=use_fp16,\n",
        "        )\n",
        "        for _ in range(start_epoch):\n",
        "            for _ in range(ipe):\n",
        "                scheduler.step()\n",
        "\n",
        "    def train_loop_fn(supervised_loader, unsupervised_loader, epoch):\n",
        "\n",
        "        # -- TRAINING LOOP\n",
        "        best_loss = None\n",
        "\n",
        "        logger.info(\"Epoch %d\" % (epoch + 1))\n",
        "\n",
        "        # -- update distributed-data-loader epoch\n",
        "        unsupervised_sampler.set_epoch(epoch)\n",
        "        if supervised_sampler is not None:\n",
        "            supervised_sampler.set_epoch(epoch)\n",
        "\n",
        "        loss_meter = AverageMeter()\n",
        "        ploss_meter = AverageMeter()\n",
        "        rloss_meter = AverageMeter()\n",
        "        time_meter = AverageMeter()\n",
        "        data_meter = AverageMeter()\n",
        "\n",
        "        for itr, udata in enumerate(unsupervised_loader):\n",
        "\n",
        "            def load_imgs():\n",
        "                # -- unsupervised imgs\n",
        "                uimgs = [u.to(device, non_blocking=True) for u in udata[:-1]]\n",
        "                # -- supervised imgs\n",
        "                global iter_supervised\n",
        "                try:\n",
        "                    sdata = next(iter_supervised)\n",
        "                except Exception:\n",
        "                    iter_supervised = iter(supervised_loader)\n",
        "                    logger.info(f\"len.supervised_loader: {len(iter_supervised)}\")\n",
        "                    sdata = next(iter_supervised)\n",
        "                finally:\n",
        "                    labels = torch.cat([labels_matrix for _ in range(supervised_views)])\n",
        "                    simgs = [s.to(device, non_blocking=True) for s in sdata[:-1]]\n",
        "                # -- concatenate supervised imgs and unsupervised imgs\n",
        "                imgs = simgs + uimgs\n",
        "                return imgs, labels\n",
        "\n",
        "            (imgs, labels) = load_imgs\n",
        "\n",
        "            def train_step():\n",
        "                with torch.cuda.amp.autocast(enabled=use_fp16):\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # --\n",
        "                    # h: representations of 'imgs' before head\n",
        "                    # z: representations of 'imgs' after head\n",
        "                    # -- If use_pred_head=False, then encoder.pred (prediction\n",
        "                    #    head) is None, and _forward_head just returns the\n",
        "                    #    identity, z=h\n",
        "                    h, z = encoder(imgs, return_before_head=True)\n",
        "\n",
        "                    # Compute paws loss in full precision\n",
        "                    with torch.cuda.amp.autocast(enabled=False):\n",
        "\n",
        "                        # Step 1. convert representations to fp32\n",
        "                        h, z = h.float(), z.float()\n",
        "\n",
        "                        # Step 2. determine anchor views/supports and their\n",
        "                        #         corresponding target views/supports\n",
        "                        # --\n",
        "                        num_support = (\n",
        "                            supervised_views * s_batch_size * classes_per_batch\n",
        "                        )\n",
        "                        # --\n",
        "                        anchor_supports = z[:num_support]\n",
        "                        anchor_views = z[num_support:]\n",
        "                        # --\n",
        "                        target_supports = h[:num_support].detach()\n",
        "                        target_views = h[num_support:].detach()\n",
        "                        target_views = torch.cat(\n",
        "                            [\n",
        "                                target_views[u_batch_size : 2 * u_batch_size],\n",
        "                                target_views[:u_batch_size],\n",
        "                            ],\n",
        "                            dim=0,\n",
        "                        )\n",
        "\n",
        "                        # Step 3. compute paws loss with me-max regularization\n",
        "                        (ploss, me_max) = paws(\n",
        "                            anchor_views=anchor_views,\n",
        "                            anchor_supports=anchor_supports,\n",
        "                            anchor_support_labels=labels,\n",
        "                            target_views=target_views,\n",
        "                            target_supports=target_supports,\n",
        "                            target_support_labels=labels,\n",
        "                        )\n",
        "                        loss = ploss + me_max\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                lr_stats = scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                return (float(loss), float(ploss), float(me_max), lr_stats)\n",
        "\n",
        "            (loss, ploss, rloss, lr_stats) = train_step()\n",
        "            loss_meter.update(loss)\n",
        "            ploss_meter.update(ploss)\n",
        "            rloss_meter.update(rloss)\n",
        "\n",
        "            if (itr % log_freq == 0) or np.isnan(loss) or np.isinf(loss):\n",
        "                csv_logger.log(\n",
        "                    epoch + 1, itr, ploss_meter.avg, rloss_meter.avg, time_meter.avg\n",
        "                )\n",
        "                logger.info(\n",
        "                    \"[%d, %5d] loss: %.5f (%.5f %.5f) \"\n",
        "                    \"(%d ms; %d ms)\"\n",
        "                    % (\n",
        "                        epoch + 1,\n",
        "                        itr,\n",
        "                        loss_meter.avg,\n",
        "                        ploss_meter.avg,\n",
        "                        rloss_meter.avg,\n",
        "                        time_meter.avg,\n",
        "                        data_meter.avg,\n",
        "                    )\n",
        "                )\n",
        "                if lr_stats is not None:\n",
        "                    logger.info(\n",
        "                        \"[%d, %5d] lr_stats: %.5f (%.2e, %.2e)\"\n",
        "                        % (epoch + 1, itr, lr_stats.avg, lr_stats.min, lr_stats.max)\n",
        "                    )\n",
        "\n",
        "            assert not np.isnan(loss), \"loss is nan\"\n",
        "\n",
        "        # -- logging/checkpointing\n",
        "        logger.info(\"avg. loss %.3f\" % loss_meter.avg)\n",
        "\n",
        "        if rank == 0:\n",
        "            save_dict = {\n",
        "                \"encoder\": encoder.state_dict(),\n",
        "                \"opt\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"unlabel_prob\": unlabeled_frac,\n",
        "                \"loss\": loss_meter.avg,\n",
        "                \"s_batch_size\": s_batch_size,\n",
        "                \"u_batch_size\": u_batch_size,\n",
        "                \"world_size\": world_size,\n",
        "                \"lr\": lr,\n",
        "                \"temperature\": temperature,\n",
        "                \"amp\": scaler.state_dict(),\n",
        "            }\n",
        "            torch.save(save_dict, latest_path)\n",
        "            if best_loss is None or best_loss > loss_meter.avg:\n",
        "                best_loss = loss_meter.avg\n",
        "                logger.info('updating \"best\" checkpoint')\n",
        "                torch.save(save_dict, best_path)\n",
        "            if (\n",
        "                (epoch + 1) % checkpoint_freq == 0\n",
        "                or (epoch + 1) % 10 == 0\n",
        "                and epoch < checkpoint_freq\n",
        "            ):\n",
        "                torch.save(save_dict, save_path.format(epoch=f\"{epoch + 1}\"))\n",
        "\n",
        "        train_supervised_loader = pl.MpDeviceLoader(supervised_loader, device)\n",
        "        train_unsupervised_loader = pl.MpDeviceLoader(unsupervised_loader, device)\n",
        "\n",
        "        for epoch in range(start_epoch, end_epoch):\n",
        "            # para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "            # train_loop_fn(para_loader.per_device_loader(device))\n",
        "            train_loop_fn(train_supervised_loader, train_unsupervised_loader, epoch)\n",
        "            xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                model_name = \"PAWS-DR-epoch-{}.pt\".format(epoch)\n",
        "                if FLAGS.save_drive:\n",
        "                    model_name = (\n",
        "                        \"/content/drive/MyDrive/Colab Notebooks/PAWS/models/PAWS-DR-pytorch/\"\n",
        "                        + model_name\n",
        "                    )\n",
        "\n",
        "                    xm.save(model.state_dict(), model_name)\n",
        "                else:\n",
        "                    xm.save(model.state_dict(), \"models/\" + model_name)\n",
        "\n",
        "            if FLAGS.metrics_debug:\n",
        "                xm.master_print(met.metrics_report(), flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9OZVpmZQk03"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    global FLAGS\n",
        "    FLAGS = args\n",
        "    global args\n",
        "    world_size = xm.xrt_world_size()\n",
        "    logger.info(f\"Initialized (rank/world-size) {rank}/{world_size}\")\n",
        "\n",
        "    # -- log/checkpointing paths\n",
        "    log_file = os.path.join(folder, f\"{tag}_r{rank}.csv\")\n",
        "    save_path = os.path.join(folder, f\"{tag}\" + \"-ep{epoch}.pth.tar\")\n",
        "    latest_path = os.path.join(folder, f\"{tag}-latest.pth.tar\")\n",
        "    best_path = os.path.join(folder, f\"{tag}\" + \"-best.pth.tar\")\n",
        "    load_path = None\n",
        "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "    accuracy, data, pred, target = train_resnet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzRQPZIOVI5J"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    FLAGS = args\n",
        "    print(FLAGS)\n",
        "    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS.num_cores, start_method=\"fork\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfnAGdoPbcFN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}