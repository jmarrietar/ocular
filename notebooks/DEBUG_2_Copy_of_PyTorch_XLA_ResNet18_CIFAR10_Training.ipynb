{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEBUG 2 - Copy of PyTorch/XLA ResNet18/CIFAR10 Training",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/DEBUG_2_Copy_of_PyTorch_XLA_ResNet18_CIFAR10_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1hxqUQn47M"
      },
      "source": [
        "## PyTorch/XLA ResNet18/CIFAR10 (GPU or TPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLQPoJ6Fn8wF"
      },
      "source": [
        "### [RUNME] Install Colab compatible PyTorch/XLA wheels and dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad03bb61-09c7-4b3b-c7cb-bbae6b79eef9"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cloud-tpu-client==0.10\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Collecting torch-xla==1.8.1\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl (145.0MB)\n",
            "\u001b[K     |████████████████████████████████| 145.0MB 47kB/s \n",
            "\u001b[?25hCollecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.30.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (56.1.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (20.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "\u001b[31mERROR: earthengine-api 0.1.266 has requirement google-api-python-client<2,>=1.12.1, but you'll have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla\n",
            "  Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6H47dPVMAIJ"
      },
      "source": [
        "import gdown"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHKXfPULVP1"
      },
      "source": [
        "def download(data, url):\n",
        "    # Download dataset\n",
        "    import zipfile\n",
        "    url = url\n",
        "    output = \"{}.zip\".format(data)\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Uncompress dataset\n",
        "    local_zip = '{}.zip'.format(data)\n",
        "    zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhpxrcKLX7W"
      },
      "source": [
        "data_samples = {\n",
        "    \"sample@200\": \"https://drive.google.com/uc?id=1FfV7YyDJvNUCDP5r3-8iQfZ2-xJp_pgb\",\n",
        "    \"sample@500\": \"https://drive.google.com/uc?id=1dHwUqpmSogEdjAB9rwDUL-OKFRUcVXte\",\n",
        "    \"sample@1000\": \"https://drive.google.com/uc?id=1DPZrHrj3Bdte5Dc6NCZ33CAqMG-Oipa2\",\n",
        "    \"sample@2000\": \"https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\",\n",
        "    \"sample@3000\": \"https://drive.google.com/uc?id=1_yre5K9YYvJgSrT4xvrI8eD_htucIywA\",\n",
        "    \"sample@4000_images\": \"https://drive.google.com/uc?id=1dqVB8EozEpwWzyuU80AauoQmsiw3Gtm2\",\n",
        "    \"sample@20000\": \"https://drive.google.com/uc?id=1MTDpLzpmhSiZq2jSdmHx2UDPn9FC8gzO\",\n",
        "    \"val-voets-tf\": \"https://drive.google.com/uc?id=1VzVgMGTkBBPG2qbzLunD9HvLzH6tcyrv\",\n",
        "    \"train_voets\": \"https://drive.google.com/uc?id=1AmcFh1MOOZ6aqKm2eO7XEdgmIEqHKTZ5\",\n",
        "    \"voets_test_images\": \"https://drive.google.com/uc?id=15S_V3B_Z3BOjCT3AbO2c887FyS5B0Lyd\"\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1J6gqYLZC7"
      },
      "source": [
        "UNLABELED = 'train_voets'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-BZSTOLaFm",
        "outputId": "c6d6f6eb-a7bc-40fb-93c0-28378823f9d2"
      },
      "source": [
        "URL_UNLABELED = data_samples[UNLABELED]\n",
        "download(UNLABELED, URL_UNLABELED)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AmcFh1MOOZ6aqKm2eO7XEdgmIEqHKTZ5\n",
            "To: /content/train_voets.zip\n",
            "3.09GB [00:30, 99.7MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IednejwkIW-K"
      },
      "source": [
        "Only run the below commented cell if you would like a nightly release"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-bBdzgeISaP"
      },
      "source": [
        "# VERSION = \"nightly\"  #@param [\"nightly\", \"20200516\"]  # or YYYYMMDD format\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION\n",
        "# import os \n",
        "# os.environ['LD_LIBRARY_PATH']='/usr/local/lib'\n",
        "# !echo $LD_LIBRARY_PATH\n",
        "\n",
        "# !sudo ln -s /usr/local/lib/libmkl_intel_lp64.so /usr/local/lib/libmkl_intel_lp64.so.1\n",
        "# !sudo ln -s /usr/local/lib/libmkl_intel_thread.so /usr/local/lib/libmkl_intel_thread.so.1\n",
        "# !sudo ln -s /usr/local/lib/libmkl_core.so /usr/local/lib/libmkl_core.so.1\n",
        "\n",
        "# !ldconfig\n",
        "# !ldd /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFzLg5gy7l6"
      },
      "source": [
        "# PyTorch/XLA GPU Setup (only if GPU runtime)\n",
        "import os\n",
        "if os.environ.get('COLAB_GPU', '0') == '1':\n",
        "  os.environ['GPU_NUM_DEVICES'] = '1'\n",
        "  os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rroH9yiAn-XE"
      },
      "source": [
        "### Define Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMojPWZUqr2s"
      },
      "source": [
        "# Result Visualization Helper\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "M, N = 4, 6\n",
        "RESULT_IMG_PATH = '/tmp/test_result.jpg'\n",
        "CIFAR10_LABELS = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def plot_results(images, labels, preds):\n",
        "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
        "  inv_norm = transforms.Normalize(\n",
        "      mean=(-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010),\n",
        "      std=(1/0.2023, 1/0.1994, 1/0.2010))\n",
        "\n",
        "  num_images = images.shape[0]\n",
        "  fig, axes = plt.subplots(M, N, figsize=(16, 9))\n",
        "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.axis('off')\n",
        "    if i >= num_images:\n",
        "      continue\n",
        "    img, label, prediction = images[i], labels[i], preds[i]\n",
        "    img = inv_norm(img)\n",
        "    img = img.permute(1, 2, 0) # (C, M, N) -> (M, N, C)\n",
        "    label, prediction = label.item(), prediction.item()\n",
        "    if label == prediction:\n",
        "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "    else:\n",
        "      ax.set_title(\n",
        "          'X {}/{}'.format(CIFAR10_LABELS[label],\n",
        "                          CIFAR10_LABELS[prediction]), color='red')\n",
        "    ax.imshow(img)\n",
        "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdPRFXIn_jH"
      },
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
        "FLAGS['batch_size'] = 64\n",
        "FLAGS['num_workers'] = 4\n",
        "FLAGS['learning_rate'] = 0.02\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 20\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd3xZvoA-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf2de7e-5137-4af5-be70-25c04983748c"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != self.expansion * planes:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_planes,\n",
        "              self.expansion * planes,\n",
        "              kernel_size=1,\n",
        "              stride=stride,\n",
        "              bias=False), nn.BatchNorm2d(self.expansion * planes))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, num_blocks, num_classes=2):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "    self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes * block.expansion\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = F.avg_pool2d(out, 4)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out = self.linear(out)\n",
        "    return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "  return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.8.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMl96KLoCq8"
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "# Only instantiate model weights once in memory.\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
        "\n",
        "def train_resnet18():\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  def get_dataset():\n",
        "    norm = transforms.Normalize(\n",
        "        mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        norm,\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        norm,\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(root=\"{}\".format(UNLABELED),\n",
        "        transform=transform_train)\n",
        "    test_dataset = datasets.ImageFolder(root=\"{}\".format(UNLABELED),\n",
        "        transform=transform_test)\n",
        "    \n",
        "    return train_dataset, test_dataset\n",
        "  \n",
        "  # Using the serial executor avoids multiple processes\n",
        "  # to download the same data.\n",
        "  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)\n",
        "\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "      train_dataset,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=True)\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      shuffle=False,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  # Scale learning rate to num cores\n",
        "  learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "  # Get loss function, optimizer, and model\n",
        "  device = xm.xla_device()\n",
        "  model = WRAPPED_MODEL.to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                        momentum=FLAGS['momentum'], weight_decay=5e-4)\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_loop_fn(loader):\n",
        "    tracker = xm.RateTracker()\n",
        "    model.train()\n",
        "    for x, (data, target) in enumerate(loader):\n",
        "      print(x)\n",
        "      print(\"JOSEE\")\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      xm.optimizer_step(optimizer)\n",
        "      tracker.add(FLAGS['batch_size'])\n",
        "      if x % FLAGS['log_steps'] == 0:\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
        "            tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "  def test_loop_fn(loader):\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    model.eval()\n",
        "    data, pred, target = None, None, None\n",
        "    for data, target in loader:\n",
        "      output = model(data)\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      total_samples += data.size()[0]\n",
        "\n",
        "    accuracy = 100.0 * correct / total_samples\n",
        "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "        xm.get_ordinal(), accuracy), flush=True)\n",
        "    return accuracy, data, pred, target\n",
        "\n",
        "  # Train and eval loops\n",
        "  accuracy = 0.0\n",
        "  data, pred, target = None, None, None\n",
        "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    train_loop_fn(para_loader.per_device_loader(device))\n",
        "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
        "    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
        "    if FLAGS['metrics_debug']:\n",
        "      xm.master_print(met.metrics_report(), flush=True)\n",
        "\n",
        "  return accuracy, data, pred, target\n",
        "  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXIVNrvwMt_g"
      },
      "source": [
        "\"\"\"\n",
        "PILAS AQUI: \n",
        " - Parece que el problema que tenia era que 500 no era un numero que se distribuia en \n",
        " los 8 cores entonces no entrenada, pero no daba error, simplemente no funcionaban\n",
        " los dataloaders paralelos\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80fe305b-cab3-4f0f-97b5-454f57ad9c8a"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  accuracy, data, pred, target = train_resnet18()\n",
        "  if rank == 0:\n",
        "    # Retrieve tensors that are on TPU core 0 and plot.\n",
        "    plot_results(data.cpu(), pred.cpu(), target.cpu())\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "          start_method='fork')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "JOSEE\n",
            "JOSEE\n",
            "0\n",
            "JOSEE\n",
            "0\n",
            "JOSEE\n",
            "[xla:5](0) Loss=0.60992 Rate=20.09 GlobalRate=20.09 Time=Wed May 26 05:42:52 2021\n",
            "[xla:4](0) Loss=0.65968 Rate=45.44 GlobalRate=45.44 Time=Wed May 26 05:42:52 2021\n",
            "[xla:6](0) Loss=0.56282 Rate=27.33 GlobalRate=27.32 Time=Wed May 26 05:42:52 2021\n",
            "[xla:0](0) Loss=0.70522 Rate=15.98 GlobalRate=15.98 Time=Wed May 26 05:42:52 2021\n",
            "0\n",
            "JOSEE\n",
            "[xla:2](0) Loss=0.58635 Rate=148.22 GlobalRate=148.22 Time=Wed May 26 05:42:53 2021\n",
            "0\n",
            "JOSEE\n",
            "[xla:3](0) Loss=0.63524 Rate=143.74 GlobalRate=143.73 Time=Wed May 26 05:42:55 2021\n",
            "0\n",
            "JOSEE\n",
            "[xla:7](0) Loss=0.66851 Rate=124.15 GlobalRate=124.14 Time=Wed May 26 05:42:56 2021\n",
            "0\n",
            "JOSEE\n",
            "[xla:1](0) Loss=0.57618 Rate=143.87 GlobalRate=143.87 Time=Wed May 26 05:42:56 2021\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "JOSEE\n",
            "1\n",
            "1\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "4\n",
            "JOSEE\n",
            "JOSEE\n",
            "4\n",
            "4\n",
            "4\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "5\n",
            "5\n",
            "5\n",
            "5\n",
            "5\n",
            "5\n",
            "5\n",
            "5\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "7\n",
            "7\n",
            "7\n",
            "7\n",
            "7\n",
            "7\n",
            "JOSEE\n",
            "JOSEE\n",
            "7\n",
            "7\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "8\n",
            "JOSEE\n",
            "JOSEE\n",
            "8\n",
            "JOSEE\n",
            "8\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "9\n",
            "9\n",
            "9\n",
            "JOSEE\n",
            "9\n",
            "JOSEE\n",
            "9\n",
            "9\n",
            "JOSEE\n",
            "9\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "9\n",
            "JOSEE\n",
            "JOSEE\n",
            "10\n",
            "JOSEE\n",
            "10\n",
            "10\n",
            "10\n",
            "JOSEE\n",
            "10\n",
            "10\n",
            "10\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "10\n",
            "JOSEE\n",
            "JOSEE\n",
            "11\n",
            "11\n",
            "11\n",
            "11\n",
            "11\n",
            "11\n",
            "11\n",
            "JOSEE\n",
            "11\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "12\n",
            "12\n",
            "12\n",
            "JOSEE\n",
            "12\n",
            "JOSEE\n",
            "12\n",
            "12\n",
            "12\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "12\n",
            "JOSEE\n",
            "JOSEE\n",
            "13\n",
            "13\n",
            "13\n",
            "13\n",
            "JOSEE\n",
            "13\n",
            "JOSEE\n",
            "13\n",
            "JOSEE\n",
            "JOSEE\n",
            "13\n",
            "JOSEE\n",
            "13\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "14\n",
            "JOSEE\n",
            "14\n",
            "JOSEE\n",
            "14\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "14\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "15\n",
            "JOSEE\n",
            "JOSEE\n",
            "15\n",
            "15\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "16\n",
            "JOSEE\n",
            "JOSEE\n",
            "17\n",
            "17\n",
            "17\n",
            "17\n",
            "17\n",
            "17\n",
            "17\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "17\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "18\n",
            "18\n",
            "18\n",
            "18\n",
            "18\n",
            "18\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "18\n",
            "JOSEE\n",
            "18\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "19\n",
            "19\n",
            "19\n",
            "JOSEE\n",
            "19\n",
            "19\n",
            "JOSEE\n",
            "19\n",
            "19\n",
            "19\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "20\n",
            "20\n",
            "20\n",
            "20\n",
            "20\n",
            "20\n",
            "JOSEE\n",
            "20\n",
            "JOSEE\n",
            "JOSEE\n",
            "20\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "[xla:0](20) Loss=2.38794 Rate=44.54 GlobalRate=55.68 Time=Wed May 26 05:43:12 2021\n",
            "[xla:6](20) Loss=2.56421 Rate=49.04 GlobalRate=59.75 Time=Wed May 26 05:43:12 2021\n",
            "[xla:3](20) Loss=0.64676 Rate=101.11 GlobalRate=74.45 Time=Wed May 26 05:43:12 2021\n",
            "[xla:7](20) Loss=1.93084 Rate=95.78 GlobalRate=78.29 Time=Wed May 26 05:43:12 2021\n",
            "[xla:2](20) Loss=1.13545 Rate=99.86 GlobalRate=69.41 Time=Wed May 26 05:43:12 2021\n",
            "[xla:1](20) Loss=2.00287 Rate=105.65 GlobalRate=81.90 Time=Wed May 26 05:43:12 2021\n",
            "21\n",
            "[xla:5](20) Loss=1.47717 Rate=46.14 GlobalRate=57.57 Time=Wed May 26 05:43:12 2021\n",
            "21\n",
            "21\n",
            "21\n",
            "21\n",
            "JOSEE\n",
            "21\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "21\n",
            "JOSEE\n",
            "JOSEE\n",
            "[xla:4](20) Loss=0.56730 Rate=56.24 GlobalRate=62.26 Time=Wed May 26 05:43:12 2021\n",
            "JOSEE\n",
            "21\n",
            "JOSEE\n",
            "22\n",
            "22\n",
            "22\n",
            "22\n",
            "22\n",
            "22\n",
            "JOSEE\n",
            "22\n",
            "22\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "23\n",
            "23\n",
            "JOSEE\n",
            "23\n",
            "23\n",
            "23\n",
            "23\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "23\n",
            "JOSEE\n",
            "JOSEE\n",
            "23\n",
            "JOSEE\n",
            "24\n",
            "24\n",
            "24\n",
            "24\n",
            "24\n",
            "24\n",
            "JOSEE\n",
            "JOSEE\n",
            "24\n",
            "24\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "25\n",
            "25\n",
            "25\n",
            "JOSEE\n",
            "25\n",
            "JOSEE\n",
            "25\n",
            "25\n",
            "25\n",
            "JOSEE\n",
            "JOSEE\n",
            "25\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "26\n",
            "26\n",
            "26\n",
            "26\n",
            "26\n",
            "26\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "26\n",
            "26\n",
            "JOSEE\n",
            "JOSEE\n",
            "27\n",
            "27\n",
            "JOSEE\n",
            "27\n",
            "27\n",
            "27\n",
            "27\n",
            "JOSEE\n",
            "27\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "27\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "28\n",
            "28\n",
            "JOSEE\n",
            "28\n",
            "JOSEE\n",
            "28\n",
            "28\n",
            "28\n",
            "28\n",
            "28\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "29\n",
            "29\n",
            "29\n",
            "29\n",
            "29\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "29\n",
            "29\n",
            "JOSEE\n",
            "JOSEE\n",
            "29\n",
            "JOSEE\n",
            "JOSEE\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "JOSEE\n",
            "JOSEE\n",
            "30\n",
            "30\n",
            "JOSEE\n",
            "JOSEE\n",
            "30\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "31\n",
            "31\n",
            "31\n",
            "JOSEE\n",
            "JOSEE\n",
            "31\n",
            "31\n",
            "31\n",
            "31\n",
            "JOSEE\n",
            "JOSEE\n",
            "31\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "JOSEE\n",
            "32\n",
            "JOSEE\n",
            "32\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "33\n",
            "33\n",
            "JOSEE\n",
            "33\n",
            "33\n",
            "33\n",
            "33\n",
            "33\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "33\n",
            "JOSEE\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "JOSEE\n",
            "34\n",
            "34\n",
            "JOSEE\n",
            "JOSEE\n",
            "34\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "34\n",
            "JOSEE\n",
            "JOSEE\n",
            "35\n",
            "35\n",
            "35\n",
            "35\n",
            "35\n",
            "JOSEE\n",
            "35\n",
            "JOSEE\n",
            "JOSEE\n",
            "35\n",
            "JOSEE\n",
            "35\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "36\n",
            "36\n",
            "36\n",
            "36\n",
            "36\n",
            "36\n",
            "36\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "36\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "37\n",
            "37\n",
            "37\n",
            "37\n",
            "37\n",
            "JOSEE\n",
            "37\n",
            "JOSEE\n",
            "37\n",
            "37\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "38\n",
            "38\n",
            "38\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "38\n",
            "38\n",
            "38\n",
            "38\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "38\n",
            "JOSEE\n",
            "JOSEE\n",
            "39\n",
            "39\n",
            "39\n",
            "39\n",
            "39\n",
            "39\n",
            "JOSEE\n",
            "39\n",
            "JOSEE\n",
            "JOSEE\n",
            "39\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "40\n",
            "40\n",
            "40\n",
            "40\n",
            "40\n",
            "40\n",
            "40\n",
            "40\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "[xla:2](40) Loss=0.57332 Rate=534.18 GlobalRate=125.44 Time=Wed May 26 05:43:14 2021\n",
            "41\n",
            "[xla:5](40) Loss=0.61504 Rate=511.17 GlobalRate=105.37 Time=Wed May 26 05:43:14 2021\n",
            "[xla:3](40) Loss=0.60279 Rate=532.73 GlobalRate=133.79 Time=Wed May 26 05:43:14 2021\n",
            "[xla:0](40) Loss=0.53591 Rate=508.62 GlobalRate=102.10 Time=Wed May 26 05:43:14 2021\n",
            "[xla:6](40) Loss=0.59802 Rate=511.45 GlobalRate=109.08 Time=Wed May 26 05:43:14 2021\n",
            "[xla:1](40) Loss=0.54662 Rate=534.38 GlobalRate=146.02 Time=Wed May 26 05:43:14 2021\n",
            "[xla:4](40) Loss=0.54130 Rate=521.97 GlobalRate=113.48 Time=Wed May 26 05:43:14 2021\n",
            "[xla:7](40) Loss=0.58633 Rate=530.32 GlobalRate=140.11 Time=Wed May 26 05:43:14 2021\n",
            "JOSEE\n",
            "41\n",
            "41\n",
            "41\n",
            "41\n",
            "41\n",
            "41\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "41\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "42\n",
            "42\n",
            "42\n",
            "42\n",
            "42\n",
            "42\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "42\n",
            "42\n",
            "JOSEE\n",
            "JOSEE\n",
            "43\n",
            "43\n",
            "43\n",
            "43\n",
            "43\n",
            "43\n",
            "43\n",
            "43\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "44\n",
            "44\n",
            "44\n",
            "44\n",
            "44\n",
            "44\n",
            "44\n",
            "44\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "45\n",
            "45\n",
            "45\n",
            "45\n",
            "45\n",
            "45\n",
            "45\n",
            "45\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "46\n",
            "46\n",
            "46\n",
            "46\n",
            "JOSEE\n",
            "46\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "46\n",
            "46\n",
            "46\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "47\n",
            "47\n",
            "47\n",
            "47\n",
            "47\n",
            "47\n",
            "47\n",
            "47\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "48\n",
            "48\n",
            "48\n",
            "48\n",
            "JOSEE\n",
            "JOSEE\n",
            "48\n",
            "48\n",
            "48\n",
            "48\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "49\n",
            "49\n",
            "49\n",
            "49\n",
            "49\n",
            "JOSEE\n",
            "JOSEE\n",
            "49\n",
            "JOSEE\n",
            "JOSEE\n",
            "49\n",
            "JOSEE\n",
            "49\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "50\n",
            "50\n",
            "50\n",
            "50\n",
            "JOSEE\n",
            "50\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "50\n",
            "JOSEE\n",
            "50\n",
            "50\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "51\n",
            "51\n",
            "51\n",
            "51\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "51\n",
            "51\n",
            "JOSEE\n",
            "51\n",
            "51\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "52\n",
            "52\n",
            "52\n",
            "52\n",
            "52\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "52\n",
            "52\n",
            "52\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "53\n",
            "53\n",
            "53\n",
            "53\n",
            "53\n",
            "53\n",
            "53\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "53\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "54\n",
            "54\n",
            "54\n",
            "JOSEE\n",
            "54\n",
            "JOSEE\n",
            "54\n",
            "54\n",
            "54\n",
            "54\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "55\n",
            "JOSEE\n",
            "JOSEE\n",
            "55\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "56\n",
            "56\n",
            "56\n",
            "JOSEE\n",
            "JOSEE\n",
            "56\n",
            "56\n",
            "56\n",
            "JOSEE\n",
            "JOSEE\n",
            "56\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "56\n",
            "JOSEE\n",
            "57\n",
            "57\n",
            "57\n",
            "57\n",
            "57\n",
            "57\n",
            "57\n",
            "JOSEE\n",
            "JOSEE\n",
            "57\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "JOSEE\n",
            "58\n",
            "58\n",
            "58\n",
            "58\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "59\n",
            "59\n",
            "59\n",
            "59\n",
            "59\n",
            "59\n",
            "59\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "59\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "60\n",
            "60\n",
            "60\n",
            "60\n",
            "60\n",
            "60\n",
            "JOSEE\n",
            "60\n",
            "60\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "[xla:0](60) Loss=0.64729 Rate=697.37 GlobalRate=143.23 Time=Wed May 26 05:43:15 2021\n",
            "[xla:4](60) Loss=0.75951 Rate=702.79 GlobalRate=158.19 Time=Wed May 26 05:43:15 2021\n",
            "[xla:7](60) Loss=0.57449 Rate=706.33 GlobalRate=192.48 Time=Wed May 26 05:43:15 2021\n",
            "[xla:3](60) Loss=0.79728 Rate=705.99 GlobalRate=184.40 Time=Wed May 26 05:43:15 2021\n",
            "[xla:1](60) Loss=0.66786 Rate=706.76 GlobalRate=199.91 Time=Wed May 26 05:43:15 2021\n",
            "[xla:5](60) Loss=0.71136 Rate=696.92 GlobalRate=147.54 Time=Wed May 26 05:43:15 2021\n",
            "[xla:6](60) Loss=0.61420 Rate=697.39 GlobalRate=152.42 Time=Wed May 26 05:43:15 2021\n",
            "[xla:2](60) Loss=0.64672 Rate=704.32 GlobalRate=173.64 Time=Wed May 26 05:43:15 2021\n",
            "61\n",
            "61\n",
            "JOSEE\n",
            "61\n",
            "61\n",
            "JOSEE\n",
            "61\n",
            "JOSEE\n",
            "61\n",
            "61\n",
            "JOSEE\n",
            "61\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "62\n",
            "62\n",
            "62\n",
            "62\n",
            "62\n",
            "62\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "62\n",
            "JOSEE\n",
            "JOSEE\n",
            "62\n",
            "JOSEE\n",
            "JOSEE\n",
            "63\n",
            "63\n",
            "63\n",
            "63\n",
            "63\n",
            "63\n",
            "63\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "63\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "JOSEE\n",
            "JOSEE\n",
            "64\n",
            "JOSEE\n",
            "64\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "65\n",
            "65\n",
            "65\n",
            "65\n",
            "65\n",
            "65\n",
            "JOSEE\n",
            "65\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "65\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "66\n",
            "66\n",
            "66\n",
            "66\n",
            "66\n",
            "66\n",
            "66\n",
            "JOSEE\n",
            "JOSEE\n",
            "66\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "67\n",
            "67\n",
            "67\n",
            "67\n",
            "67\n",
            "67\n",
            "67\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "67\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "68\n",
            "68\n",
            "68\n",
            "68\n",
            "JOSEE\n",
            "JOSEE\n",
            "68\n",
            "JOSEE\n",
            "68\n",
            "68\n",
            "68\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "69\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "JOSEE\n",
            "70\n",
            "70\n",
            "70\n",
            "70\n",
            "70\n",
            "JOSEE\n",
            "70\n",
            "70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-97f9d40af0ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n\u001b[0;32m---> 12\u001b[0;31m           start_method='fork')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m         ready = multiprocessing.connection.wait(\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wt7wEVJoFmf"
      },
      "source": [
        "## Visualize Predictions"
      ]
    }
  ]
}