{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PAWS_DR_fine_tune_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/PAWS_DR_fine_tune_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5hlD-M-AULJ",
        "outputId": "2da33312-8b0d-49a6-8e8b-c2df75925ce3"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install --quiet -v --no-cache-dir ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/content/apex\n",
            "Processing /content/apex\n",
            "Building wheels for collected packages: apex\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp37-none-any.whl size=204709 sha256=0c548200d5145154af1c0389b9aec007902f337b8951bec325b3d931b431d61f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c6jhcdzb/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Found existing installation: apex 0.1\n",
            "    Uninstalling apex-0.1:\n",
            "      Successfully uninstalled apex-0.1\n",
            "Successfully installed apex-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCL0T19BAhwZ",
        "outputId": "284649d6-4ee0-4dd9-ea7e-6612daa83c6e"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmQET3rEAknO"
      },
      "source": [
        "!pip install --quiet -U PyYAML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxKLx9eqAndQ",
        "outputId": "48dd6705-3c28-49f0-b349-2d56a61ef8cd"
      },
      "source": [
        "!git clone -b feature/DR-images-v2 https://github.com/jmarrietar/suncet.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'suncet'...\n",
            "remote: Enumerating objects: 286, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
            "remote: Total 286 (delta 166), reused 216 (delta 102), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (286/286), 1.10 MiB | 3.70 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c0W3iBTApIP",
        "outputId": "d7af7103-2b05-41f7-bef0-a24b96076ab8"
      },
      "source": [
        "cd suncet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/suncet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U4gvRnhMAqWJ",
        "outputId": "b35c4d04-3e22-4820-b7c0-3c9ff1b2d680"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/suncet'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4On1j-ZxAr2v",
        "outputId": "21a04f85-e59e-4519-f8a5-668e21b856e3"
      },
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Check whether the file is already in the desired path or if it needs to be downloaded\n",
        "# File downloaded from source : https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
        "\n",
        "base_path = '/content/suncet/datasets/dr/'\n",
        "file_path = 'sample@1000.zip'\n",
        "\n",
        "if not os.path.isfile(base_path + file_path):\n",
        "    subprocess.run(['mkdir', '-p', base_path])\n",
        "    subprocess.run(['mkdir', '-p', 'logs'])\n",
        "    subprocess.call(['python', 'download.py', '-d', file_path.split('.')[0]])\n",
        "else:\n",
        "    print('File already downloaded!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File already downloaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln-Ge4v-A_yP"
      },
      "source": [
        "`main.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZjEepEFBCQF",
        "outputId": "cae7f4c4-11ab-46b4-8830-2f628d9a1a3d"
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "import pprint\n",
        "import yaml\n",
        "\n",
        "from src.paws_train import main as paws\n",
        "from src.suncet_train import main as suncet\n",
        "from src.fine_tune import main as fine_tune\n",
        "from src.snn_fine_tune import main as snn_fine_tune\n",
        "\n",
        "from src.utils import init_distributed\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    \"--fname\", type=str, help=\"name of config file to load\", default=\"configs.yaml\"\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--devices\",\n",
        "    type=str,\n",
        "    nargs=\"+\",\n",
        "    default=[\"cuda:0\"],\n",
        "    help=\"which devices to use on local machine\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--sel\",\n",
        "    type=str,\n",
        "    help=\"which script to run\",\n",
        "    choices=[\"paws_train\", \"suncet_train\", \"fine_tune\", \"snn_fine_tune\"],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--sel'], dest='sel', nargs=None, const=None, default=None, type=<class 'str'>, choices=['paws_train', 'suncet_train', 'fine_tune', 'snn_fine_tune'], help='which script to run', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS_6NXCIBFzI"
      },
      "source": [
        "args = parser.parse_args(['--sel', 'fine_tune',\n",
        "                          '--fname', 'configs/paws/dr_fine_tune.yaml'\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0G_CvPcBrD9"
      },
      "source": [
        "fname = args.fname\n",
        "sel = args.sel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YHQTaIUBrGw",
        "outputId": "10a3ada5-00f3-4924-f667-007b555003f4"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "\n",
        "logger.info(f'called-params {sel} {fname}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:called-params fine_tune configs/paws/dr_fine_tune.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guYIscowBrJs"
      },
      "source": [
        "rank = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyCicIMIBrMx",
        "outputId": "631eba70-0a3f-4e10-9dda-f3e143b4abe1"
      },
      "source": [
        "# -- load script params\n",
        "params = None\n",
        "with open(fname, 'r') as y_file:\n",
        "    params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
        "    logger.info('loaded params...')\n",
        "    if rank == 0:\n",
        "        pp = pprint.PrettyPrinter(indent=4)\n",
        "        pp.pprint(params)\n",
        "\n",
        "if rank == 0:\n",
        "    dump = os.path.join(params['logging']['folder'], f'params-{sel}.yaml')\n",
        "    with open(dump, 'w') as f:\n",
        "        yaml.dump(params, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loaded params...\n",
            "{   'data': {   'data_seed': 152,\n",
            "                'dataset': 'dr_fine_tune',\n",
            "                'image_folder': 'dr/sample@1000/',\n",
            "                'normalize': True,\n",
            "                'num_classes': 1000,\n",
            "                'root_path': 'datasets/',\n",
            "                'subset_path': 'dr_subsets/',\n",
            "                'unlabeled_frac': 0.9},\n",
            "    'logging': {   'folder': 'logs/',\n",
            "                   'pretrain_path': 'paws-latest.pth.tar',\n",
            "                   'write_tag': 'paws-latest-SNN'},\n",
            "    'meta': {   'copy_data': True,\n",
            "                'device': 'cuda:0',\n",
            "                'load_checkpoint': False,\n",
            "                'master_port': 4029,\n",
            "                'model_name': 'resnet50',\n",
            "                'training': True,\n",
            "                'use_fp16': True},\n",
            "    'optimization': {   'epochs': 50,\n",
            "                        'lr': 0.02,\n",
            "                        'use_lars': False,\n",
            "                        'weight_decay': 0.0,\n",
            "                        'zero_init': True}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBZ_cA-KBrO3"
      },
      "source": [
        "args = params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqU77odjG_82",
        "outputId": "8fcc5fbf-2862-488d-9c68-cf0236f977a0"
      },
      "source": [
        "args"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': {'data_seed': 152,\n",
              "  'dataset': 'dr_fine_tune',\n",
              "  'image_folder': 'dr/sample@1000/',\n",
              "  'normalize': True,\n",
              "  'num_classes': 1000,\n",
              "  'root_path': 'datasets/',\n",
              "  'subset_path': 'dr_subsets/',\n",
              "  'unlabeled_frac': 0.9},\n",
              " 'logging': {'folder': 'logs/',\n",
              "  'pretrain_path': 'paws-latest.pth.tar',\n",
              "  'write_tag': 'paws-latest-SNN'},\n",
              " 'meta': {'copy_data': True,\n",
              "  'device': 'cuda:0',\n",
              "  'load_checkpoint': False,\n",
              "  'master_port': 4029,\n",
              "  'model_name': 'resnet50',\n",
              "  'training': True,\n",
              "  'use_fp16': True},\n",
              " 'optimization': {'epochs': 50,\n",
              "  'lr': 0.02,\n",
              "  'use_lars': False,\n",
              "  'weight_decay': 0.0,\n",
              "  'zero_init': True}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a9EGKyBzEG"
      },
      "source": [
        "# FINE TUNE TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mQKJg0ABrSi"
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "\n",
        "# -- FOR DISTRIBUTED TRAINING ENSURE ONLY 1 DEVICE VISIBLE PER PROCESS\n",
        "try:\n",
        "    # -- WARNING: IF DOING DISTRIBUTED TRAINING ON A NON-SLURM CLUSTER, MAKE\n",
        "    # --          SURE TO UPDATE THIS TO GET LOCAL-RANK ON NODE, OR ENSURE\n",
        "    # --          THAT YOUR JOBS ARE LAUNCHED WITH ONLY 1 DEVICE VISIBLE\n",
        "    # --          TO EACH PROCESS\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = os.environ['SLURM_LOCALID']\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import copy\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import src.resnet as resnet\n",
        "import src.wide_resnet as wide_resnet\n",
        "from src.utils import (\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule\n",
        ")\n",
        "from src.data_manager import (\n",
        "    init_data,\n",
        "    make_transforms\n",
        ")\n",
        "from src.sgd import SGD\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from src.lars import LARS\n",
        "\n",
        "# --\n",
        "log_timings = True\n",
        "log_freq = 10\n",
        "checkpoint_freq = 50\n",
        "# --\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "np.random.seed(_GLOBAL_SEED)\n",
        "torch.manual_seed(_GLOBAL_SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logger = logging.getLogger()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh8OlmleBrXj"
      },
      "source": [
        "def load_pretrained(\n",
        "    r_path,\n",
        "    encoder,\n",
        "    device_str\n",
        "):\n",
        "    checkpoint = torch.load(r_path, map_location='cpu')\n",
        "    pretrained_dict = {k.replace('module.', ''): v for k, v in checkpoint['encoder'].items()}\n",
        "    for k, v in encoder.state_dict().items():\n",
        "        if k not in pretrained_dict:\n",
        "            logger.info(f'key \"{k}\" could not be found in loaded state dict')\n",
        "        elif pretrained_dict[k].shape != v.shape:\n",
        "            logger.info(f'key \"{k}\" is of different shape in model and loaded state dict')\n",
        "            pretrained_dict[k] = v\n",
        "    msg = encoder.load_state_dict(pretrained_dict, strict=False)\n",
        "    logger.info(f'loaded pretrained model with msg: {msg}')\n",
        "    logger.info(f'loaded pretrained encoder from epoch: {checkpoint[\"epoch\"]} '\n",
        "                f'path: {r_path}')\n",
        "    del checkpoint\n",
        "    return encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZkNsaIrBrbD"
      },
      "source": [
        "def load_from_path(\n",
        "    r_path,\n",
        "    encoder,\n",
        "    opt,\n",
        "    sched,\n",
        "    scaler,\n",
        "    device_str,\n",
        "    use_fp16=False\n",
        "):\n",
        "    encoder = load_pretrained(r_path, encoder, device_str)\n",
        "    checkpoint = torch.load(r_path, map_location=device_str)\n",
        "\n",
        "    best_acc = None\n",
        "    if 'best_top1_acc' in checkpoint:\n",
        "        best_acc = checkpoint['best_top1_acc']\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    if opt is not None:\n",
        "        if use_fp16:\n",
        "            scaler.load_state_dict(checkpoint['amp'])\n",
        "        opt.load_state_dict(checkpoint['opt'])\n",
        "        sched.load_state_dict(checkpoint['sched'])\n",
        "        logger.info(f'loaded optimizers from epoch {epoch}')\n",
        "    logger.info(f'read-path: {r_path}')\n",
        "    del checkpoint\n",
        "    return encoder, opt, sched, epoch, best_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfV-VdVHBrdQ"
      },
      "source": [
        "def init_model(\n",
        "    device,\n",
        "    device_str,\n",
        "    num_classes,\n",
        "    training,\n",
        "    use_fp16,\n",
        "    r_enc_path,\n",
        "    iterations_per_epoch,\n",
        "    world_size,\n",
        "    ref_lr,\n",
        "    num_epochs,\n",
        "    use_lars=False,\n",
        "    zero_init=True,\n",
        "    model_name='resnet50',\n",
        "    warmup_epochs=0,\n",
        "    weight_decay=0\n",
        "):\n",
        "    # -- init model\n",
        "    if 'wide_resnet' in model_name:\n",
        "        encoder = wide_resnet.__dict__[model_name](dropout_rate=0.0)\n",
        "        hidden_dim = 128\n",
        "    else:\n",
        "        encoder = resnet.__dict__[model_name]()\n",
        "        hidden_dim = 2048\n",
        "        if 'w2' in model_name:\n",
        "            hidden_dim *= 2\n",
        "        elif 'w4' in model_name:\n",
        "            hidden_dim *= 4\n",
        "\n",
        "    # -- projection head\n",
        "    encoder.fc = torch.nn.Sequential(OrderedDict([\n",
        "        ('fc1', torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "        ('bn1', torch.nn.BatchNorm1d(hidden_dim)),\n",
        "        ('relu1', torch.nn.ReLU(inplace=True)),\n",
        "        ('fc2', torch.nn.Linear(hidden_dim, 1)), # YO changed\n",
        "        ('sg', torch.nn.Sigmoid())  # YO \n",
        "    ]))\n",
        "\n",
        "    encoder.to(device)\n",
        "    encoder = load_pretrained(\n",
        "        r_path=r_enc_path,\n",
        "        encoder=encoder,\n",
        "        device_str=device_str)\n",
        "\n",
        "    if zero_init:\n",
        "        for p in encoder.fc.fc2.parameters():\n",
        "            torch.nn.init.zeros_(p)\n",
        "\n",
        "    # -- init optimizer\n",
        "    optimizer, scheduler = None, None\n",
        "    if training:\n",
        "        param_groups = [\n",
        "            {'params': (p for n, p in encoder.named_parameters()\n",
        "                        if ('bias' not in n) and ('bn' not in n))},\n",
        "            {'params': (p for n, p in encoder.named_parameters()\n",
        "                        if ('bias' in n) or ('bn' in n)),\n",
        "             'LARS_exclude': True,\n",
        "             'weight_decay': 0}\n",
        "        ]\n",
        "        optimizer = SGD(\n",
        "            param_groups,\n",
        "            nesterov=True,\n",
        "            weight_decay=weight_decay,\n",
        "            momentum=0.9,\n",
        "            lr=ref_lr)\n",
        "        scheduler = WarmupCosineSchedule(\n",
        "            optimizer,\n",
        "            warmup_steps=warmup_epochs*iterations_per_epoch,\n",
        "            start_lr=ref_lr,\n",
        "            ref_lr=ref_lr,\n",
        "            T_max=num_epochs*iterations_per_epoch)\n",
        "        if use_lars:\n",
        "            optimizer = LARS(optimizer, trust_coefficient=0.001)\n",
        "    if world_size > 1:\n",
        "        encoder = DistributedDataParallel(encoder, broadcast_buffers=False)\n",
        "\n",
        "    return encoder, optimizer, scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq3v7zFLBrgG"
      },
      "source": [
        "# -- META\n",
        "model_name = args['meta']['model_name']\n",
        "port = args['meta']['master_port']\n",
        "load_checkpoint = args['meta']['load_checkpoint']\n",
        "training = args['meta']['training']\n",
        "copy_data = args['meta']['copy_data']\n",
        "use_fp16 = args['meta']['use_fp16']\n",
        "device = torch.device(args['meta']['device'])\n",
        "torch.cuda.set_device(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os4hgZObBrrP"
      },
      "source": [
        "# -- DATA\n",
        "unlabeled_frac = args['data']['unlabeled_frac']\n",
        "normalize = args['data']['normalize']\n",
        "root_path = args['data']['root_path']\n",
        "image_folder = args['data']['image_folder']\n",
        "dataset_name = args['data']['dataset']\n",
        "subset_path = args['data']['subset_path']\n",
        "num_classes = args['data']['num_classes']\n",
        "data_seed = None\n",
        "if 'cifar10' in dataset_name:\n",
        "    data_seed = args['data']['data_seed']\n",
        "crop_scale = (0.5, 1.0) if 'cifar10' in dataset_name else (0.08, 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1zEZGGQBrw3"
      },
      "source": [
        "# -- OPTIMIZATION\n",
        "wd = float(args['optimization']['weight_decay'])\n",
        "ref_lr = args['optimization']['lr']\n",
        "use_lars = args['optimization']['use_lars']\n",
        "zero_init = args['optimization']['zero_init']\n",
        "num_epochs = args['optimization']['epochs']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT9t_7siDEiA"
      },
      "source": [
        "# -- LOGGING\n",
        "folder = args['logging']['folder']\n",
        "tag = args['logging']['write_tag']\n",
        "r_file_enc = args['logging']['pretrain_path']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12zIU-oxDEqO"
      },
      "source": [
        "# -- log/checkpointing paths\n",
        "r_enc_path = os.path.join(folder, r_file_enc)\n",
        "w_enc_path = os.path.join(folder, f'{tag}-fine-tune.pth.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQX97cCGDEtl",
        "outputId": "a1d3a7f4-5ddd-4fe0-e485-3f6ce206789b"
      },
      "source": [
        "# -- init distributed\n",
        "world_size, rank = init_distributed(port)\n",
        "logger.info(f'initialized rank/world-size: {rank}/{world_size}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:distributed training not available\n",
            "INFO:root:initialized rank/world-size: 0/1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgKbHSR2DEwa"
      },
      "source": [
        "# -- optimization/evaluation params\n",
        "if training:\n",
        "    batch_size = 32\n",
        "else:\n",
        "    batch_size = 16\n",
        "    unlabeled_frac = 0.0\n",
        "    load_checkpoint = True\n",
        "    num_epochs = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRpDSN1zDEz8"
      },
      "source": [
        "# -- init loss\n",
        "#criterion = torch.nn.CrossEntropyLoss() \n",
        "criterion = torch.nn.BCELoss() # YO CHANGED HERE\n",
        "#criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROxh1sWHDE3L",
        "outputId": "84f44bc2-2639-42fc-c387-4ea197ad8e71"
      },
      "source": [
        "# -- make train data transforms and data loaders/samples\n",
        "transform, init_transform = make_transforms(\n",
        "    dataset_name=dataset_name,\n",
        "    subset_path=subset_path,\n",
        "    unlabeled_frac=unlabeled_frac,\n",
        "    training=training,\n",
        "    crop_scale=crop_scale,\n",
        "    split_seed=data_seed,\n",
        "    basic_augmentations=True,\n",
        "    normalize=normalize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:making imagenet data transforms\n",
            "INFO:root:keep file: dr_subsets/90percent.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WZOnPx7DE-P",
        "outputId": "9baaa4da-e1c6-4767-b12d-955677f0dcf9"
      },
      "source": [
        "(data_loader,\n",
        "    dist_sampler) = init_data(\n",
        "        dataset_name=dataset_name,\n",
        "        transform=transform,\n",
        "        init_transform=init_transform,\n",
        "        u_batch_size=None,\n",
        "        s_batch_size=batch_size,\n",
        "        classes_per_batch=2,\n",
        "        world_size=world_size,\n",
        "        rank=rank,\n",
        "        root_path=root_path,\n",
        "        image_folder=image_folder,\n",
        "        training=training,\n",
        "        copy_data=copy_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:copying data locally\n",
            "INFO:root:No job-id, will load directly from network file\n",
            "INFO:root:data-path datasets/dr/sample@1000/train/\n",
            "INFO:root:Initialized ImageDR\n",
            "INFO:root:ImageNet fine-tune dataset created\n",
            "self.multicrop_transform (0, None)\n",
            "INFO:root:flipping coin to keep labels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1ZaEXJ0DFCc",
        "outputId": "83557d31-fb5c-49b0-e75d-334db81ba450"
      },
      "source": [
        "ipe = len(data_loader)\n",
        "logger.info(f'initialized data-loader (ipe {ipe})')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:initialized data-loader (ipe 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr3kuterDFL7",
        "outputId": "2063cbd2-c812-4918-cf86-bb77a025304e"
      },
      "source": [
        "# -- init model and optimizer\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "encoder, optimizer, scheduler = init_model(\n",
        "    device=device,\n",
        "    device_str=args['meta']['device'],\n",
        "    num_classes=num_classes,\n",
        "    training=training,\n",
        "    use_fp16=use_fp16,\n",
        "    r_enc_path=r_enc_path,\n",
        "    iterations_per_epoch=ipe,\n",
        "    world_size=world_size,\n",
        "    ref_lr=ref_lr,\n",
        "    weight_decay=wd,\n",
        "    use_lars=use_lars,\n",
        "    zero_init=zero_init,\n",
        "    num_epochs=num_epochs,\n",
        "    model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:key \"fc.fc2.weight\" is of different shape in model and loaded state dict\n",
            "INFO:root:key \"fc.fc2.bias\" is of different shape in model and loaded state dict\n",
            "INFO:root:loaded pretrained model with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['pred.bn1.weight', 'pred.bn1.bias', 'pred.bn1.running_mean', 'pred.bn1.running_var', 'pred.bn1.num_batches_tracked', 'pred.fc1.weight', 'pred.fc1.bias', 'pred.bn2.weight', 'pred.bn2.bias', 'pred.bn2.running_mean', 'pred.bn2.running_var', 'pred.bn2.num_batches_tracked', 'pred.fc2.weight', 'pred.fc2.bias', 'fc.bn2.weight', 'fc.bn2.bias', 'fc.bn2.running_mean', 'fc.bn2.running_var', 'fc.bn2.num_batches_tracked', 'fc.fc3.weight', 'fc.fc3.bias'])\n",
            "INFO:root:loaded pretrained encoder from epoch: 10 path: logs/paws-latest.pth.tar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO78XesVOKdA",
        "outputId": "3db6f91d-3a49-40fa-da43-ca8e4a72e132"
      },
      "source": [
        "encoder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU(inplace=True)\n",
              "    (fc2): Linear(in_features=2048, out_features=1, bias=True)\n",
              "    (sg): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG0qD6-iDFTW"
      },
      "source": [
        "best_acc = None\n",
        "start_epoch = 0\n",
        "# -- load checkpoint\n",
        "if not training or load_checkpoint:\n",
        "    encoder, optimizer, scheduler, start_epoch, best_acc = load_from_path(\n",
        "        r_path=w_enc_path,\n",
        "        encoder=encoder,\n",
        "        opt=optimizer,\n",
        "        sched=scheduler,\n",
        "        scaler=scaler,\n",
        "        device_str=args['meta']['device'],\n",
        "        use_fp16=use_fp16)\n",
        "if not training:\n",
        "    logger.info('putting model in eval mode')\n",
        "    encoder.eval()\n",
        "    logger.info(sum(p.numel() for n, p in encoder.named_parameters()\n",
        "                    if p.requires_grad and ('fc' not in n)))\n",
        "    start_epoch = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y4gAC5mOQ7s"
      },
      "source": [
        "num_epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYRdk6r0DFVk",
        "outputId": "419948d7-0b57-4fe0-8478-bcdbc8ef97c8"
      },
      "source": [
        "for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "    def train_step():\n",
        "        # -- update distributed-data-loader epoch\n",
        "        top1_correct, total = 0, 0\n",
        "        for i, data in enumerate(data_loader):\n",
        "            with torch.cuda.amp.autocast(enabled=False): # Yo\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "                labels = labels.unsqueeze(1) ## YO\n",
        "                labels = labels.float() ## YO\n",
        "\n",
        "                outputs = encoder(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            total += inputs.shape[0]\n",
        "            top1_correct += float(sum(((outputs>0.5)*1 == labels)*1))\n",
        "            top1_acc = 100. * (top1_correct / total)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            if i % log_freq == 0:\n",
        "                logger.info('[%d, %5d] %.3f%% (loss: %.3f)'\n",
        "                            % (epoch + 1, i, top1_acc, loss))\n",
        "        return 100. * (top1_correct / total)\n",
        "\n",
        "    train_top1 = 0.\n",
        "    train_top1 = train_step()\n",
        "\n",
        "    log_str = 'train:'\n",
        "    logger.info('[%d] (%s: %.3f%%) '\n",
        "                % (epoch + 1, log_str, train_top1))\n",
        "\n",
        "    # -- logging/checkpointing\n",
        "    if rank == 0:\n",
        "\n",
        "        save_dict = {\n",
        "            'encoder': encoder.state_dict(),\n",
        "            'opt': optimizer.state_dict(),\n",
        "            'sched': scheduler.state_dict(),\n",
        "            'epoch': epoch + 1,\n",
        "            'unlabel_prob': unlabeled_frac,\n",
        "            'world_size': world_size,\n",
        "            'batch_size': batch_size,\n",
        "            'lr': ref_lr,\n",
        "            'amp': scaler.state_dict()\n",
        "        }\n",
        "        torch.save(save_dict, w_enc_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:[1,     0] 50.000% (loss: 1.057)\n",
            "INFO:root:[1] (train:: 50.000%) \n",
            "INFO:root:[2,     0] 48.438% (loss: 0.949)\n",
            "INFO:root:[2] (train:: 48.438%) \n",
            "INFO:root:[3,     0] 54.688% (loss: 0.714)\n",
            "INFO:root:[3] (train:: 54.688%) \n",
            "INFO:root:[4,     0] 51.562% (loss: 1.286)\n",
            "INFO:root:[4] (train:: 51.562%) \n",
            "INFO:root:[5,     0] 50.000% (loss: 1.317)\n",
            "INFO:root:[5] (train:: 50.000%) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3H-iYM4MTnD"
      },
      "source": [
        "## Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P5UUGnrhTkf2",
        "outputId": "6e7e004a-5035-4133-8e6d-0b9080db88e9"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjNgGdzpM5Ei",
        "outputId": "0d5be30a-d043-45c7-ef2a-8b4edd88ea46"
      },
      "source": [
        "cd suncet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/suncet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnzF3XuAMXFW",
        "outputId": "7e362d16-b51d-4714-b0c3-7e6afc7f6f46"
      },
      "source": [
        "!python main.py --sel fine_tune --fname configs/paws/dr_fine_tune.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:called-params fine_tune configs/paws/dr_fine_tune.yaml\n",
            "INFO:root:loaded params...\n",
            "{   'data': {   'data_seed': 152,\n",
            "                'dataset': 'dr_fine_tune',\n",
            "                'image_folder': 'dr/sample@1000/',\n",
            "                'normalize': True,\n",
            "                'num_classes': 1000,\n",
            "                'root_path': 'datasets/',\n",
            "                'subset_path': 'dr_subsets/',\n",
            "                'unlabeled_frac': 0.9},\n",
            "    'logging': {   'folder': 'logs/',\n",
            "                   'pretrain_path': 'paws-latest.pth.tar',\n",
            "                   'write_tag': 'paws-latest-SNN'},\n",
            "    'meta': {   'copy_data': True,\n",
            "                'device': 'cuda:0',\n",
            "                'load_checkpoint': False,\n",
            "                'master_port': 4029,\n",
            "                'model_name': 'resnet50',\n",
            "                'training': True,\n",
            "                'use_fp16': True},\n",
            "    'optimization': {   'epochs': 5,\n",
            "                        'lr': 0.02,\n",
            "                        'use_lars': False,\n",
            "                        'weight_decay': 0.0,\n",
            "                        'zero_init': True}}\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for 1 nodes.\n",
            "INFO:root:Running fine_tune (rank: 0/1)\n",
            "INFO:root:initialized rank/world-size: 0/1\n",
            "INFO:root:making imagenet data transforms\n",
            "INFO:root:keep file: dr_subsets/90percent.txt\n",
            "INFO:root:copying data locally\n",
            "INFO:root:No job-id, will load directly from network file\n",
            "INFO:root:data-path datasets/dr/sample@1000/train/\n",
            "INFO:root:Initialized ImageDR\n",
            "INFO:root:ImageNet fine-tune dataset created\n",
            "self.multicrop_transform (0, None)\n",
            "INFO:root:flipping coin to keep labels\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "INFO:root:initialized data-loader (ipe 3)\n",
            "INFO:root:key \"fc.fc2.weight\" is of different shape in model and loaded state dict\n",
            "INFO:root:key \"fc.fc2.bias\" is of different shape in model and loaded state dict\n",
            "INFO:root:loaded pretrained model with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['pred.bn1.weight', 'pred.bn1.bias', 'pred.bn1.running_mean', 'pred.bn1.running_var', 'pred.bn1.num_batches_tracked', 'pred.fc1.weight', 'pred.fc1.bias', 'pred.bn2.weight', 'pred.bn2.bias', 'pred.bn2.running_mean', 'pred.bn2.running_var', 'pred.bn2.num_batches_tracked', 'pred.fc2.weight', 'pred.fc2.bias', 'fc.bn2.weight', 'fc.bn2.bias', 'fc.bn2.running_mean', 'fc.bn2.running_var', 'fc.bn2.num_batches_tracked', 'fc.fc3.weight', 'fc.fc3.bias'])\n",
            "INFO:root:loaded pretrained encoder from epoch: 10 path: logs/paws-latest.pth.tar\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "INFO:root:[1,     0] 50.000% (loss: 0.693)\n",
            "INFO:root:[1] (train:: 46.875%) \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "INFO:root:[2,     0] 56.250% (loss: 1.090)\n",
            "INFO:root:[2] (train:: 50.000%) \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "INFO:root:[3,     0] 50.000% (loss: 0.975)\n",
            "INFO:root:[3] (train:: 56.250%) \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "INFO:root:[4,     0] 50.000% (loss: 0.716)\n",
            "INFO:root:[4] (train:: 53.125%) \n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "INFO:root:[5,     0] 53.125% (loss: 0.741)\n",
            "INFO:root:[5] (train:: 54.167%) \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}