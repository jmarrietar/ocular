{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " PAWS_TPU-DR-PyTorch/XLA",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/PAWS_TPU_DR_PyTorch_XLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wF3c4xmKQofR",
        "outputId": "fbb25bc8-5c58-4338-87d7-15e957baa2b0"
      },
      "source": [
        "\"\"\"\n",
        "To Do: \n",
        "  - Dont Donwload several Times ResNet model. How?. \n",
        "\"\"\""
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTo Do: \\n  - Dont Donwload several Times ResNet model. How?. \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed60edb-3dc2-4e6e-c61b-aa97b5950729"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-xla==1.8.1\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl (145.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 145.0 MB 10 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.34.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.278 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy5ndp3nJPbD",
        "outputId": "783c2bec-f83c-4e15-8d63-d66b0b6048d0"
      },
      "source": [
        "!pip uninstall torch -y\n",
        "!pip install torch==1.8.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.9.0+cu102\n",
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd3xZvoA-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84720764-6145-498d-84d5-6ac357b3b295"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import argparse\n",
        "import yaml\n",
        "import pprint\n",
        "import logging\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.8.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_c5YRxZRjL1o",
        "outputId": "27c00db9-a2a2-4f37-edd5-c16848908672"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "mskoYvt0MCaq",
        "outputId": "5cd963c4-971c-4309-9ae0-8d4e37d3193f"
      },
      "source": [
        "pip install -U PyYAML"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l9EOMw3yQQlX",
        "outputId": "e9c7ee02-1e44-49f0-a148-fc9f76154ec2"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6KiMba_MHkU",
        "outputId": "cdbc11b3-54bd-4459-97b0-a0d10fdf9a94"
      },
      "source": [
        "!git clone -b feature/DR-images-v2 https://github.com/jmarrietar/suncet.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'suncet'...\n",
            "remote: Enumerating objects: 356, done.\u001b[K\n",
            "remote: Counting objects: 100% (356/356), done.\u001b[K\n",
            "remote: Compressing objects: 100% (235/235), done.\u001b[K\n",
            "remote: Total 356 (delta 215), reused 248 (delta 118), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (356/356), 1.11 MiB | 8.36 MiB/s, done.\n",
            "Resolving deltas: 100% (215/215), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtWmyuh4MK0J",
        "outputId": "48bf48e1-ebfa-4b48-d599-6e24885242ab"
      },
      "source": [
        "cd suncet"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/suncet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mKok-0lMK4s"
      },
      "source": [
        "!mkdir datasets\n",
        "!mkdir datasets/dr\n",
        "!mkdir logs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTLU6dWXMK9B",
        "outputId": "6fddfdc3-9b8a-4eb2-fe78-3555007e0ccc"
      },
      "source": [
        "!python download.py -d sample@2000"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\n",
            "To: /content/suncet/datasets/dr/sample@2000.zip\n",
            "214MB [00:00, 274MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pCIJ3u5MWBZ",
        "outputId": "a908f2c6-a06e-4db1-df3a-299a5efd1157"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--fname', type=str,\n",
        "    help='name of config file to load',\n",
        "    default='configs.yaml')\n",
        "parser.add_argument(\n",
        "    '--devices', type=str, nargs='+', default=['cuda:0'],\n",
        "    help='which devices to use on local machine')\n",
        "parser.add_argument(\n",
        "    '--sel', type=str,\n",
        "    help='which script to run',\n",
        "    choices=[\n",
        "        'paws_train',\n",
        "        'suncet_train',\n",
        "        'fine_tune',\n",
        "        'snn_fine_tune'\n",
        "    ])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--sel'], dest='sel', nargs=None, const=None, default=None, type=<class 'str'>, choices=['paws_train', 'suncet_train', 'fine_tune', 'snn_fine_tune'], help='which script to run', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "halcMg65MqwY"
      },
      "source": [
        "args = parser.parse_args(['--sel', 'paws_train',\n",
        "                            '--fname', 'configs/paws/dr_train.yaml'\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz8KUfZZPNlq"
      },
      "source": [
        "fname = args.fname\n",
        "sel = args.sel"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7G3xNSPPUrl"
      },
      "source": [
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "\n",
        "logger.info(f'called-params {sel} {fname}')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iRvWJmdPWqr",
        "outputId": "f899f54b-28e1-4d27-94e1-1fb71697c369"
      },
      "source": [
        "# -- load script params\n",
        "params = None\n",
        "with open(fname, 'r') as y_file:\n",
        "    #params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
        "    params = yaml.load(y_file)\n",
        "    logger.info('loaded params...')\n",
        "    #if rank == 0:\n",
        "    pp = pprint.PrettyPrinter(indent=4)\n",
        "    pp.pprint(params)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{   'criterion': {   'classes_per_batch': 2,\n",
            "                     'me_max': True,\n",
            "                     'sharpen': 0.25,\n",
            "                     'supervised_imgs_per_class': 8,\n",
            "                     'supervised_views': 1,\n",
            "                     'temperature': 0.1,\n",
            "                     'unsupervised_batch_size': 32},\n",
            "    'data': {   'color_jitter_strength': 1.0,\n",
            "                'data_seed': None,\n",
            "                'dataset': 'dr',\n",
            "                'label_smoothing': 0.1,\n",
            "                'multicrop': 6,\n",
            "                'normalize': True,\n",
            "                'root_path': 'datasets/',\n",
            "                's_image_folder': 'dr/sample@2000/',\n",
            "                'subset_path': 'dr_subsets',\n",
            "                'u_image_folder': 'dr/sample@2000/',\n",
            "                'unique_classes_per_rank': False,\n",
            "                'unlabeled_frac': 0.9},\n",
            "    'logging': {'folder': 'logs/', 'write_tag': 'paws'},\n",
            "    'meta': {   'copy_data': True,\n",
            "                'device': 'cuda:0',\n",
            "                'load_checkpoint': False,\n",
            "                'model_name': 'resnet50',\n",
            "                'output_dim': 2048,\n",
            "                'read_checkpoint': None,\n",
            "                'use_fp16': True,\n",
            "                'use_pred_head': True},\n",
            "    'optimization': {   'epochs': 100,\n",
            "                        'final_lr': 1e-05,\n",
            "                        'lr': 0.001,\n",
            "                        'momentum': 0.9,\n",
            "                        'nesterov': False,\n",
            "                        'start_lr': 0.001,\n",
            "                        'warmup': 10,\n",
            "                        'weight_decay': 1e-06}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IyTUPjAPYst"
      },
      "source": [
        "args = params"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdPRFXIn_jH"
      },
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 2\n",
        "FLAGS['learning_rate'] = 0.02\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False\n",
        "FLAGS['model_name'] = args[\"meta\"][\"model_name\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzhD4JoAS5MP"
      },
      "source": [
        "model_name = args[\"meta\"][\"model_name\"]\n",
        "output_dim = args[\"meta\"][\"output_dim\"]\n",
        "multicrop = args[\"data\"][\"multicrop\"]\n",
        "\n",
        "# -- CRITERTION\n",
        "reg = args[\"criterion\"][\"me_max\"]\n",
        "supervised_views = args[\"criterion\"][\"supervised_views\"]\n",
        "classes_per_batch = args[\"criterion\"][\"classes_per_batch\"]\n",
        "s_batch_size = args[\"criterion\"][\"supervised_imgs_per_class\"]\n",
        "u_batch_size = args[\"criterion\"][\"unsupervised_batch_size\"]\n",
        "temperature = args[\"criterion\"][\"temperature\"]\n",
        "sharpen = args[\"criterion\"][\"sharpen\"]\n",
        "\n",
        "# -- DATA\n",
        "unlabeled_frac = args[\"data\"][\"unlabeled_frac\"]\n",
        "color_jitter = args[\"data\"][\"color_jitter_strength\"]\n",
        "normalize = args[\"data\"][\"normalize\"]\n",
        "root_path = args[\"data\"][\"root_path\"]\n",
        "s_image_folder = args[\"data\"][\"s_image_folder\"]\n",
        "u_image_folder = args[\"data\"][\"u_image_folder\"]\n",
        "dataset_name = args[\"data\"][\"dataset\"]\n",
        "subset_path = args[\"data\"][\"subset_path\"]\n",
        "unique_classes = args[\"data\"][\"unique_classes_per_rank\"]\n",
        "label_smoothing = args[\"data\"][\"label_smoothing\"]\n",
        "data_seed = None\n",
        "\n",
        "copy_data = args[\"meta\"][\"copy_data\"]\n",
        "use_pred_head = args[\"meta\"][\"use_pred_head\"]\n",
        "\n",
        "use_fp16 = args[\"meta\"][\"use_fp16\"]\n",
        "\n",
        "# -- OPTIMIZATION\n",
        "wd = float(args[\"optimization\"][\"weight_decay\"])\n",
        "num_epochs = args[\"optimization\"][\"epochs\"]\n",
        "warmup = args[\"optimization\"][\"warmup\"]\n",
        "start_lr = args[\"optimization\"][\"start_lr\"]\n",
        "lr = args[\"optimization\"][\"lr\"]\n",
        "final_lr = args[\"optimization\"][\"final_lr\"]\n",
        "mom = args[\"optimization\"][\"momentum\"]\n",
        "nesterov = args[\"optimization\"][\"nesterov\"]\n",
        "\n",
        "\n",
        "# -- META\n",
        "load_model = args[\"meta\"][\"load_checkpoint\"]\n",
        "r_file = args[\"meta\"][\"read_checkpoint\"]\n",
        "\n",
        "\n",
        "# -- LOGGING\n",
        "folder = args[\"logging\"][\"folder\"]\n",
        "tag = args[\"logging\"][\"write_tag\"]\n",
        "# ----------------------------------------------------------------------- #"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBzLKYbOUtYO"
      },
      "source": [
        "crop_scale = (0.14, 1.0) if multicrop > 0 else (0.08, 1.0)\n",
        "mc_scale = (0.05, 0.14)\n",
        "mc_size = 96"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7N60VarMvrC"
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import src.resnet as resnet\n",
        "import src.wide_resnet as wide_resnet\n",
        "from src.utils import (\n",
        "    gpu_timer,\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule,\n",
        "    CSVLogger,\n",
        "    AverageMeter,\n",
        ")\n",
        "from src.losses import init_paws_loss, make_labels_matrix\n",
        "from src.data_manager import init_data, make_transforms, make_multicrop_transform\n",
        "from src.sgd import SGD\n",
        "from src.lars import LARS\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "#import apex\n",
        "from torch.nn.parallel import DistributedDataParallel"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCx_JdBzD05x"
      },
      "source": [
        "from src.paws_train import init_model\n",
        "from src.data_manager import ImageNet\n",
        "from src.data_manager import GaussianBlur"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1hxqUQn47M"
      },
      "source": [
        "## PyTorch/XLA ResNet18/CIFAR10 (GPU or TPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSWXG7FKF5n"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import gdown\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from src.utils import (\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule,\n",
        "    CSVLogger,\n",
        "    AverageMeter,\n",
        ")\n",
        "from src.utils import (\n",
        "    AllGather,\n",
        "    AllReduce\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHKXfPULVP1"
      },
      "source": [
        "def download(data, url):\n",
        "    # Download dataset\n",
        "    import zipfile\n",
        "    url = url\n",
        "    output = \"{}.zip\".format(data)\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Uncompress dataset\n",
        "    local_zip = '{}.zip'.format(data)\n",
        "    zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhpxrcKLX7W"
      },
      "source": [
        "data_samples = {\n",
        "    \"sample@200\": \"https://drive.google.com/uc?id=1FfV7YyDJvNUCDP5r3-8iQfZ2-xJp_pgb\",\n",
        "    \"sample@500\": \"https://drive.google.com/uc?id=1dHwUqpmSogEdjAB9rwDUL-OKFRUcVXte\",\n",
        "    \"sample@1000\": \"https://drive.google.com/uc?id=1DPZrHrj3Bdte5Dc6NCZ33CAqMG-Oipa2\",\n",
        "    \"sample@2000\": \"https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\",\n",
        "    \"sample@3000\": \"https://drive.google.com/uc?id=1_yre5K9YYvJgSrT4xvrI8eD_htucIywA\",\n",
        "    \"sample@4000_images\": \"https://drive.google.com/uc?id=1dqVB8EozEpwWzyuU80AauoQmsiw3Gtm2\",\n",
        "    \"sample@20000\": \"https://drive.google.com/uc?id=1MTDpLzpmhSiZq2jSdmHx2UDPn9FC8gzO\",\n",
        "    \"val-voets-tf\": \"https://drive.google.com/uc?id=1VzVgMGTkBBPG2qbzLunD9HvLzH6tcyrv\",\n",
        "    \"train_voets\": \"https://drive.google.com/uc?id=1AmcFh1MOOZ6aqKm2eO7XEdgmIEqHKTZ5\",\n",
        "    \"voets_test_images\": \"https://drive.google.com/uc?id=15S_V3B_Z3BOjCT3AbO2c887FyS5B0Lyd\"\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1J6gqYLZC7"
      },
      "source": [
        "UNLABELED = 'sample@2000'"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-BZSTOLaFm",
        "outputId": "c38a9f53-d098-4f91-a3bd-e4cf9cfee1c9"
      },
      "source": [
        "URL_UNLABELED = data_samples[UNLABELED]\n",
        "download(UNLABELED, URL_UNLABELED)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\n",
            "To: /content/suncet/sample@2000.zip\n",
            "214MB [00:00, 341MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IednejwkIW-K"
      },
      "source": [
        "Only run the below commented cell if you would like a nightly release"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rroH9yiAn-XE"
      },
      "source": [
        "### Define Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umuD5GhXawwo"
      },
      "source": [
        "multicrop=multicrop\n",
        "tau=temperature\n",
        "T=sharpen\n",
        "me_max=reg\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Make semi-supervised PAWS loss\n",
        "\n",
        ":param multicrop: number of small multi-crop views\n",
        ":param tau: cosine similarity temperature\n",
        ":param T: target sharpenning temperature\n",
        ":param me_max: whether to perform me-max regularization\n",
        "\"\"\"\n",
        "\n",
        "def sharpen_func(p):\n",
        "    sharp_p = p**(1./T)\n",
        "    sharp_p /= torch.sum(sharp_p, dim=1, keepdim=True)\n",
        "    return sharp_p\n",
        "\n",
        "def snn(query, supports, labels, tau):\n",
        "\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "    \"\"\" Soft Nearest Neighbours similarity classifier \"\"\"\n",
        "    # Step 1: normalize embeddings\n",
        "    query = torch.nn.functional.normalize(query)\n",
        "    supports = torch.nn.functional.normalize(supports)\n",
        "\n",
        "    # Step 2: gather embeddings from all workers\n",
        "    supports = AllGather.apply(supports)\n",
        "\n",
        "    # Step 3: compute similarlity between local embeddings\n",
        "\n",
        "    #result = softmax(query @ supports.T / tau) @ labels\n",
        "    result = torch.matmul(softmax(torch.matmul(query, supports.T / tau)), labels)\n",
        "\n",
        "    return result\n",
        "\n",
        "def my_loss_func(\n",
        "    anchor_views,\n",
        "    anchor_supports,\n",
        "    anchor_support_labels,\n",
        "    target_views,\n",
        "    target_supports,\n",
        "    target_support_labels,\n",
        "    sharpen=sharpen_func,\n",
        "    snn=snn\n",
        "):\n",
        "    # -- NOTE: num views of each unlabeled instance = 2+multicrop\n",
        "    batch_size = len(anchor_views) // (2+multicrop)\n",
        "\n",
        "    # Step 1: compute anchor predictions\n",
        "    probs = snn(anchor_views, anchor_supports, anchor_support_labels, tau)\n",
        "\n",
        "    # Step 2: compute targets for anchor predictions\n",
        "    with torch.no_grad():\n",
        "        targets = snn(target_views, target_supports, target_support_labels, tau)\n",
        "        targets = sharpen(targets)\n",
        "        if multicrop > 0:\n",
        "            mc_target = 0.5*(targets[:batch_size]+targets[batch_size:])\n",
        "            targets = torch.cat([targets, *[mc_target for _ in range(multicrop)]], dim=0)\n",
        "        targets[targets < 1e-4] *= 0  # numerical stability\n",
        "\n",
        "    # Step 3: compute cross-entropy loss H(targets, queries)\n",
        "    #loss = torch.mean(torch.sum(torch.log(probs**(-targets)), dim=1))\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    targets2 = targets.argmax(-1)\n",
        "    loss = criterion(probs, targets2)\n",
        "\n",
        "    # Step 4: compute me-max regularizer\n",
        "    rloss = 0.\n",
        "    if me_max:\n",
        "        avg_probs = AllReduce.apply(torch.mean(sharpen(probs), dim=0))\n",
        "        rloss -= torch.sum(torch.log(avg_probs**(-avg_probs)))\n",
        "\n",
        "    return loss, rloss"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQlRwv7TzBN"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG4xTLJxT0WA"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMl96KLoCq8"
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "# Only instantiate model weights once in memory.\n",
        "#WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
        "\n",
        "def train_resnet18():\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  ############# PAWS CODE ##################\n",
        "\n",
        "  device = xm.xla_device()\n",
        "\n",
        "  # -- init model\n",
        "  encoder = init_model(\n",
        "    device=device,\n",
        "    model_name=model_name,\n",
        "    use_pred=use_pred_head,\n",
        "    output_dim=output_dim,\n",
        "  )\n",
        "\n",
        "  # -- init losses\n",
        "  #paws = init_paws_loss(multicrop=multicrop, tau=temperature, T=sharpen, me_max=reg)\n",
        "  # -- assume support images are sampled with ClassStratifiedSampler\n",
        "\n",
        "  # TO DO: Hacer el smooth de los labels Matrix.\n",
        "\n",
        "# -- make data transforms\n",
        "  transform, init_transform = make_transforms(\n",
        "    dataset_name=dataset_name,\n",
        "    subset_path=subset_path,\n",
        "    unlabeled_frac=unlabeled_frac,\n",
        "    training=True,\n",
        "    split_seed=data_seed,\n",
        "    crop_scale=crop_scale,\n",
        "    basic_augmentations=False,\n",
        "    color_jitter=color_jitter,\n",
        "    normalize=normalize,\n",
        "  )\n",
        "  multicrop_transform = (multicrop, None)\n",
        "  if multicrop > 0:\n",
        "    multicrop_transform = make_multicrop_transform(\n",
        "        dataset_name=dataset_name,\n",
        "        num_crops=multicrop,\n",
        "        size=mc_size,\n",
        "        crop_scale=mc_scale,\n",
        "        normalize=normalize,\n",
        "        color_distortion=color_jitter,\n",
        "    )\n",
        "\n",
        "# -- init data-loaders/samplers\n",
        "  (\n",
        "    unsupervised_loader,\n",
        "    unsupervised_sampler,\n",
        "    supervised_loader,\n",
        "    supervised_sampler,\n",
        ") = init_data(\n",
        "    dataset_name=dataset_name,\n",
        "    transform=transform,\n",
        "    init_transform=init_transform,\n",
        "    supervised_views=supervised_views,\n",
        "    u_batch_size=u_batch_size,\n",
        "    s_batch_size=s_batch_size,\n",
        "    unique_classes=unique_classes,\n",
        "    classes_per_batch=classes_per_batch,\n",
        "    multicrop_transform=multicrop_transform,\n",
        "    world_size=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    root_path=root_path,\n",
        "    s_image_folder=s_image_folder,\n",
        "    u_image_folder=u_image_folder,\n",
        "    training=True,\n",
        "    copy_data=copy_data,\n",
        "  )\n",
        "\n",
        "  #iter_supervised = None\n",
        "  ipe = len(unsupervised_loader)\n",
        "\n",
        "  logger.info(f\"iterations per epoch: {ipe}\")\n",
        "\n",
        "  # -- init optimizer and scheduler\n",
        "  #scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "  \n",
        "  # To Do: Optimizer Doble check learning rate\n",
        "  optimizer = torch.optim.Adam(encoder.parameters(), 0.0001, weight_decay=5e-4)\n",
        "\n",
        "  def train_loop_fn(supervised_loader, unsupervised_loader, epoch):\n",
        "      tracker = xm.RateTracker()\n",
        "      # -- TRAINING LOOP\n",
        "      best_loss = None\n",
        "\n",
        "      unsupervised_sampler.set_epoch(epoch)\n",
        "\n",
        "      loss_meter = AverageMeter()\n",
        "      ploss_meter = AverageMeter()\n",
        "      rloss_meter = AverageMeter()\n",
        "      time_meter = AverageMeter()\n",
        "      data_meter = AverageMeter()\n",
        "\n",
        "      for itr, udata in enumerate(unsupervised_loader):\n",
        "          def load_imgs(supervised_loader):\n",
        "              # -- unsupervised imgs\n",
        "              uimgs = [u.to(device, non_blocking=True) for u in udata[:-1]]\n",
        "              # -- supervised imgs\n",
        "              global iter_supervised\n",
        "              try:\n",
        "                  sdata = next(iter_supervised)\n",
        "              except Exception:\n",
        "                  iter_supervised = iter(supervised_loader)\n",
        "                  sdata = next(iter_supervised)\n",
        "              finally:\n",
        "                  idx = sdata[1].clone().detach()\n",
        "                  idx = idx.to(device)\n",
        "\n",
        "                  labels_matrix = torch.zeros(len(idx), idx.max()+1, device = device).scatter_(1, idx.unsqueeze(1), 1.)\n",
        "  \n",
        "                  labels_matrix = labels_matrix.to(device)\n",
        "                  labels = torch.cat([labels_matrix for _ in range(supervised_views)])\n",
        "                  simgs = [s.to(device, non_blocking=True) for s in sdata[:-1]]\n",
        "              # -- concatenate supervised imgs and unsupervised imgs\n",
        "              imgs = simgs + uimgs\n",
        "              return imgs, labels\n",
        "          \n",
        "          imgs, labels = load_imgs(supervised_loader)\n",
        "          \n",
        "          def train_step():\n",
        "\n",
        "              #with torch.cuda.amp.autocast(enabled=use_fp16):\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              # --\n",
        "              # h: representations of 'imgs' before head\n",
        "              # z: representations of 'imgs' after head\n",
        "              # -- If use_pred_head=False, then encoder.pred (prediction\n",
        "              #    head) is None, and _forward_head just returns the\n",
        "              #    identity, z=h\n",
        "              h, z = encoder(imgs, return_before_head=True)\n",
        "\n",
        "              # Compute paws loss in full precision\n",
        "              #with torch.cuda.amp.autocast(enabled=False):\n",
        "\n",
        "              # Step 1. convert representations to fp32\n",
        "              h, z = h.float(), z.float()\n",
        "\n",
        "              # Step 2. determine anchor views/supports and their\n",
        "              #         corresponding target views/supports\n",
        "              # --\n",
        "              num_support = (\n",
        "                  supervised_views * s_batch_size * classes_per_batch\n",
        "              )\n",
        "\n",
        "              # --\n",
        "              anchor_supports = z[:num_support]\n",
        "              anchor_views = z[num_support:]\n",
        "              # --\n",
        "              target_supports = h[:num_support].detach()\n",
        "              target_views = h[num_support:].detach()\n",
        "              target_views = torch.cat(\n",
        "                  [\n",
        "                      target_views[u_batch_size : 2 * u_batch_size],\n",
        "                      target_views[:u_batch_size],\n",
        "                  ],\n",
        "                  dim=0,\n",
        "              )\n",
        "\n",
        "              # Step 3. compute paws loss with me-max regularization\n",
        "              ploss, me_max = my_loss_func(\n",
        "                  anchor_views=anchor_views,\n",
        "                  anchor_supports=anchor_supports,\n",
        "                  anchor_support_labels=labels,\n",
        "                  target_views=target_views,\n",
        "                  target_supports=target_supports,\n",
        "                  target_support_labels=labels,\n",
        "              )\n",
        "\n",
        "              loss = ploss + me_max\n",
        "              loss.backward()\n",
        "\n",
        "              xm.optimizer_step(optimizer)\n",
        "\n",
        "              if itr % FLAGS.get(\"log_steps\") == 0:\n",
        "                  print(\n",
        "                      \"[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}\".format(\n",
        "                          xm.get_ordinal(),\n",
        "                          itr,\n",
        "                          loss.item(),\n",
        "                          tracker.rate(),\n",
        "                          tracker.global_rate(),\n",
        "                          time.asctime(),\n",
        "                      ),\n",
        "                      flush=True,\n",
        "                  )\n",
        " \n",
        "              return (float(loss), float(ploss), float(me_max))\n",
        "\n",
        "          (loss, ploss, rloss) = train_step()\n",
        "          loss_meter.update(loss)\n",
        "          ploss_meter.update(ploss)\n",
        "          rloss_meter.update(rloss)\n",
        "\n",
        "  data, pred, target = None, None, None\n",
        "\n",
        "  start_epoch = 0\n",
        "  end_epoch = FLAGS.get(\"num_epochs\")\n",
        "\n",
        "  train_supervised_loader = pl.MpDeviceLoader(supervised_loader, device)\n",
        "  train_unsupervised_loader = pl.MpDeviceLoader(unsupervised_loader, device)\n",
        "\n",
        "  for epoch in range(start_epoch, end_epoch):\n",
        "      train_loop_fn(train_supervised_loader, train_unsupervised_loader, epoch)\n",
        "      xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "  #return accuracy, data, pred, target\n",
        "  return 0, 0, 0, 0"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFXrf8Qd7V-E"
      },
      "source": [
        "import time"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fc644d-2fde-4baa-92e0-ea2c31a49f10"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  accuracy, data, pred, target = train_resnet18()\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[xla:0](0) Loss=0.00342 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:00 2021\n",
            "[xla:6](0) Loss=0.29423 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:10 2021\n",
            "[xla:2](0) Loss=0.04341 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:11 2021\n",
            "[xla:3](0) Loss=-0.01230 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:18 2021\n",
            "[xla:5](0) Loss=0.00045 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:19 2021\n",
            "[xla:7](0) Loss=0.19677 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:19 2021\n",
            "[xla:1](0) Loss=0.02202 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:19 2021\n",
            "[xla:4](0) Loss=0.00098 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:20:25 2021\n",
            "Finished training epoch 0\n",
            "[xla:7](0) Loss=0.04498 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:01 2021\n",
            "[xla:5](0) Loss=0.00952 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:02 2021\n",
            "[xla:4](0) Loss=-0.00206 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:04 2021\n",
            "[xla:3](0) Loss=0.00430 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:05 2021\n",
            "[xla:2](0) Loss=0.01928 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:08 2021\n",
            "[xla:1](0) Loss=0.02405 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:09 2021\n",
            "[xla:0](0) Loss=-0.00786 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:13 2021\n",
            "[xla:6](0) Loss=0.11816 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:21:15 2021\n",
            "Finished training epoch 1\n",
            "[xla:1](0) Loss=-0.00574 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:24:54 2021\n",
            "[xla:0](0) Loss=0.02990 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:24:57 2021\n",
            "[xla:5](0) Loss=-0.00061 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:25:04 2021\n",
            "[xla:7](0) Loss=0.02293 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:25:06 2021\n",
            "[xla:4](0) Loss=-0.01003 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:25:08 2021\n",
            "[xla:6](0) Loss=0.01799 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:25:08 2021\n",
            "[xla:2](0) Loss=0.06469 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:25:08 2021\n",
            "[xla:3](0) Loss=0.00988 Rate=0.00 GlobalRate=0.00 Time=Sat Aug 21 18:25:08 2021\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}