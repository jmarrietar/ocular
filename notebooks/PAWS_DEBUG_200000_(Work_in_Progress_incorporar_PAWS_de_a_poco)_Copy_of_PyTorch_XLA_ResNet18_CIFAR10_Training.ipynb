{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " PAWS_DEBUG 200000 (Work in Progress incorporar PAWS de a poco) - Copy of PyTorch/XLA ResNet18/CIFAR10 Training",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/PAWS_DEBUG_200000_(Work_in_Progress_incorporar_PAWS_de_a_poco)_Copy_of_PyTorch_XLA_ResNet18_CIFAR10_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab3fc2b-7101-47fa-8b62-d46b5d0ef23d"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-xla==1.8.1\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl (145.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 145.0 MB 11 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.34.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.278 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy5ndp3nJPbD",
        "outputId": "30af2d57-7f9e-4c17-d043-56d570f01d04"
      },
      "source": [
        "!pip uninstall torch -y\n",
        "!pip install torch==1.8.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.9.0+cu102\n",
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd3xZvoA-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7531eea5-6f65-4bde-c0a4-93121f058448"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import argparse\n",
        "import yaml\n",
        "import pprint\n",
        "import logging\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.8.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "mskoYvt0MCaq",
        "outputId": "b74a6434-78b9-4556-b129-eb9018d436a4"
      },
      "source": [
        "pip install -U PyYAML"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l9EOMw3yQQlX",
        "outputId": "09f8db52-2b54-4112-9daf-6ad17b3f73bb"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6KiMba_MHkU",
        "outputId": "1e98ddf9-148c-4361-9215-0b37fcca7f04"
      },
      "source": [
        "!git clone -b feature/DR-images-v2 https://github.com/jmarrietar/suncet.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'suncet'...\n",
            "remote: Enumerating objects: 346, done.\u001b[K\n",
            "remote: Counting objects: 100% (346/346), done.\u001b[K\n",
            "remote: Compressing objects: 100% (224/224), done.\u001b[K\n",
            "remote: Total 346 (delta 207), reused 246 (delta 119), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (346/346), 1.11 MiB | 8.73 MiB/s, done.\n",
            "Resolving deltas: 100% (207/207), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtWmyuh4MK0J",
        "outputId": "1ef54659-1ff2-4217-cdd0-3e7cf6f45ece"
      },
      "source": [
        "cd suncet"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/suncet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mKok-0lMK4s"
      },
      "source": [
        "!mkdir datasets\n",
        "!mkdir datasets/dr\n",
        "!mkdir logs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTLU6dWXMK9B",
        "outputId": "16c9cd10-2896-42b9-e210-feb385a17efc"
      },
      "source": [
        "!python download.py -d sample@2000"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\n",
            "To: /content/suncet/datasets/dr/sample@2000.zip\n",
            "214MB [00:01, 127MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pCIJ3u5MWBZ",
        "outputId": "6188c5b7-82be-43c9-d384-080d5271fb06"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--fname', type=str,\n",
        "    help='name of config file to load',\n",
        "    default='configs.yaml')\n",
        "parser.add_argument(\n",
        "    '--devices', type=str, nargs='+', default=['cuda:0'],\n",
        "    help='which devices to use on local machine')\n",
        "parser.add_argument(\n",
        "    '--sel', type=str,\n",
        "    help='which script to run',\n",
        "    choices=[\n",
        "        'paws_train',\n",
        "        'suncet_train',\n",
        "        'fine_tune',\n",
        "        'snn_fine_tune'\n",
        "    ])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--sel'], dest='sel', nargs=None, const=None, default=None, type=<class 'str'>, choices=['paws_train', 'suncet_train', 'fine_tune', 'snn_fine_tune'], help='which script to run', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "halcMg65MqwY"
      },
      "source": [
        "args = parser.parse_args(['--sel', 'paws_train',\n",
        "                            '--fname', 'configs/paws/dr_train.yaml'\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz8KUfZZPNlq"
      },
      "source": [
        "fname = args.fname\n",
        "sel = args.sel"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7G3xNSPPUrl"
      },
      "source": [
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "\n",
        "logger.info(f'called-params {sel} {fname}')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iRvWJmdPWqr",
        "outputId": "2fd87e5d-6be7-45d6-ec9e-a1000d3fb6fa"
      },
      "source": [
        "# -- load script params\n",
        "params = None\n",
        "with open(fname, 'r') as y_file:\n",
        "    #params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
        "    params = yaml.load(y_file)\n",
        "    logger.info('loaded params...')\n",
        "    #if rank == 0:\n",
        "    pp = pprint.PrettyPrinter(indent=4)\n",
        "    pp.pprint(params)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{   'criterion': {   'classes_per_batch': 2,\n",
            "                     'me_max': True,\n",
            "                     'sharpen': 0.25,\n",
            "                     'supervised_imgs_per_class': 8,\n",
            "                     'supervised_views': 1,\n",
            "                     'temperature': 0.1,\n",
            "                     'unsupervised_batch_size': 32},\n",
            "    'data': {   'color_jitter_strength': 1.0,\n",
            "                'data_seed': None,\n",
            "                'dataset': 'dr',\n",
            "                'label_smoothing': 0.1,\n",
            "                'multicrop': 6,\n",
            "                'normalize': True,\n",
            "                'root_path': 'datasets/',\n",
            "                's_image_folder': 'dr/sample@2000/',\n",
            "                'subset_path': 'dr_subsets',\n",
            "                'u_image_folder': 'dr/sample@2000/',\n",
            "                'unique_classes_per_rank': False,\n",
            "                'unlabeled_frac': 0.9},\n",
            "    'logging': {'folder': 'logs/', 'write_tag': 'paws'},\n",
            "    'meta': {   'copy_data': True,\n",
            "                'device': 'cuda:0',\n",
            "                'load_checkpoint': False,\n",
            "                'model_name': 'resnet50',\n",
            "                'output_dim': 2048,\n",
            "                'read_checkpoint': None,\n",
            "                'use_fp16': True,\n",
            "                'use_pred_head': True},\n",
            "    'optimization': {   'epochs': 100,\n",
            "                        'final_lr': 1e-05,\n",
            "                        'lr': 0.001,\n",
            "                        'momentum': 0.9,\n",
            "                        'nesterov': False,\n",
            "                        'start_lr': 0.001,\n",
            "                        'warmup': 10,\n",
            "                        'weight_decay': 1e-06}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IyTUPjAPYst"
      },
      "source": [
        "args = params"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdPRFXIn_jH"
      },
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 2\n",
        "FLAGS['learning_rate'] = 0.02\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3npOhg9RuaH"
      },
      "source": [
        "FLAGS['model_name'] = args[\"meta\"][\"model_name\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8IkOQ9iZTzQ"
      },
      "source": [
        "model_name = args[\"meta\"][\"model_name\"]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzhD4JoAS5MP"
      },
      "source": [
        "output_dim = args[\"meta\"][\"output_dim\"]\n",
        "multicrop = args[\"data\"][\"multicrop\"]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFQzNvSTTFax"
      },
      "source": [
        "# -- CRITERTION\n",
        "reg = args[\"criterion\"][\"me_max\"]\n",
        "supervised_views = args[\"criterion\"][\"supervised_views\"]\n",
        "classes_per_batch = args[\"criterion\"][\"classes_per_batch\"]\n",
        "s_batch_size = args[\"criterion\"][\"supervised_imgs_per_class\"]\n",
        "u_batch_size = args[\"criterion\"][\"unsupervised_batch_size\"]\n",
        "temperature = args[\"criterion\"][\"temperature\"]\n",
        "sharpen = args[\"criterion\"][\"sharpen\"]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VPX6eJ7UgMK"
      },
      "source": [
        "# -- DATA\n",
        "unlabeled_frac = args[\"data\"][\"unlabeled_frac\"]\n",
        "color_jitter = args[\"data\"][\"color_jitter_strength\"]\n",
        "normalize = args[\"data\"][\"normalize\"]\n",
        "root_path = args[\"data\"][\"root_path\"]\n",
        "s_image_folder = args[\"data\"][\"s_image_folder\"]\n",
        "u_image_folder = args[\"data\"][\"u_image_folder\"]\n",
        "dataset_name = args[\"data\"][\"dataset\"]\n",
        "subset_path = args[\"data\"][\"subset_path\"]\n",
        "unique_classes = args[\"data\"][\"unique_classes_per_rank\"]\n",
        "label_smoothing = args[\"data\"][\"label_smoothing\"]\n",
        "data_seed = None"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBzLKYbOUtYO"
      },
      "source": [
        "crop_scale = (0.14, 1.0) if multicrop > 0 else (0.08, 1.0)\n",
        "mc_scale = (0.05, 0.14)\n",
        "mc_size = 96"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZVxR2ZU7pn"
      },
      "source": [
        "copy_data = args[\"meta\"][\"copy_data\"]\n",
        "use_pred_head = args[\"meta\"][\"use_pred_head\"]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqLceQ2_aoGr"
      },
      "source": [
        "use_fp16 = args[\"meta\"][\"use_fp16\"]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s_OIC5Ma-1V"
      },
      "source": [
        "# -- OPTIMIZATION\n",
        "wd = float(args[\"optimization\"][\"weight_decay\"])\n",
        "num_epochs = args[\"optimization\"][\"epochs\"]\n",
        "warmup = args[\"optimization\"][\"warmup\"]\n",
        "start_lr = args[\"optimization\"][\"start_lr\"]\n",
        "lr = args[\"optimization\"][\"lr\"]\n",
        "final_lr = args[\"optimization\"][\"final_lr\"]\n",
        "mom = args[\"optimization\"][\"momentum\"]\n",
        "nesterov = args[\"optimization\"][\"nesterov\"]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lEMWRcAPa38"
      },
      "source": [
        "# ----------------------------------------------------------------------- #\n",
        "#  PASSED IN PARAMS FROM CONFIG FILE\n",
        "# ----------------------------------------------------------------------- #\n",
        "# -- META\n",
        "\n",
        "\n",
        "load_model = args[\"meta\"][\"load_checkpoint\"]\n",
        "r_file = args[\"meta\"][\"read_checkpoint\"]\n",
        "\n",
        "\n",
        "#device = torch.device(args[\"meta\"][\"device\"])\n",
        "device = xm.xla_device()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -- LOGGING\n",
        "folder = args[\"logging\"][\"folder\"]\n",
        "tag = args[\"logging\"][\"write_tag\"]\n",
        "# ----------------------------------------------------------------------- #\n",
        "\n",
        "# -- init torch distributed backend\n",
        "#world_size, rank = init_distributed()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7N60VarMvrC"
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import src.resnet as resnet\n",
        "import src.wide_resnet as wide_resnet\n",
        "from src.utils import (\n",
        "    gpu_timer,\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule,\n",
        "    CSVLogger,\n",
        "    AverageMeter,\n",
        ")\n",
        "from src.losses import init_paws_loss, make_labels_matrix\n",
        "from src.data_manager import init_data, make_transforms, make_multicrop_transform\n",
        "from src.sgd import SGD\n",
        "from src.lars import LARS\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "#import apex\n",
        "from torch.nn.parallel import DistributedDataParallel"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWD0feLVfKo-"
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "from logging import getLogger\n",
        "\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "\n",
        "import torch\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def init_data(\n",
        "    dataset_name,\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch,\n",
        "    world_size,\n",
        "    rank,\n",
        "    unique_classes=False,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    root_path=None,\n",
        "    s_image_folder=None,\n",
        "    u_image_folder=None,\n",
        "    image_folder=None,\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    stratify=False,\n",
        "    drop_last=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    :param dataset_name: ['imagenet', 'cifar10', 'cifar10_fine_tune', 'imagenet_fine_tune']\n",
        "    :param transform: torchvision transform to apply to each batch of data\n",
        "    :param init_transform: transform to apply once to all data at the start\n",
        "    :param u_batch_size: unsupervised batch-size\n",
        "    :param s_batch_size: supervised batch-size (images per class)\n",
        "    :param classes_per_batch: num. classes sampled in each supervised batch per gpu\n",
        "    :param unique_classes: whether each GPU should load different classes\n",
        "    :param multicrop_transform: number of smaller multi-crop images to return\n",
        "    :param supervised_views: number of views to generate of each labeled imgs\n",
        "    :param world_size: number of workers for distributed training\n",
        "    :param rank: rank of worker in distributed training\n",
        "    :param root_path: path to the root directory containing all dataset\n",
        "    :param image_folder: name of folder in 'root_path' containing data to load\n",
        "    :param training: whether to load training data\n",
        "    :param stratify: whether to class stratify 'fine_tune' data loaders\n",
        "    :param copy_data: whether to copy data locally to node at start of training\n",
        "    \"\"\"\n",
        "\n",
        "    if dataset_name == \"dr\":\n",
        "        return _init_dr_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            u_batch_size=u_batch_size,\n",
        "            s_batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            multicrop_transform=multicrop_transform,\n",
        "            supervised_views=supervised_views,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            s_image_folder=s_image_folder,\n",
        "            u_image_folder=u_image_folder,\n",
        "            training=training,\n",
        "            copy_data=copy_data,\n",
        "        )\n",
        "\n",
        "    elif dataset_name == \"dr_fine_tune\":\n",
        "        batch_size = s_batch_size\n",
        "        return _init_imgnt_ft_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            batch_size=batch_size,\n",
        "            stratify=stratify,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            drop_last=drop_last,\n",
        "            copy_data=copy_data,\n",
        "        )\n",
        "\n",
        "\n",
        "def _init_dr_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch,\n",
        "    world_size,\n",
        "    rank,\n",
        "    unique_classes=False,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    root_path=\"/datasets/\",\n",
        "    s_image_folder=\"imagenet_full_size/061417/\",\n",
        "    u_image_folder=\"imagenet_full_size/061417/\",\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    tar_folder=\"imagenet_full_size/\",\n",
        "    tar_file=\"imagenet_full_size-061417.tar\",\n",
        "):\n",
        "    s_imagedr = ImageDR(\n",
        "        root=root_path,\n",
        "        image_folder=s_image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data,\n",
        "    )\n",
        "\n",
        "    u_imagedr = ImageDR(\n",
        "        root=root_path,\n",
        "        image_folder=u_image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data,\n",
        "    )\n",
        "    logger.info(\"ImageDR dataset created\")\n",
        "\n",
        "    logger.info(\"Making unsupervised ImageDR data loader...\")\n",
        "    unsupervised_set = TransImageDR(\n",
        "        dataset=u_imagedr,\n",
        "        supervised=False,\n",
        "        init_transform=init_transform,\n",
        "        multicrop_transform=multicrop_transform,\n",
        "        seed=_GLOBAL_SEED,\n",
        "    )\n",
        "    unsupervised_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset=unsupervised_set, num_replicas=world_size, rank=rank\n",
        "    )\n",
        "    unsupervised_loader = torch.utils.data.DataLoader(\n",
        "        unsupervised_set,\n",
        "        sampler=unsupervised_sampler,\n",
        "        batch_size=u_batch_size,\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "    logger.info(\"ImageDR unsupervised data loader created\")\n",
        "\n",
        "    supervised_sampler, supervised_loader = None, None\n",
        "    if classes_per_batch > 0 and s_batch_size > 0:\n",
        "        logger.info(\"Making supervised ImageDR data loader...\")\n",
        "\n",
        "        supervised_set = TransImageDR(\n",
        "            dataset=s_imagedr,\n",
        "            supervised=True,\n",
        "            supervised_views=supervised_views,\n",
        "            init_transform=init_transform,\n",
        "            seed=_GLOBAL_SEED,\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        supervised_sampler = ClassStratifiedSampler(\n",
        "            data_source=supervised_set,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            seed=_GLOBAL_SEED,\n",
        "        )\n",
        "        supervised_loader = torch.utils.data.DataLoader(\n",
        "            supervised_set,\n",
        "            batch_sampler=supervised_sampler,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "        \"\"\"\n",
        "        #supervised_set = datasets.ImageFolder(\n",
        "        #    root=root_path+s_image_folder\n",
        "        #)\n",
        "\n",
        "        supervised_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset=supervised_set, num_replicas=world_size, rank=rank\n",
        "        )\n",
        "\n",
        "        supervised_loader = torch.utils.data.DataLoader(\n",
        "            supervised_set,\n",
        "            sampler=supervised_sampler,\n",
        "            batch_size=s_batch_size*2, # YOO LE Agrege la Multiplicacion\n",
        "            drop_last=True,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "        \"\"\"\n",
        "        if len(supervised_loader) > 0:\n",
        "            tmp = ceil(len(unsupervised_loader) / len(supervised_loader))\n",
        "            supervised_sampler.set_inner_epochs(tmp)\n",
        "            logger.info(f\"supervised-reset-period {tmp}\")\n",
        "        \"\"\"\n",
        "        logger.info(\"ImageDR supervised data loader created\")\n",
        "\n",
        "    return (\n",
        "        unsupervised_loader,\n",
        "        unsupervised_sampler,\n",
        "        supervised_loader,\n",
        "        supervised_sampler,\n",
        "    )\n",
        "\n",
        "\n",
        "def _init_imgnt_ft_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    batch_size,\n",
        "    world_size,\n",
        "    rank,\n",
        "    stratify=False,\n",
        "    classes_per_batch=1,\n",
        "    unique_classes=False,\n",
        "    root_path=\"/datasets/\",\n",
        "    image_folder=\"imagenet_full_size/061417/\",\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    drop_last=True,\n",
        "    tar_folder=\"imagenet_full_size/\",\n",
        "    tar_file=\"imagenet_full_size-061417.tar\",\n",
        "):\n",
        "    imagenet = ImageDR(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data,\n",
        "    )\n",
        "    logger.info(\"ImageNet fine-tune dataset created\")\n",
        "    dataset = TransImageDR(\n",
        "        dataset=imagenet,\n",
        "        supervised=True,\n",
        "        init_transform=init_transform,\n",
        "        seed=_GLOBAL_SEED,\n",
        "    )\n",
        "\n",
        "    if not stratify:\n",
        "        dist_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset=dataset, num_replicas=world_size, rank=rank\n",
        "        )\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            sampler=dist_sampler,\n",
        "            batch_size=batch_size,\n",
        "            drop_last=drop_last,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "    else:\n",
        "        dist_sampler = ClassStratifiedSampler(\n",
        "            data_source=dataset,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            seed=_GLOBAL_SEED,\n",
        "            unique_classes=unique_classes,\n",
        "        )\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_sampler=dist_sampler, pin_memory=True, num_workers=8\n",
        "        )\n",
        "\n",
        "    return (data_loader, dist_sampler)\n",
        "\n",
        "\n",
        "def make_transforms(\n",
        "    dataset_name,\n",
        "    subset_path=None,\n",
        "    unlabeled_frac=1.0,\n",
        "    training=True,\n",
        "    basic_augmentations=False,\n",
        "    force_center_crop=False,\n",
        "    crop_scale=(0.08, 1.0),\n",
        "    color_jitter=1.0,\n",
        "    normalize=False,\n",
        "    split_seed=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    :param dataset_name: ['imagenet', 'cifar10']\n",
        "    :param subset_path: path to .txt file denoting subset of data to use\n",
        "    :param unlabeled_frac: fraction of data that is unlabeled\n",
        "    :param training: whether to load training data\n",
        "    :param basic_augmentations: whether to use simple data-augmentations\n",
        "    :param force_center_crop: whether to force use of a center-crop\n",
        "    :param color_jitter: strength of color-jitter\n",
        "    :param normalize: whether to normalize color channels\n",
        "    \"\"\"\n",
        "\n",
        "    if \"dr\" in dataset_name:\n",
        "        logger.info(\"making imagenet data transforms\")\n",
        "\n",
        "        # -- file identifying which imagenet labels to keep\n",
        "        keep_file = None\n",
        "        if subset_path is not None:\n",
        "            if unlabeled_frac >= 0:\n",
        "                keep_file = os.path.join(\n",
        "                    subset_path, f\"{int(unlabeled_frac* 100)}percent.txt\"\n",
        "                )\n",
        "            else:\n",
        "                keep_file = os.path.join(subset_path, \"val.txt\")\n",
        "            logger.info(f\"keep file: {keep_file}\")\n",
        "\n",
        "        return _make_imgnt_transforms(\n",
        "            unlabel_prob=unlabeled_frac,\n",
        "            training=training,\n",
        "            basic=basic_augmentations,\n",
        "            force_center_crop=force_center_crop,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_jitter,\n",
        "            scale=crop_scale,\n",
        "            keep_file=keep_file,\n",
        "        )\n",
        "\n",
        "\n",
        "def _make_imgnt_transforms(\n",
        "    unlabel_prob,\n",
        "    training=True,\n",
        "    basic=False,\n",
        "    force_center_crop=False,\n",
        "    normalize=False,\n",
        "    scale=(0.08, 1.0),\n",
        "    color_distortion=1.0,\n",
        "    keep_file=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Make data transformations\n",
        "\n",
        "    :param unlabel_prob: probability of sampling unlabeled data point\n",
        "    :param training: generate data transforms for train (alternativly test)\n",
        "    :param basic: whether train transforms include more sofisticated transforms\n",
        "    :param force_center_crop: whether to override settings and apply center crop to image\n",
        "    :param normalize: whether to normalize image means and stds\n",
        "    :param scale: random scaling range for image before resizing\n",
        "    :param color_distortion: strength of color distortion\n",
        "    :param keep_file: file containing names of images to use for semisupervised\n",
        "    \"\"\"\n",
        "\n",
        "    def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "        rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "        color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
        "        return color_distort\n",
        "\n",
        "    logger.debug(\n",
        "        f\"uprob: {unlabel_prob}\\t training: {training}\\t basic: {basic}\\t normalize: {normalize}\\t color_distortion: {color_distortion}\"\n",
        "    )\n",
        "    if training and (not force_center_crop):\n",
        "        if basic:\n",
        "            transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.RandomResizedCrop(size=224, scale=scale),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ToTensor(),\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            logger.debug(\"making training (non-basic) transforms\")\n",
        "            transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.RandomResizedCrop(size=224, scale=scale),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    get_color_distortion(s=color_distortion),\n",
        "                    GaussianBlur(p=0.5),\n",
        "                    transforms.ToTensor(),\n",
        "                ]\n",
        "            )\n",
        "    else:\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size=256),\n",
        "                transforms.CenterCrop(size=224),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transform,\n",
        "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def init_transform(\n",
        "        root, samples, class_to_idx, seed, keep_file=keep_file, training=training\n",
        "    ):\n",
        "        \"\"\"Transforms applied to dataset at the start of training\"\"\"\n",
        "\n",
        "        new_targets, new_samples = [], []\n",
        "        if training and (keep_file is not None) and os.path.exists(keep_file):\n",
        "            logger.info(f\"Using {keep_file}\")\n",
        "            with open(keep_file, \"r\") as rfile:\n",
        "                for line in rfile:\n",
        "                    class_name = line.split(\"_\")[0]\n",
        "                    target = class_to_idx[class_name]\n",
        "                    img = line.split(\"\\n\")[0]\n",
        "                    new_samples.append((os.path.join(root, class_name, img), target))\n",
        "                    new_targets.append(target)\n",
        "        else:\n",
        "            logger.info(\"flipping coin to keep labels\")\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(seed)\n",
        "            for sample in samples:\n",
        "                if torch.bernoulli(torch.tensor(unlabel_prob), generator=g) == 0:\n",
        "                    target = sample[1]\n",
        "                    new_samples.append((sample[0], target))\n",
        "                    new_targets.append(target)\n",
        "\n",
        "        return np.array(new_targets), np.array(new_samples)\n",
        "\n",
        "    return transform, init_transform\n",
        "\n",
        "\n",
        "def make_multicrop_transform(\n",
        "    dataset_name, num_crops, size, crop_scale, normalize, color_distortion\n",
        "):\n",
        "    if \"dr\" in dataset_name:\n",
        "        return _make_multicrop_imgnt_transforms(\n",
        "            num_crops=num_crops,\n",
        "            size=size,\n",
        "            scale=crop_scale,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_distortion,\n",
        "        )\n",
        "\n",
        "\n",
        "def _make_multicrop_imgnt_transforms(\n",
        "    num_crops, size=96, scale=(0.05, 0.14), normalize=False, color_distortion=1.0,\n",
        "):\n",
        "    def get_color_distortion(s=1.0):\n",
        "        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "        rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "        color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
        "        return color_distort\n",
        "\n",
        "    logger.debug(\"making multicrop transforms\")\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomResizedCrop(size=size, scale=scale),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            get_color_distortion(s=color_distortion),\n",
        "            GaussianBlur(p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transform,\n",
        "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    return (num_crops, transform)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wznE3tw1ZoX5"
      },
      "source": [
        "class ClassStratifiedSampler(torch.utils.data.Sampler):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_source,\n",
        "        world_size,\n",
        "        rank,\n",
        "        batch_size=1,\n",
        "        classes_per_batch=10,\n",
        "        epochs=1,\n",
        "        seed=0,\n",
        "        unique_classes=False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ClassStratifiedSampler\n",
        "\n",
        "        Batch-sampler that samples 'batch-size' images from subset of randomly\n",
        "        chosen classes e.g., if classes a,b,c are randomly sampled,\n",
        "        the sampler returns\n",
        "            torch.cat([a,b,c], [a,b,c], ..., [a,b,c], dim=0)\n",
        "        where a,b,c, are images from classes a,b,c respectively.\n",
        "        Sampler, samples images WITH REPLACEMENT (i.e., not epoch-based)\n",
        "\n",
        "        :param data_source: dataset of type \"TransImageNet\" or \"TransCIFAR10'\n",
        "        :param world_size: total number of workers in network\n",
        "        :param rank: local rank in network\n",
        "        :param batch_size: num. images to load from each class\n",
        "        :param classes_per_batch: num. classes to randomly sample for batch\n",
        "        :param epochs: num consecutive epochs thru data_source before gen.reset\n",
        "        :param seed: common seed across workers for subsampling classes\n",
        "        :param unique_classes: true ==> each worker samples a distinct set of classes; false ==> all workers sample the same classes\n",
        "        \"\"\"\n",
        "        super(ClassStratifiedSampler, self).__init__(data_source)\n",
        "        self.data_source = data_source\n",
        "\n",
        "        self.rank = rank\n",
        "        self.world_size = world_size\n",
        "        self.cpb = classes_per_batch\n",
        "        self.unique_cpb = unique_classes\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = len(data_source.classes)\n",
        "        self.epochs = epochs\n",
        "        self.outer_epoch = 0\n",
        "\n",
        "        if not self.unique_cpb:\n",
        "            assert self.num_classes % self.cpb == 0\n",
        "\n",
        "        self.base_seed = seed  # instance seed\n",
        "        self.seed = seed  # subsample sampler seed\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.outer_epoch = epoch\n",
        "\n",
        "    def set_inner_epochs(self, epochs):\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def _next_perm(self):\n",
        "        self.seed += 1\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.seed)\n",
        "        self._perm = torch.randperm(self.num_classes, generator=g)\n",
        "\n",
        "    def _get_perm_ssi(self):\n",
        "        start = self._ssi\n",
        "        #print(\"start: {}\".format(start))\n",
        "        end = self._ssi + self.cpb\n",
        "        #print(\"end: {}\".format(end))\n",
        "        subsample = self._perm[start:end]\n",
        "        #print(\"subsample {}\".format(subsample))\n",
        "        return subsample\n",
        "\n",
        "    def _next_ssi(self):\n",
        "        if not self.unique_cpb:\n",
        "            self._ssi = (self._ssi + self.cpb) % self.num_classes\n",
        "            if self._ssi == 0:\n",
        "                self._next_perm()\n",
        "        else:\n",
        "            self._ssi += self.cpb * self.world_size\n",
        "            max_end = self._ssi + self.cpb * (self.world_size - self.rank)\n",
        "            if max_end > self.num_classes:\n",
        "                self._ssi = self.rank * self.cpb\n",
        "                self._next_perm()\n",
        "\n",
        "    def _get_local_samplers(self, epoch):\n",
        "        \"\"\"Generate samplers for local data set in given epoch\"\"\"\n",
        "        seed = int(\n",
        "            self.base_seed\n",
        "            + epoch\n",
        "            + self.epochs * self.rank\n",
        "            + self.outer_epoch * self.epochs * self.world_size\n",
        "        )\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "        samplers = []\n",
        "        for t in range(self.num_classes):\n",
        "            t_indices = np.array(self.data_source.target_indices[t])\n",
        "            if not self.unique_cpb:\n",
        "                i_size = len(t_indices) // self.world_size\n",
        "                if i_size > 0:\n",
        "                    t_indices = t_indices[self.rank * i_size : (self.rank + 1) * i_size]\n",
        "            if len(t_indices) > 1:\n",
        "                t_indices = t_indices[torch.randperm(len(t_indices), generator=g)]\n",
        "            samplers.append(iter(t_indices))\n",
        "\n",
        "        return samplers\n",
        "\n",
        "    def _subsample_samplers(self, samplers):\n",
        "        \"\"\"Subsample a small set of samplers from all class-samplers\"\"\"\n",
        "        subsample = self._get_perm_ssi()\n",
        "        subsampled_samplers = []\n",
        "        for i in subsample:\n",
        "            subsampled_samplers.append(samplers[i])\n",
        "          \n",
        "        self._next_ssi()\n",
        "        return zip(*subsampled_samplers)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._ssi = self.rank * self.cpb if self.unique_cpb else 0\n",
        "        self._next_perm()\n",
        "\n",
        "        # -- iterations per epoch (extract batch-size samples from each class)\n",
        "        ipe = (\n",
        "            self.num_classes // self.cpb\n",
        "            if not self.unique_cpb\n",
        "            else self.num_classes // (self.cpb * self.world_size)\n",
        "        ) * self.batch_size\n",
        "\n",
        "        print(\"ipe is {}\".format(ipe))\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            # -- shuffle class order\n",
        "            samplers = self._get_local_samplers(epoch)\n",
        "            subsampled_samplers = self._subsample_samplers(samplers)\n",
        "\n",
        "            # #####\n",
        "\n",
        "            # #####\n",
        "\n",
        "            counter, batch = 0, []\n",
        "            for i in range(ipe):\n",
        "                #print(\"batch is {}\".format(batch))\n",
        "                try:\n",
        "                  batch += list(next(subsampled_samplers))\n",
        "                except Exception:\n",
        "                    #print(\"ERRROR intento agregar esto a la lista.\")\n",
        "                    #print(list(next(subsampled_samplers)))\n",
        "                    traceback.print_exc()\n",
        "                counter += 1\n",
        "                #print(\"counter {} ipe {}\".format(counter, ipe))\n",
        "                if counter == self.batch_size:\n",
        "                    yield batch\n",
        "                    counter, batch = 0, []\n",
        "                    if i + 1 < ipe:\n",
        "                        subsampled_samplers = self._subsample_samplers(samplers)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.batch_size == 0:\n",
        "            return 0\n",
        "\n",
        "        ipe = (\n",
        "            self.num_classes // self.cpb\n",
        "            if not self.unique_cpb\n",
        "            else self.num_classes // (self.cpb * self.world_size)\n",
        "        )\n",
        "        return self.epochs * ipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP1JxultMMz0"
      },
      "source": [
        "class ImageNet(torchvision.datasets.ImageFolder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        image_folder=\"imagenet_full_size/061417/\",\n",
        "        tar_folder=\"imagenet_full_size/\",\n",
        "        tar_file=\"imagenet_full_size-061417.tar\",\n",
        "        train=True,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "        job_id=None,\n",
        "        local_rank=None,\n",
        "        copy_data=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ImageNet\n",
        "        Dataset wrapper (can copy data locally to machine)\n",
        "        :param root: root network directory for ImageNet data\n",
        "        :param image_folder: path to images inside root network directory\n",
        "        :param tar_file: zipped image_folder inside root network directory\n",
        "        :param train: whether to load train data (or validation)\n",
        "        :param transform: data-augmentations (applied in data-loader)\n",
        "        :param target_transform: target-transform to apply in data-loader\n",
        "        :param job_id: scheduler job-id used to create dir on local machine\n",
        "        :param copy_data: whether to copy data from network file locally\n",
        "        \"\"\"\n",
        "\n",
        "        suffix = \"train/\" if train else \"val/\"\n",
        "        data_path = None\n",
        "        if copy_data:\n",
        "            logger.info(\"copying data locally\")\n",
        "            data_path = copy_imgnt_locally(\n",
        "                root=root,\n",
        "                suffix=suffix,\n",
        "                image_folder=image_folder,\n",
        "                tar_folder=tar_folder,\n",
        "                tar_file=tar_file,\n",
        "                job_id=job_id,\n",
        "                local_rank=local_rank,\n",
        "            )\n",
        "        if (not copy_data) or (data_path is None):\n",
        "            data_path = os.path.join(root, image_folder, suffix)\n",
        "        logger.info(f\"data-path {data_path}\")\n",
        "\n",
        "        super(ImageNet, self).__init__(\n",
        "            root=data_path, transform=transform, target_transform=target_transform\n",
        "        )\n",
        "        logger.info(\"Initialized ImageNet\")\n",
        "\n",
        "\n",
        "class ImageDR(torchvision.datasets.ImageFolder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        image_folder=\"imagenet_full_size/061417/\",\n",
        "        tar_folder=\"imagenet_full_size/\",\n",
        "        tar_file=\"imagenet_full_size-061417.tar\",\n",
        "        train=True,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "        job_id=None,\n",
        "        local_rank=None,\n",
        "        copy_data=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ImageNet\n",
        "\n",
        "        Dataset wrapper (can copy data locally to machine)\n",
        "\n",
        "        :param root: root network directory for DR data\n",
        "        :param image_folder: path to images inside root network directory\n",
        "        :param tar_file: zipped image_folder inside root network directory\n",
        "        :param train: whether to load train data (or validation)\n",
        "        :param transform: data-augmentations (applied in data-loader)\n",
        "        :param target_transform: target-transform to apply in data-loader\n",
        "        :param job_id: scheduler job-id used to create dir on local machine\n",
        "        :param copy_data: whether to copy data from network file locally\n",
        "        \"\"\"\n",
        "\n",
        "        suffix = \"train/\" if train else \"val/\"\n",
        "        data_path = None\n",
        "        if copy_data:\n",
        "            logger.info(\"copying data locally\")\n",
        "            data_path = copy_imgnt_locally(\n",
        "                root=root,\n",
        "                suffix=suffix,\n",
        "                image_folder=image_folder,\n",
        "                tar_folder=tar_folder,\n",
        "                tar_file=tar_file,\n",
        "                job_id=job_id,\n",
        "                local_rank=local_rank,\n",
        "            )\n",
        "        if (not copy_data) or (data_path is None):\n",
        "            data_path = os.path.join(root, image_folder, suffix)\n",
        "        logger.info(f\"data-path {data_path}\")\n",
        "\n",
        "        super(ImageDR, self).__init__(\n",
        "            root=data_path, transform=transform, target_transform=target_transform\n",
        "        )\n",
        "        logger.info(\"Initialized ImageDR\")\n",
        "\n",
        "\n",
        "class TransImageDR(ImageNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        supervised=False,\n",
        "        supervised_views=1,\n",
        "        init_transform=None,\n",
        "        multicrop_transform=(0, None),\n",
        "        seed=0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TransImageDR\n",
        "\n",
        "        Dataset that can apply transforms to images on initialization and can\n",
        "        return multiple transformed copies of the same image in each call\n",
        "        to __getitem__\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.supervised = supervised\n",
        "        self.supervised_views = supervised_views\n",
        "        self.multicrop_transform = multicrop_transform\n",
        "\n",
        "        print(\"self.multicrop_transform {}\".format(self.multicrop_transform))\n",
        "\n",
        "        self.targets, self.samples = dataset.targets, dataset.samples\n",
        "        if self.supervised:\n",
        "            self.targets, self.samples = init_transform(\n",
        "                dataset.root, dataset.samples, dataset.class_to_idx, seed\n",
        "            )\n",
        "            logger.debug(f\"num-labeled {len(self.samples)}\")\n",
        "            mint = None\n",
        "            self.target_indices = []\n",
        "            for t in range(len(dataset.classes)):\n",
        "                indices = np.squeeze(np.argwhere(self.targets == t)).tolist()\n",
        "                self.target_indices.append(indices)\n",
        "                mint = len(indices) if mint is None else min(mint, len(indices))\n",
        "                logger.debug(f\"num-labeled target {t} {len(indices)}\")\n",
        "            logger.debug(f\"min. labeled indices {mint}\")\n",
        "\n",
        "    @property\n",
        "    def classes(self):\n",
        "        return self.dataset.classes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.targets[index]\n",
        "        path = self.samples[index][0]\n",
        "        img = self.dataset.loader(path)\n",
        "\n",
        "        if self.dataset.target_transform is not None:\n",
        "            target = self.dataset.target_transform(target)\n",
        "\n",
        "        if self.dataset.transform is not None:\n",
        "            if self.supervised:\n",
        "                ans = (\n",
        "                    *[\n",
        "                        self.dataset.transform(img)\n",
        "                        for _ in range(self.supervised_views)\n",
        "                    ],\n",
        "                    target,\n",
        "                )\n",
        "                return ans\n",
        "            else:\n",
        "                img_1 = self.dataset.transform(img)\n",
        "                img_2 = self.dataset.transform(img)\n",
        "\n",
        "                multicrop, mc_transform = self.multicrop_transform\n",
        "                if multicrop > 0 and mc_transform is not None:\n",
        "                    mc_imgs = [mc_transform(img) for _ in range(int(multicrop))]\n",
        "                    ans = img_1, img_2, *mc_imgs, target\n",
        "                    return ans\n",
        "\n",
        "                return img_1, img_2, target\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def copy_imgnt_locally(\n",
        "    root,\n",
        "    suffix,\n",
        "    image_folder=\"imagenet_full_size/061417/\",\n",
        "    tar_folder=\"imagenet_full_size/\",\n",
        "    tar_file=\"imagenet_full_size-061417.tar\",\n",
        "    job_id=None,\n",
        "    local_rank=None,\n",
        "):\n",
        "    if job_id is None:\n",
        "        try:\n",
        "            job_id = os.environ[\"SLURM_JOBID\"]\n",
        "        except Exception:\n",
        "            logger.info(\"No job-id, will load directly from network file\")\n",
        "            return None\n",
        "\n",
        "    if local_rank is None:\n",
        "        try:\n",
        "            local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
        "        except Exception:\n",
        "            logger.info(\"No job-id, will load directly from network file\")\n",
        "            return None\n",
        "\n",
        "    source_file = os.path.join(root, tar_folder, tar_file)\n",
        "    target = f\"/scratch/slurm_tmpdir/{job_id}/\"\n",
        "    target_file = os.path.join(target, tar_file)\n",
        "    data_path = os.path.join(target, image_folder, suffix)\n",
        "    logger.info(f\"{source_file}\\n{target}\\n{target_file}\\n{data_path}\")\n",
        "\n",
        "    tmp_sgnl_file = os.path.join(target, \"copy_signal.txt\")\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        if local_rank == 0:\n",
        "            commands = [[\"tar\", \"-xf\", source_file, \"-C\", target]]\n",
        "            for cmnd in commands:\n",
        "                start_time = time.time()\n",
        "                logger.info(f\"Executing {cmnd}\")\n",
        "                subprocess.run(cmnd)\n",
        "                logger.info(f\"Cmnd took {(time.time()-start_time)/60.} min.\")\n",
        "            with open(tmp_sgnl_file, \"+w\") as f:\n",
        "                print(\"Done copying locally.\", file=f)\n",
        "        else:\n",
        "            while not os.path.exists(tmp_sgnl_file):\n",
        "                time.sleep(60)\n",
        "                logger.info(f\"{local_rank}: Checking {tmp_sgnl_file}\")\n",
        "\n",
        "    return data_path\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.0):\n",
        "        self.prob = p\n",
        "        self.radius_min = radius_min\n",
        "        self.radius_max = radius_max\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if torch.bernoulli(torch.tensor(self.prob)) == 0:\n",
        "            return img\n",
        "\n",
        "        radius = self.radius_min + torch.rand(1) * (self.radius_max - self.radius_min)\n",
        "        return img.filter(ImageFilter.GaussianBlur(radius=radius))\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhgKoRE3OKlW"
      },
      "source": [
        "def load_checkpoint(r_path, encoder, opt, scaler, use_fp16=False):\n",
        "    checkpoint = torch.load(r_path, map_location=\"cpu\")\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "    # -- loading encoder\n",
        "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
        "    logger.info(f\"loaded encoder from epoch {epoch}\")\n",
        "\n",
        "    # -- loading optimizer\n",
        "    opt.load_state_dict(checkpoint[\"opt\"])\n",
        "    if use_fp16:\n",
        "        scaler.load_state_dict(checkpoint[\"amp\"])\n",
        "    logger.info(f\"loaded optimizers from epoch {epoch}\")\n",
        "    logger.info(f\"read-path: {r_path}\")\n",
        "    del checkpoint\n",
        "    return encoder, opt, epoch\n",
        "\n",
        "\n",
        "def init_model(device, model_name=\"resnet50\", use_pred=False, output_dim=128):\n",
        "    if \"wide_resnet\" in model_name:\n",
        "        encoder = wide_resnet.__dict__[model_name](dropout_rate=0.0)\n",
        "        hidden_dim = 128\n",
        "    else:\n",
        "        encoder = resnet.__dict__[model_name]() \n",
        "\n",
        "        # Load pre-trained ResNetImagenNet\n",
        "        #logger.info(\"Load pre-trained ResNet ImagenNet weigths ...\")\n",
        "        #state_dict = load_state_dict_from_url('https://download.pytorch.org/models/resnet50-0676ba61.pth',progress=True)\n",
        "        #log = encoder.load_state_dict(state_dict, strict=False)\n",
        "        #logger.info(log)\n",
        "    \n",
        "        hidden_dim = 2048\n",
        "        if \"w2\" in model_name:\n",
        "            hidden_dim *= 2\n",
        "        elif \"w4\" in model_name:\n",
        "            hidden_dim *= 4\n",
        "\n",
        "    # -- projection head\n",
        "    encoder.fc = torch.nn.Sequential(\n",
        "        OrderedDict(\n",
        "            [\n",
        "                (\"fc1\", torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "                (\"bn1\", torch.nn.BatchNorm1d(hidden_dim)),\n",
        "                (\"relu1\", torch.nn.ReLU(inplace=True)),\n",
        "                (\"fc2\", torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "                (\"bn2\", torch.nn.BatchNorm1d(hidden_dim)),\n",
        "                (\"relu2\", torch.nn.ReLU(inplace=True)),\n",
        "                (\"fc3\", torch.nn.Linear(hidden_dim, output_dim)),\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # -- prediction head\n",
        "    encoder.pred = None\n",
        "    if use_pred:\n",
        "        mx = 4  # 4x bottleneck prediction head\n",
        "        pred_head = OrderedDict([])\n",
        "        pred_head[\"bn1\"] = torch.nn.BatchNorm1d(output_dim)\n",
        "        pred_head[\"fc1\"] = torch.nn.Linear(output_dim, output_dim // mx)\n",
        "        pred_head[\"bn2\"] = torch.nn.BatchNorm1d(output_dim // mx)\n",
        "        pred_head[\"relu\"] = torch.nn.ReLU(inplace=True)\n",
        "        pred_head[\"fc2\"] = torch.nn.Linear(output_dim // mx, output_dim)\n",
        "        encoder.pred = torch.nn.Sequential(pred_head)\n",
        "\n",
        "    encoder.to(device)\n",
        "    logger.info(encoder)\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def init_opt(\n",
        "    encoder,\n",
        "    iterations_per_epoch,\n",
        "    start_lr,\n",
        "    ref_lr,\n",
        "    ref_mom,\n",
        "    nesterov,\n",
        "    warmup,\n",
        "    num_epochs,\n",
        "    weight_decay=1e-6,\n",
        "    final_lr=0.0,\n",
        "):\n",
        "    param_groups = [\n",
        "        {\n",
        "            \"params\": (\n",
        "                p\n",
        "                for n, p in encoder.named_parameters()\n",
        "                if (\"bias\" not in n) and (\"bn\" not in n)\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"params\": (\n",
        "                p for n, p in encoder.named_parameters() if (\"bias\" in n) or (\"bn\" in n)\n",
        "            ),\n",
        "            \"LARS_exclude\": True,\n",
        "            \"weight_decay\": 0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = SGD(\n",
        "        param_groups,\n",
        "        weight_decay=weight_decay,\n",
        "        momentum=0.9,\n",
        "        nesterov=nesterov,\n",
        "        lr=ref_lr,\n",
        "    )\n",
        "    scheduler = WarmupCosineSchedule(\n",
        "        optimizer,\n",
        "        warmup_steps=warmup * iterations_per_epoch,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=ref_lr,\n",
        "        final_lr=final_lr,\n",
        "        T_max=num_epochs * iterations_per_epoch,\n",
        "    )\n",
        "    optimizer = LARS(optimizer, trust_coefficient=0.001)\n",
        "    return encoder, optimizer, scheduler"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1hxqUQn47M"
      },
      "source": [
        "## PyTorch/XLA ResNet18/CIFAR10 (GPU or TPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSWXG7FKF5n"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import gdown\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from src.utils import (\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule,\n",
        "    CSVLogger,\n",
        "    AverageMeter,\n",
        ")\n",
        "from src.utils import (\n",
        "    AllGather,\n",
        "    AllReduce\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHKXfPULVP1"
      },
      "source": [
        "def download(data, url):\n",
        "    # Download dataset\n",
        "    import zipfile\n",
        "    url = url\n",
        "    output = \"{}.zip\".format(data)\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Uncompress dataset\n",
        "    local_zip = '{}.zip'.format(data)\n",
        "    zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhpxrcKLX7W"
      },
      "source": [
        "data_samples = {\n",
        "    \"sample@200\": \"https://drive.google.com/uc?id=1FfV7YyDJvNUCDP5r3-8iQfZ2-xJp_pgb\",\n",
        "    \"sample@500\": \"https://drive.google.com/uc?id=1dHwUqpmSogEdjAB9rwDUL-OKFRUcVXte\",\n",
        "    \"sample@1000\": \"https://drive.google.com/uc?id=1DPZrHrj3Bdte5Dc6NCZ33CAqMG-Oipa2\",\n",
        "    \"sample@2000\": \"https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\",\n",
        "    \"sample@3000\": \"https://drive.google.com/uc?id=1_yre5K9YYvJgSrT4xvrI8eD_htucIywA\",\n",
        "    \"sample@4000_images\": \"https://drive.google.com/uc?id=1dqVB8EozEpwWzyuU80AauoQmsiw3Gtm2\",\n",
        "    \"sample@20000\": \"https://drive.google.com/uc?id=1MTDpLzpmhSiZq2jSdmHx2UDPn9FC8gzO\",\n",
        "    \"val-voets-tf\": \"https://drive.google.com/uc?id=1VzVgMGTkBBPG2qbzLunD9HvLzH6tcyrv\",\n",
        "    \"train_voets\": \"https://drive.google.com/uc?id=1AmcFh1MOOZ6aqKm2eO7XEdgmIEqHKTZ5\",\n",
        "    \"voets_test_images\": \"https://drive.google.com/uc?id=15S_V3B_Z3BOjCT3AbO2c887FyS5B0Lyd\"\n",
        "}"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1J6gqYLZC7"
      },
      "source": [
        "UNLABELED = 'sample@2000'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-BZSTOLaFm",
        "outputId": "c1426117-ae86-41d6-b960-0cb0c3cf7083"
      },
      "source": [
        "URL_UNLABELED = data_samples[UNLABELED]\n",
        "download(UNLABELED, URL_UNLABELED)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\n",
            "To: /content/suncet/sample@2000.zip\n",
            "214MB [00:00, 297MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IednejwkIW-K"
      },
      "source": [
        "Only run the below commented cell if you would like a nightly release"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFzLg5gy7l6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a765016a-b78d-49b6-daf0-f88e2427ce28"
      },
      "source": [
        "\"\"\"\n",
        "# PyTorch/XLA GPU Setup (only if GPU runtime)\n",
        "if os.environ.get('COLAB_GPU', '0') == '1':\n",
        "  os.environ['GPU_NUM_DEVICES'] = '1'\n",
        "  os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# PyTorch/XLA GPU Setup (only if GPU runtime)\\nif os.environ.get('COLAB_GPU', '0') == '1':\\n  os.environ['GPU_NUM_DEVICES'] = '1'\\n  os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rroH9yiAn-XE"
      },
      "source": [
        "### Define Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-JwDHNgcNoY7",
        "outputId": "c77505d8-a2c5-46f0-91dd-75833edea2c9"
      },
      "source": [
        "UNLABELED"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sample@2000'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umuD5GhXawwo"
      },
      "source": [
        "multicrop=multicrop\n",
        "tau=temperature\n",
        "T=sharpen\n",
        "me_max=reg\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Make semi-supervised PAWS loss\n",
        "\n",
        ":param multicrop: number of small multi-crop views\n",
        ":param tau: cosine similarity temperature\n",
        ":param T: target sharpenning temperature\n",
        ":param me_max: whether to perform me-max regularization\n",
        "\"\"\"\n",
        "\n",
        "def sharpen_func(p):\n",
        "    sharp_p = p**(1./T)\n",
        "    sharp_p /= torch.sum(sharp_p, dim=1, keepdim=True)\n",
        "    return sharp_p\n",
        "\n",
        "def snn(query, supports, labels, tau):\n",
        "    #device = xm.xla_device() # YOOOOO\n",
        "\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "    \"\"\" Soft Nearest Neighbours similarity classifier \"\"\"\n",
        "    # Step 1: normalize embeddings\n",
        "    query = torch.nn.functional.normalize(query)\n",
        "\n",
        "    #query = query.to(device) # YOOOOO\n",
        "\n",
        "    #print(\"Query is:::\")\n",
        "    #print(query)\n",
        "\n",
        "    supports = torch.nn.functional.normalize(supports)\n",
        "\n",
        "    #labels = labels.to(device) # YOOOOO\n",
        "    #supports = supports.to(device) # YOOOOO\n",
        "\n",
        "    #print(\"supports is:::::\")\n",
        "    #print(supports)\n",
        "    # Step 2: gather embeddings from all workers\n",
        "    supports = AllGather.apply(supports)\n",
        "\n",
        "    # Step 3: compute similarlity between local embeddings\n",
        "    #return softmax(query @ supports.T / tau) @ labels\n",
        "    #return softmax(query @ supports.T / tau) @ labels\n",
        "    ans = softmax(torch.matmul(query, supports.T / tau))\n",
        "\n",
        "    print(\"ans shape is {}\".format(ans.shape))\n",
        "    #print(ans)\n",
        "    print(\"labels shape is {}\".format(labels.shape))\n",
        "    #print(labels)\n",
        "\n",
        "    #result = softmax(query @ supports.T / tau) @ labels\n",
        "    result = torch.matmul(softmax(torch.matmul(query, supports.T / tau)), labels)\n",
        "    #result = ans\n",
        "    #print(\"result is:\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def my_loss_func(\n",
        "    anchor_views,\n",
        "    anchor_supports,\n",
        "    anchor_support_labels,\n",
        "    target_views,\n",
        "    target_supports,\n",
        "    target_support_labels,\n",
        "    sharpen=sharpen_func,\n",
        "    snn=snn\n",
        "):\n",
        "    # -- NOTE: num views of each unlabeled instance = 2+multicrop\n",
        "    batch_size = len(anchor_views) // (2+multicrop)\n",
        "\n",
        "    # Step 1: compute anchor predictions\n",
        "    probs = snn(anchor_views, anchor_supports, anchor_support_labels, tau)\n",
        "\n",
        "    # Step 2: compute targets for anchor predictions\n",
        "    with torch.no_grad():\n",
        "        targets = snn(target_views, target_supports, target_support_labels, tau)\n",
        "        targets = sharpen(targets)\n",
        "        if multicrop > 0:\n",
        "            mc_target = 0.5*(targets[:batch_size]+targets[batch_size:])\n",
        "            targets = torch.cat([targets, *[mc_target for _ in range(multicrop)]], dim=0)\n",
        "        targets[targets < 1e-4] *= 0  # numerical stability\n",
        "\n",
        "    # Step 3: compute cross-entropy loss H(targets, queries)\n",
        "    #loss = torch.mean(torch.sum(torch.log(probs**(-targets)), dim=1))\n",
        "\n",
        "    print(\"probs shape is {}\".format(probs.shape))\n",
        "    print(\"targets shape is {}\".format(targets.shape))\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    targets2 = targets.argmax(-1)\n",
        "    loss = criterion(probs, targets2)\n",
        "\n",
        "    # Step 4: compute me-max regularizer\n",
        "    rloss = 0.\n",
        "    if me_max:\n",
        "        avg_probs = AllReduce.apply(torch.mean(sharpen(probs), dim=0))\n",
        "        rloss -= torch.sum(torch.log(avg_probs**(-avg_probs)))\n",
        "\n",
        "    #return loss, rloss\n",
        "    return loss, rloss"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQlRwv7TzBN"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX90auWuTnDP"
      },
      "source": [
        "##########################################################\n",
        "##########################################################\n",
        "################# DEBUG DATA LOADERS #####################\n",
        "##########################################################\n",
        "##########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDMibhAKULo8"
      },
      "source": [
        "device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p3kqRM2ULye",
        "outputId": "a2e0fe72-f440-40f0-e0f8-b11b2b5eba62"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='xla', index=1)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MU1fmIHUL4j"
      },
      "source": [
        "# -- init model\n",
        "encoder = init_model(\n",
        "  device=device,\n",
        "  model_name=model_name,\n",
        "  use_pred=use_pred_head,\n",
        "  output_dim=output_dim,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "priSczb4Wb1J"
      },
      "source": [
        "labels_matrix = make_labels_matrix(\n",
        "    num_classes=classes_per_batch,\n",
        "    s_batch_size=s_batch_size,\n",
        "    world_size=xm.xrt_world_size(),\n",
        "    device=device,\n",
        "    unique_classes=unique_classes,\n",
        "    smoothing=label_smoothing,\n",
        "  ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L3leGM4UL-e",
        "outputId": "9cbe4dd1-4b29-4430-b62b-aa9a0dd81d4b"
      },
      "source": [
        "# -- make data transforms\n",
        "transform, init_transform = make_transforms(\n",
        "  dataset_name=dataset_name,\n",
        "  subset_path=subset_path,\n",
        "  unlabeled_frac=unlabeled_frac,\n",
        "  training=True,\n",
        "  split_seed=data_seed,\n",
        "  crop_scale=crop_scale,\n",
        "  basic_augmentations=False,\n",
        "  color_jitter=color_jitter,\n",
        "  normalize=normalize,\n",
        ")\n",
        "multicrop_transform = (multicrop, None)\n",
        "if multicrop > 0:\n",
        "  multicrop_transform = make_multicrop_transform(\n",
        "      dataset_name=dataset_name,\n",
        "      num_crops=multicrop,\n",
        "      size=mc_size,\n",
        "      crop_scale=mc_scale,\n",
        "      normalize=normalize,\n",
        "      color_distortion=color_jitter,\n",
        "  )\n",
        "\n",
        "# -- init data-loaders/samplers\n",
        "(\n",
        "  unsupervised_loader,\n",
        "  unsupervised_sampler,\n",
        "  supervised_loader,\n",
        "  supervised_sampler,\n",
        ") = init_data(\n",
        "  dataset_name=dataset_name,\n",
        "  transform=transform,\n",
        "  init_transform=init_transform,\n",
        "  supervised_views=supervised_views,\n",
        "  u_batch_size=u_batch_size,\n",
        "  s_batch_size=s_batch_size,\n",
        "  unique_classes=unique_classes,\n",
        "  classes_per_batch=classes_per_batch,\n",
        "  multicrop_transform=multicrop_transform,\n",
        "  world_size=xm.xrt_world_size(),\n",
        "  rank=xm.get_ordinal(),\n",
        "  root_path=root_path,\n",
        "  s_image_folder=s_image_folder,\n",
        "  u_image_folder=u_image_folder,\n",
        "  training=True,\n",
        "  copy_data=copy_data,\n",
        ")\n",
        "iter_supervised = None\n",
        "ipe = len(unsupervised_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7f45eb493550>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZSwYNSDC2zf",
        "outputId": "2ca25ee6-0cce-451f-aab7-4c7ee26f4454"
      },
      "source": [
        "len(supervised_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G7BR7CVX8dc"
      },
      "source": [
        "iter_supervised = None\n",
        "ipe = len(unsupervised_loader)\n",
        "logger.info(f\"iterations per epoch: {ipe}\")\n",
        "\n",
        "  # -- init optimizer and scheduler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "encoder, optimizer, scheduler = init_opt(\n",
        "      encoder=encoder,\n",
        "      weight_decay=wd,\n",
        "      start_lr=start_lr,\n",
        "      ref_lr=lr,\n",
        "      final_lr=final_lr,\n",
        "      ref_mom=mom,\n",
        "      nesterov=nesterov,\n",
        "      iterations_per_epoch=ipe,\n",
        "      warmup=warmup,\n",
        "      num_epochs=num_epochs,\n",
        "  )\n",
        "\n",
        "  #if xm.xrt_world_size() > 1:\n",
        "  #    encoder = DistributedDataParallel(encoder, broadcast_buffers=False)\n",
        "\n",
        "start_epoch = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICERXWGsYSfU"
      },
      "source": [
        "# train_loop_fn(supervised_loader, unsupervised_loader, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BJtaswkYaen"
      },
      "source": [
        "epoch = 0 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRJLPjdsYSi9"
      },
      "source": [
        "\n",
        "# -- update distributed-data-loader epoch\n",
        "unsupervised_sampler.set_epoch(epoch)\n",
        "#if supervised_sampler is not None:\n",
        "#    supervised_sampler.set_epoch(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZdPBuBDYjr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619f307b-cad0-4c59-d028-6abbeb8069fe"
      },
      "source": [
        "for itr, udata in enumerate(unsupervised_loader):\n",
        "  print(\"Entre a `for itr, udata`\")\n",
        "  def load_imgs():\n",
        "      # -- unsupervised imgs\n",
        "      uimgs = [u.to(device, non_blocking=True) for u in udata[:-1]]\n",
        "      # -- supervised imgs\n",
        "      global iter_supervised\n",
        "      try:\n",
        "          sdata = next(iter_supervised)\n",
        "      except Exception:\n",
        "          iter_supervised = iter(supervised_loader)\n",
        "          logger.info(f\"len.supervised_loader: {len(iter_supervised)}\")\n",
        "          sdata = next(iter_supervised)\n",
        "      finally:\n",
        "          idx = sdata[1].clone().detach()\n",
        "          labels_matrix = torch.zeros(len(idx), idx.max()+1).scatter_(1, idx.unsqueeze(1), 1.)\n",
        "          labels_matrix = labels_matrix.to(device)\n",
        "          labels = torch.cat([labels_matrix for _ in range(supervised_views)])\n",
        "          simgs = [s.to(device, non_blocking=True) for s in sdata[:-1]]\n",
        "      # -- concatenate supervised imgs and unsupervised imgs\n",
        "      imgs = simgs + uimgs\n",
        "      return imgs, labels\n",
        "\n",
        "  imgs, labels = load_imgs()\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entre a `for itr, udata`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yUkGkoV8TQ7p",
        "outputId": "afefe67c-4d13-4e8c-b8cf-55a6f84effe3"
      },
      "source": [
        "\"\"\"\n",
        "PREGUNTA: \n",
        "  1. Puedo utilizar esto sin el Sampler?. \n",
        "  2. Como deberia hacer el labels matrix para que me funcione!. Hacerlo!. \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nPREGUNTA: \\n  1. Puedo utilizar esto sin el Sampler?. \\n  2. Como deberia hacer el labels matrix para que me funcione!. Hacerlo!. \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPgb6BMcYylU"
      },
      "source": [
        "imgs, labels = load_imgs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FBzwArKajXp"
      },
      "source": [
        "# def train_step():"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk7JwNw5qWIB"
      },
      "source": [
        "optimizer.zero_grad()\n",
        "\n",
        "# --\n",
        "# h: representations of 'imgs' before head\n",
        "# z: representations of 'imgs' after head\n",
        "# -- If use_pred_head=False, then encoder.pred (prediction\n",
        "#    head) is None, and _forward_head just returns the\n",
        "#    identity, z=h\n",
        "h, z = encoder(imgs, return_before_head=True)\n",
        "\n",
        "h, z = h.float(), z.float()\n",
        "\n",
        "num_support = (\n",
        "    supervised_views * s_batch_size * classes_per_batch\n",
        ")\n",
        "\n",
        "anchor_supports = z[:num_support]\n",
        "anchor_views = z[num_support:]\n",
        "\n",
        "target_supports = h[:num_support].detach()\n",
        "target_views = h[num_support:].detach()\n",
        "\n",
        "target_views = torch.cat(\n",
        "    [\n",
        "        target_views[u_batch_size : 2 * u_batch_size],\n",
        "        target_views[:u_batch_size],\n",
        "    ],\n",
        "    dim=0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyA6o4s7qhuN",
        "outputId": "15338782-5553-453c-e514-94c3e3300a07"
      },
      "source": [
        "h.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([272, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5waBNLJbfRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234b427e-766d-49e7-93ba-6f9c39222567"
      },
      "source": [
        "ploss, me_max = my_loss_func(\n",
        "    anchor_views=anchor_views,\n",
        "    anchor_supports=anchor_supports,\n",
        "    anchor_support_labels=labels,\n",
        "    target_views=target_views,\n",
        "    target_supports=target_supports,\n",
        "    target_support_labels=labels,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGUF2inEyDHM"
      },
      "source": [
        "loss = ploss + me_max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNJvM6JDjvdb",
        "outputId": "025df868-537f-479f-d836-d0183e724c04"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2600, device='xla:1', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bMfGNr2178j",
        "outputId": "8f2102fc-5ae0-4810-a74b-161f34778307"
      },
      "source": [
        "ploss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5331, device='xla:1', grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9ToG1W52YDp",
        "outputId": "4196bd91-fc78-4404-93d0-7b8a5b5adb2a"
      },
      "source": [
        "me_max"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.2731, device='xla:1', grad_fn=<RsubBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8IcsySXkB2q"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "I--Mon0Dm9oY",
        "outputId": "e15cd871-4371-4213-c486-7f142bb8daa7"
      },
      "source": [
        "xm.optimizer_step(optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-8c9dcfe9e01b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(optimizer, barrier, optimizer_args, groups)\u001b[0m\n\u001b[1;32m    779\u001b[0m   \"\"\"\n\u001b[1;32m    780\u001b[0m   \u001b[0mreduce_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptimizer_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbarrier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0mmark_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/suncet/src/lars.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0madaptive_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrust_coefficient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam_norm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                         \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaptive_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0madaptive_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/suncet/src/utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, val, n)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8aNkJvd5y_J",
        "outputId": "ab9c2933-b7db-43a0-9d67-55e2f5409563"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    initial_lr: 0.001\n",
              "    lr: 0.001\n",
              "    momentum: 0.9\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              "\n",
              "Parameter Group 1\n",
              "    LARS_exclude: True\n",
              "    initial_lr: 0.001\n",
              "    lr: 0.001\n",
              "    momentum: 0.9\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8zko5ZuTqyr"
      },
      "source": [
        "##########################################################\n",
        "##########################################################\n",
        "##########################################################\n",
        "##########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG4xTLJxT0WA"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMl96KLoCq8"
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "# Only instantiate model weights once in memory.\n",
        "#WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
        "\n",
        "def train_resnet18():\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  ############# PAWS CODE ##################\n",
        "\n",
        "  device = xm.xla_device()\n",
        "\n",
        "  # -- init model\n",
        "  encoder = init_model(\n",
        "    device=device,\n",
        "    model_name=model_name,\n",
        "    use_pred=use_pred_head,\n",
        "    output_dim=output_dim,\n",
        "  )\n",
        "\n",
        "  # -- init losses\n",
        "  #paws = init_paws_loss(multicrop=multicrop, tau=temperature, T=sharpen, me_max=reg)\n",
        "  # -- assume support images are sampled with ClassStratifiedSampler\n",
        "  labels_matrix = make_labels_matrix(\n",
        "    num_classes=classes_per_batch,\n",
        "    s_batch_size=s_batch_size,\n",
        "    world_size=xm.xrt_world_size(),\n",
        "    device=device,\n",
        "    unique_classes=unique_classes,\n",
        "    smoothing=label_smoothing,\n",
        "  ) \n",
        "\n",
        "  print(\"normalize {}\".format(normalize))\n",
        "\n",
        "# -- make data transforms\n",
        "  transform, init_transform = make_transforms(\n",
        "    dataset_name=dataset_name,\n",
        "    subset_path=subset_path,\n",
        "    unlabeled_frac=unlabeled_frac,\n",
        "    training=True,\n",
        "    split_seed=data_seed,\n",
        "    crop_scale=crop_scale,\n",
        "    basic_augmentations=False,\n",
        "    color_jitter=color_jitter,\n",
        "    normalize=normalize,\n",
        "  )\n",
        "  multicrop_transform = (multicrop, None)\n",
        "  if multicrop > 0:\n",
        "    multicrop_transform = make_multicrop_transform(\n",
        "        dataset_name=dataset_name,\n",
        "        num_crops=multicrop,\n",
        "        size=mc_size,\n",
        "        crop_scale=mc_scale,\n",
        "        normalize=normalize,\n",
        "        color_distortion=color_jitter,\n",
        "    )\n",
        "\n",
        "# -- init data-loaders/samplers\n",
        "  (\n",
        "    unsupervised_loader,\n",
        "    unsupervised_sampler,\n",
        "    supervised_loader,\n",
        "    supervised_sampler,\n",
        ") = init_data(\n",
        "    dataset_name=dataset_name,\n",
        "    transform=transform,\n",
        "    init_transform=init_transform,\n",
        "    supervised_views=supervised_views,\n",
        "    u_batch_size=u_batch_size,\n",
        "    s_batch_size=s_batch_size,\n",
        "    unique_classes=unique_classes,\n",
        "    classes_per_batch=classes_per_batch,\n",
        "    multicrop_transform=multicrop_transform,\n",
        "    world_size=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    root_path=root_path,\n",
        "    s_image_folder=s_image_folder,\n",
        "    u_image_folder=u_image_folder,\n",
        "    training=True,\n",
        "    copy_data=copy_data,\n",
        "  )\n",
        "  #iter_supervised = None\n",
        "  ipe = len(unsupervised_loader)\n",
        "\n",
        "  print(\"len unsuupervised dataloader IS: {}\".format(ipe))\n",
        "\n",
        "\n",
        "  logger.info(f\"iterations per epoch: {ipe}\")\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "  scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "  encoder, optimizer, scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        weight_decay=wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        ref_mom=mom,\n",
        "        nesterov=nesterov,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "  \n",
        "    #if xm.xrt_world_size() > 1:\n",
        "    #    encoder = DistributedDataParallel(encoder, broadcast_buffers=False)\n",
        "\n",
        "  start_epoch = 0\n",
        "\n",
        "  def train_loop_fn(supervised_loader, unsupervised_loader, epoch):\n",
        "      print(\"Entre a `train_loop_fn`\")\n",
        "\n",
        "      # -- TRAINING LOOP\n",
        "      best_loss = None\n",
        "\n",
        "      logger.info(\"Epoch %d\" % (epoch + 1))\n",
        "\n",
        "      \"\"\"\n",
        "      TO DO: Hacer un Double CHECK EN este Sampler!!!\n",
        "      \"\"\"\n",
        "      # -- update distributed-data-loader epoch\n",
        "      unsupervised_sampler.set_epoch(epoch)\n",
        "      #if supervised_sampler is not None:\n",
        "      #    supervised_sampler.set_epoch(epoch)\n",
        "\n",
        "      loss_meter = AverageMeter()\n",
        "      ploss_meter = AverageMeter()\n",
        "      rloss_meter = AverageMeter()\n",
        "      time_meter = AverageMeter()\n",
        "      data_meter = AverageMeter()\n",
        "\n",
        "      print(\"len supervised loader dentro de 'train_loop_fn` is {}\".format(len(supervised_loader)))\n",
        "      print(\"len unsupervised_loader loader dentro de 'train_loop_fn` is {}\".format(len(unsupervised_loader)))\n",
        "\n",
        "      for itr, udata in enumerate(unsupervised_loader):\n",
        "          print(\"Entre a `for itr, udata`\")\n",
        "          def load_imgs(supervised_loader):\n",
        "              # -- unsupervised imgs\n",
        "              uimgs = [u.to(device, non_blocking=True) for u in udata[:-1]]\n",
        "              # -- supervised imgs\n",
        "              global iter_supervised\n",
        "              print(\"len supervised loader is {}\".format(len(supervised_loader)))\n",
        "              try:\n",
        "                  print(\"Entro al Try\")\n",
        "                  sdata = next(iter_supervised)\n",
        "              except Exception:\n",
        "                  print(\"Entro al Except!!\")\n",
        "                  iter_supervised = iter(supervised_loader)\n",
        "                  logger.info(f\"len.supervised_loader: {len(iter_supervised)}\")\n",
        "                  sdata = next(iter_supervised)\n",
        "              finally:\n",
        "                  idx = sdata[1].clone().detach()\n",
        "                  idx = idx.to(device)\n",
        "\n",
        "                  labels_matrix = torch.zeros(len(idx), idx.max()+1, device = device).scatter_(1, idx.unsqueeze(1), 1.)\n",
        "  \n",
        "                  labels_matrix = labels_matrix.to(device)\n",
        "                  labels = torch.cat([labels_matrix for _ in range(supervised_views)])\n",
        "                  simgs = [s.to(device, non_blocking=True) for s in sdata[:-1]]\n",
        "              # -- concatenate supervised imgs and unsupervised imgs\n",
        "              imgs = simgs + uimgs\n",
        "              return imgs, labels\n",
        "          \n",
        "          imgs, labels = load_imgs(supervised_loader)\n",
        "          \n",
        "          def train_step():\n",
        "              print(\"Entre a train_step()\")\n",
        "              \"\"\"\n",
        "              DEBUG HERE: EN alguna parte de Aqui se esta rompiendo!!!. \n",
        "              \"\"\"\n",
        "              #with torch.cuda.amp.autocast(enabled=use_fp16):\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              # --\n",
        "              # h: representations of 'imgs' before head\n",
        "              # z: representations of 'imgs' after head\n",
        "              # -- If use_pred_head=False, then encoder.pred (prediction\n",
        "              #    head) is None, and _forward_head just returns the\n",
        "              #    identity, z=h\n",
        "              h, z = encoder(imgs, return_before_head=True)\n",
        "\n",
        "              print(\"Extrajo h y z\")\n",
        "\n",
        "              # Compute paws loss in full precision\n",
        "              #with torch.cuda.amp.autocast(enabled=False):\n",
        "\n",
        "              # Step 1. convert representations to fp32\n",
        "              h, z = h.float(), z.float()\n",
        "              print(\"Step 1. convert representations to fp32\")\n",
        "\n",
        "              # Step 2. determine anchor views/supports and their\n",
        "              #         corresponding target views/supports\n",
        "              # --\n",
        "              print(\"Step 2. determine anchor views/supports and their corresponding target views/supports\")\n",
        "              \n",
        "              num_support = (\n",
        "                  supervised_views * s_batch_size * classes_per_batch\n",
        "              )\n",
        "\n",
        "              print(\"num_support is {}\".format(num_support))\n",
        "\n",
        "              # --\n",
        "              anchor_supports = z[:num_support]\n",
        "              anchor_views = z[num_support:]\n",
        "              # --\n",
        "              target_supports = h[:num_support].detach()\n",
        "              target_views = h[num_support:].detach()\n",
        "              target_views = torch.cat(\n",
        "                  [\n",
        "                      target_views[u_batch_size : 2 * u_batch_size],\n",
        "                      target_views[:u_batch_size],\n",
        "                  ],\n",
        "                  dim=0,\n",
        "              )\n",
        "\n",
        "              # Step 3. compute paws loss with me-max regularization\n",
        "              print(\"# Step 3. compute paws loss with me-max regularization\")\n",
        "\n",
        "              #ploss, me_max ,lr_stats= 0,0,0 # FAKE LOSS \n",
        "\n",
        "              print(\"`anchor_views` shape is {}\".format(anchor_views.shape))\n",
        "              print(\"`anchor_supports` shape is {}\".format(anchor_supports.shape))\n",
        "              print(\"`labels shape` is {}\".format(labels.shape))\n",
        "              print(\"`target_views` shape is {}\".format(target_views.shape))\n",
        "              print(\"`target_supports` shape is {}\".format(target_supports.shape))\n",
        "\n",
        "              ploss, me_max = my_loss_func(\n",
        "                  anchor_views=anchor_views,\n",
        "                  anchor_supports=anchor_supports,\n",
        "                  anchor_support_labels=labels,\n",
        "                  target_views=target_views,\n",
        "                  target_supports=target_supports,\n",
        "                  target_support_labels=labels,\n",
        "              )\n",
        "\n",
        "              print(\"Termine Calculo de PAWS loss\")\n",
        "\n",
        "              loss = ploss + me_max\n",
        "\n",
        "              print(\"loss is {}\".format(loss))\n",
        "\n",
        "              print(\"Doing loss backward\")\n",
        "              loss.backward()\n",
        "              print(\"Optimizing step optimizer\")\n",
        "              xm.optimizer_step(optimizer)\n",
        "              print(\"Finish Optimizer Step\")\n",
        " \n",
        "              #scaler.scale(loss).backward() # DOUBLE CHECK HERE, SI ESTA CORDE CON TPU?\n",
        "              #lr_stats = scaler.step(optimizer) # FIX THIS !!!!\n",
        "              #scaler.update() # FIX THIS !!!!\n",
        "              #scheduler.step() # FIX THIS!!!\n",
        "              #return (float(loss), float(ploss), float(me_max), lr_stats)\n",
        "              return (float(loss), float(ploss), float(me_max))\n",
        "\n",
        "          (loss, ploss, rloss) = train_step()\n",
        "          #loss_meter.update(loss)\n",
        "          #ploss_meter.update(ploss)\n",
        "          #rloss_meter.update(rloss)\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  \"\"\"\n",
        "  # Scale learning rate to num cores\n",
        "  learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "  # Get loss function, optimizer, and model\n",
        "  device = xm.xla_device()\n",
        "  model = WRAPPED_MODEL.to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                        momentum=FLAGS['momentum'], weight_decay=5e-4)\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_loop_fn(loader):\n",
        "    tracker = xm.RateTracker()\n",
        "    model.train()\n",
        "    for x, (data, target) in enumerate(loader):\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      xm.optimizer_step(optimizer)\n",
        "      tracker.add(FLAGS['batch_size'])\n",
        "      if x % FLAGS['log_steps'] == 0:\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
        "            tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "\n",
        "  # Train and eval loops\n",
        "  accuracy = 0.0\n",
        "  data, pred, target = None, None, None\n",
        "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    train_loop_fn(para_loader.per_device_loader(device))\n",
        "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "  return accuracy, data, pred, target\n",
        "  \"\"\"\n",
        "  data, pred, target = None, None, None\n",
        "\n",
        "  train_supervised_loader = pl.MpDeviceLoader(supervised_loader, device)\n",
        "  #train_supervised_loader = supervised_loader # YOOO\n",
        "  train_unsupervised_loader = pl.MpDeviceLoader(unsupervised_loader, device)\n",
        "\n",
        "  #for epoch in range(start_epoch, end_epoch):\n",
        "  for epoch in range(start_epoch, 10):\n",
        "      print(\"Epoch is {}\".format(epoch))\n",
        "      #para_loader_1 = pl.ParallelLoader(supervised_loader, [device])\n",
        "      #para_loader_2 = pl.ParallelLoader(unsupervised_loader, [device])\n",
        "      # train_loop_fn(para_loader.per_device_loader(device))\n",
        "      #train_loop_fn(train_supervised_loader, train_unsupervised_loader, epoch)\n",
        "      train_loop_fn(train_supervised_loader, train_unsupervised_loader, epoch)\n",
        "      xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "  #return accuracy, data, pred, target\n",
        "  return 0, 0, 0, 0"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7245c62-20d6-4931-846c-59d48140aba1"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  accuracy, data, pred, target = train_resnet18()\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878cca3d0>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878ccd310>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "Entre a train_step()\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "loss is -0.0066547393798828125\n",
            "Doing loss backward\n",
            "Optimizing step optimizer\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878d41d90>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "Entre a train_step()\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878cce5d0>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878d43a10>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "loss is 0.30544400215148926\n",
            "Doing loss backward\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878ccb490>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "Optimizing step optimizer\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878cd7050>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fb878cc8750>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Epoch is 0\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a train_step()\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "Entre a train_step()\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "Entre a train_step()\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "loss is 0.003358900547027588\n",
            "Doing loss backward\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "loss is 0.14205539226531982\n",
            "Doing loss backward\n",
            "Entre a train_step()\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "Optimizing step optimizer\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "loss is 0.013239562511444092\n",
            "Doing loss backward\n",
            "Optimizing step optimizer\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "Optimizing step optimizer\n",
            "loss is 0.00756758451461792\n",
            "Doing loss backward\n",
            "Optimizing step optimizer\n",
            "Entre a train_step()\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "Entre a train_step()\n",
            "Extrajo h y z\n",
            "Step 1. convert representations to fp32\n",
            "Step 2. determine anchor views/supports and their corresponding target views/supports\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "loss is 0.02000051736831665\n",
            "loss is 0.006768405437469482\n",
            "Doing loss backward\n",
            "Doing loss backward\n",
            "Optimizing step optimizer\n",
            "Optimizing step optimizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6apeUZ9kwie"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}