{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " PAWS_DEBUG 200000 (Work in Progress incorporar PAWS de a poco) - Copy of PyTorch/XLA ResNet18/CIFAR10 Training",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmarrietar/ocular/blob/master/notebooks/PAWS_DEBUG_200000_(Work_in_Progress_incorporar_PAWS_de_a_poco)_Copy_of_PyTorch_XLA_ResNet18_CIFAR10_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O53lrJMDn9Rd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75623e86-0ded-4387-f257-db0caa74038e"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-xla==1.8.1\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl (145.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 145.0 MB 11 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.34.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.278 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy5ndp3nJPbD",
        "outputId": "24418e12-d234-49b2-ed35-55adfa739058"
      },
      "source": [
        "!pip uninstall torch -y\n",
        "!pip install torch==1.8.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.9.0+cu102\n",
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 1.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Micd3xZvoA-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5bc7d3-1493-432e-8c35-58ab59225dc6"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import argparse\n",
        "import yaml\n",
        "import pprint\n",
        "import logging\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.8.1...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.8.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J55IUqtQ1fF"
      },
      "source": [
        "from src.data_manager import GaussianBlur"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "mskoYvt0MCaq",
        "outputId": "0c467166-3d62-42c6-c675-2e8f22359f8f"
      },
      "source": [
        "pip install -U PyYAML"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Collecting PyYAML\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l9EOMw3yQQlX",
        "outputId": "fb877781-8fc9-403a-ea81-3de9bb3a0955"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6KiMba_MHkU",
        "outputId": "90189b57-d12a-495f-fcda-05792db2fc99"
      },
      "source": [
        "!git clone -b feature/DR-images-v2 https://github.com/jmarrietar/suncet.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'suncet'...\n",
            "remote: Enumerating objects: 351, done.\u001b[K\n",
            "remote: Counting objects: 100% (351/351), done.\u001b[K\n",
            "remote: Compressing objects: 100% (230/230), done.\u001b[K\n",
            "remote: Total 351 (delta 211), reused 247 (delta 118), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (351/351), 1.11 MiB | 8.48 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtWmyuh4MK0J",
        "outputId": "b99e4abf-f189-4de6-bca6-5a7bc99328db"
      },
      "source": [
        "cd suncet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/suncet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mKok-0lMK4s"
      },
      "source": [
        "!mkdir datasets\n",
        "!mkdir datasets/dr\n",
        "!mkdir logs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTLU6dWXMK9B",
        "outputId": "d3c37773-bfbd-496b-9227-431dc5f422ce"
      },
      "source": [
        "!python download.py -d sample@2000"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\n",
            "To: /content/suncet/datasets/dr/sample@2000.zip\n",
            "214MB [00:01, 141MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pCIJ3u5MWBZ",
        "outputId": "9cfc9b1d-c9d0-4b2c-b431-e58d332f3c50"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\n",
        "    '--fname', type=str,\n",
        "    help='name of config file to load',\n",
        "    default='configs.yaml')\n",
        "parser.add_argument(\n",
        "    '--devices', type=str, nargs='+', default=['cuda:0'],\n",
        "    help='which devices to use on local machine')\n",
        "parser.add_argument(\n",
        "    '--sel', type=str,\n",
        "    help='which script to run',\n",
        "    choices=[\n",
        "        'paws_train',\n",
        "        'suncet_train',\n",
        "        'fine_tune',\n",
        "        'snn_fine_tune'\n",
        "    ])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--sel'], dest='sel', nargs=None, const=None, default=None, type=<class 'str'>, choices=['paws_train', 'suncet_train', 'fine_tune', 'snn_fine_tune'], help='which script to run', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "halcMg65MqwY"
      },
      "source": [
        "args = parser.parse_args(['--sel', 'paws_train',\n",
        "                            '--fname', 'configs/paws/dr_train.yaml'\n",
        "])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz8KUfZZPNlq"
      },
      "source": [
        "fname = args.fname\n",
        "sel = args.sel"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7G3xNSPPUrl"
      },
      "source": [
        "logging.basicConfig()\n",
        "logger = logging.getLogger()\n",
        "\n",
        "logger.info(f'called-params {sel} {fname}')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iRvWJmdPWqr",
        "outputId": "c4c99295-fa38-4721-8f4e-9b6fabf80f13"
      },
      "source": [
        "# -- load script params\n",
        "params = None\n",
        "with open(fname, 'r') as y_file:\n",
        "    #params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
        "    params = yaml.load(y_file)\n",
        "    logger.info('loaded params...')\n",
        "    #if rank == 0:\n",
        "    pp = pprint.PrettyPrinter(indent=4)\n",
        "    pp.pprint(params)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{   'criterion': {   'classes_per_batch': 2,\n",
            "                     'me_max': True,\n",
            "                     'sharpen': 0.25,\n",
            "                     'supervised_imgs_per_class': 8,\n",
            "                     'supervised_views': 1,\n",
            "                     'temperature': 0.1,\n",
            "                     'unsupervised_batch_size': 32},\n",
            "    'data': {   'color_jitter_strength': 1.0,\n",
            "                'data_seed': None,\n",
            "                'dataset': 'dr',\n",
            "                'label_smoothing': 0.1,\n",
            "                'multicrop': 6,\n",
            "                'normalize': True,\n",
            "                'root_path': 'datasets/',\n",
            "                's_image_folder': 'dr/sample@2000/',\n",
            "                'subset_path': 'dr_subsets',\n",
            "                'u_image_folder': 'dr/sample@2000/',\n",
            "                'unique_classes_per_rank': False,\n",
            "                'unlabeled_frac': 0.9},\n",
            "    'logging': {'folder': 'logs/', 'write_tag': 'paws'},\n",
            "    'meta': {   'copy_data': True,\n",
            "                'device': 'cuda:0',\n",
            "                'load_checkpoint': False,\n",
            "                'model_name': 'resnet50',\n",
            "                'output_dim': 2048,\n",
            "                'read_checkpoint': None,\n",
            "                'use_fp16': True,\n",
            "                'use_pred_head': True},\n",
            "    'optimization': {   'epochs': 100,\n",
            "                        'final_lr': 1e-05,\n",
            "                        'lr': 0.001,\n",
            "                        'momentum': 0.9,\n",
            "                        'nesterov': False,\n",
            "                        'start_lr': 0.001,\n",
            "                        'warmup': 10,\n",
            "                        'weight_decay': 1e-06}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IyTUPjAPYst"
      },
      "source": [
        "args = params"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMdPRFXIn_jH"
      },
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['data_dir'] = \"/tmp/cifar\"\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 2\n",
        "FLAGS['learning_rate'] = 0.02\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['num_cores'] = 8 if os.environ.get('TPU_NAME', None) else 1\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3npOhg9RuaH"
      },
      "source": [
        "FLAGS['model_name'] = args[\"meta\"][\"model_name\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8IkOQ9iZTzQ"
      },
      "source": [
        "model_name = args[\"meta\"][\"model_name\"]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzhD4JoAS5MP"
      },
      "source": [
        "output_dim = args[\"meta\"][\"output_dim\"]\n",
        "multicrop = args[\"data\"][\"multicrop\"]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFQzNvSTTFax"
      },
      "source": [
        "# -- CRITERTION\n",
        "reg = args[\"criterion\"][\"me_max\"]\n",
        "supervised_views = args[\"criterion\"][\"supervised_views\"]\n",
        "classes_per_batch = args[\"criterion\"][\"classes_per_batch\"]\n",
        "s_batch_size = args[\"criterion\"][\"supervised_imgs_per_class\"]\n",
        "u_batch_size = args[\"criterion\"][\"unsupervised_batch_size\"]\n",
        "temperature = args[\"criterion\"][\"temperature\"]\n",
        "sharpen = args[\"criterion\"][\"sharpen\"]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VPX6eJ7UgMK"
      },
      "source": [
        "# -- DATA\n",
        "unlabeled_frac = args[\"data\"][\"unlabeled_frac\"]\n",
        "color_jitter = args[\"data\"][\"color_jitter_strength\"]\n",
        "normalize = args[\"data\"][\"normalize\"]\n",
        "root_path = args[\"data\"][\"root_path\"]\n",
        "s_image_folder = args[\"data\"][\"s_image_folder\"]\n",
        "u_image_folder = args[\"data\"][\"u_image_folder\"]\n",
        "dataset_name = args[\"data\"][\"dataset\"]\n",
        "subset_path = args[\"data\"][\"subset_path\"]\n",
        "unique_classes = args[\"data\"][\"unique_classes_per_rank\"]\n",
        "label_smoothing = args[\"data\"][\"label_smoothing\"]\n",
        "data_seed = None"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBzLKYbOUtYO"
      },
      "source": [
        "crop_scale = (0.14, 1.0) if multicrop > 0 else (0.08, 1.0)\n",
        "mc_scale = (0.05, 0.14)\n",
        "mc_size = 96"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZVxR2ZU7pn"
      },
      "source": [
        "copy_data = args[\"meta\"][\"copy_data\"]\n",
        "use_pred_head = args[\"meta\"][\"use_pred_head\"]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqLceQ2_aoGr"
      },
      "source": [
        "use_fp16 = args[\"meta\"][\"use_fp16\"]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s_OIC5Ma-1V"
      },
      "source": [
        "# -- OPTIMIZATION\n",
        "wd = float(args[\"optimization\"][\"weight_decay\"])\n",
        "num_epochs = args[\"optimization\"][\"epochs\"]\n",
        "warmup = args[\"optimization\"][\"warmup\"]\n",
        "start_lr = args[\"optimization\"][\"start_lr\"]\n",
        "lr = args[\"optimization\"][\"lr\"]\n",
        "final_lr = args[\"optimization\"][\"final_lr\"]\n",
        "mom = args[\"optimization\"][\"momentum\"]\n",
        "nesterov = args[\"optimization\"][\"nesterov\"]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lEMWRcAPa38"
      },
      "source": [
        "# ----------------------------------------------------------------------- #\n",
        "#  PASSED IN PARAMS FROM CONFIG FILE\n",
        "# ----------------------------------------------------------------------- #\n",
        "# -- META\n",
        "\n",
        "\n",
        "load_model = args[\"meta\"][\"load_checkpoint\"]\n",
        "r_file = args[\"meta\"][\"read_checkpoint\"]\n",
        "\n",
        "\n",
        "#device = torch.device(args[\"meta\"][\"device\"])\n",
        "device = xm.xla_device()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -- LOGGING\n",
        "folder = args[\"logging\"][\"folder\"]\n",
        "tag = args[\"logging\"][\"write_tag\"]\n",
        "# ----------------------------------------------------------------------- #\n",
        "\n",
        "# -- init torch distributed backend\n",
        "#world_size, rank = init_distributed()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7N60VarMvrC"
      },
      "source": [
        "import logging\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "import traceback\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import src.resnet as resnet\n",
        "import src.wide_resnet as wide_resnet\n",
        "from src.utils import (\n",
        "    gpu_timer,\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule,\n",
        "    CSVLogger,\n",
        "    AverageMeter,\n",
        ")\n",
        "from src.losses import init_paws_loss, make_labels_matrix\n",
        "from src.data_manager import init_data, make_transforms, make_multicrop_transform\n",
        "from src.sgd import SGD\n",
        "from src.lars import LARS\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "#import apex\n",
        "from torch.nn.parallel import DistributedDataParallel"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCx_JdBzD05x"
      },
      "source": [
        "from src.paws_train import init_model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsTSK2JqDcnQ"
      },
      "source": [
        "from src.data_manager import ImageNet"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWD0feLVfKo-"
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "from logging import getLogger\n",
        "\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "\n",
        "import torch\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from PIL import ImageFilter\n",
        "\n",
        "_GLOBAL_SEED = 0\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def init_data(\n",
        "    dataset_name,\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch,\n",
        "    world_size,\n",
        "    rank,\n",
        "    unique_classes=False,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    root_path=None,\n",
        "    s_image_folder=None,\n",
        "    u_image_folder=None,\n",
        "    image_folder=None,\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    stratify=False,\n",
        "    drop_last=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    :param dataset_name: ['imagenet', 'cifar10', 'cifar10_fine_tune', 'imagenet_fine_tune']\n",
        "    :param transform: torchvision transform to apply to each batch of data\n",
        "    :param init_transform: transform to apply once to all data at the start\n",
        "    :param u_batch_size: unsupervised batch-size\n",
        "    :param s_batch_size: supervised batch-size (images per class)\n",
        "    :param classes_per_batch: num. classes sampled in each supervised batch per gpu\n",
        "    :param unique_classes: whether each GPU should load different classes\n",
        "    :param multicrop_transform: number of smaller multi-crop images to return\n",
        "    :param supervised_views: number of views to generate of each labeled imgs\n",
        "    :param world_size: number of workers for distributed training\n",
        "    :param rank: rank of worker in distributed training\n",
        "    :param root_path: path to the root directory containing all dataset\n",
        "    :param image_folder: name of folder in 'root_path' containing data to load\n",
        "    :param training: whether to load training data\n",
        "    :param stratify: whether to class stratify 'fine_tune' data loaders\n",
        "    :param copy_data: whether to copy data locally to node at start of training\n",
        "    \"\"\"\n",
        "\n",
        "    if dataset_name == \"dr\":\n",
        "        return _init_dr_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            u_batch_size=u_batch_size,\n",
        "            s_batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            multicrop_transform=multicrop_transform,\n",
        "            supervised_views=supervised_views,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            s_image_folder=s_image_folder,\n",
        "            u_image_folder=u_image_folder,\n",
        "            training=training,\n",
        "            copy_data=copy_data,\n",
        "        )\n",
        "\n",
        "    elif dataset_name == \"dr_fine_tune\":\n",
        "        batch_size = s_batch_size\n",
        "        return _init_imgnt_ft_data(\n",
        "            transform=transform,\n",
        "            init_transform=init_transform,\n",
        "            batch_size=batch_size,\n",
        "            stratify=stratify,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            root_path=root_path,\n",
        "            image_folder=image_folder,\n",
        "            training=training,\n",
        "            drop_last=drop_last,\n",
        "            copy_data=copy_data,\n",
        "        )\n",
        "\n",
        "\n",
        "def _init_dr_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    u_batch_size,\n",
        "    s_batch_size,\n",
        "    classes_per_batch,\n",
        "    world_size,\n",
        "    rank,\n",
        "    unique_classes=False,\n",
        "    multicrop_transform=(0, None),\n",
        "    supervised_views=1,\n",
        "    root_path=\"/datasets/\",\n",
        "    s_image_folder=\"imagenet_full_size/061417/\",\n",
        "    u_image_folder=\"imagenet_full_size/061417/\",\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    tar_folder=\"imagenet_full_size/\",\n",
        "    tar_file=\"imagenet_full_size-061417.tar\",\n",
        "):\n",
        "    s_imagedr = ImageDR(\n",
        "        root=root_path,\n",
        "        image_folder=s_image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data,\n",
        "    )\n",
        "\n",
        "    u_imagedr = ImageDR(\n",
        "        root=root_path,\n",
        "        image_folder=u_image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data,\n",
        "    )\n",
        "    logger.info(\"ImageDR dataset created\")\n",
        "\n",
        "    logger.info(\"Making unsupervised ImageDR data loader...\")\n",
        "    unsupervised_set = TransImageDR(\n",
        "        dataset=u_imagedr,\n",
        "        supervised=False,\n",
        "        init_transform=init_transform,\n",
        "        multicrop_transform=multicrop_transform,\n",
        "        seed=_GLOBAL_SEED,\n",
        "    )\n",
        "    unsupervised_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset=unsupervised_set, num_replicas=world_size, rank=rank\n",
        "    )\n",
        "    unsupervised_loader = torch.utils.data.DataLoader(\n",
        "        unsupervised_set,\n",
        "        sampler=unsupervised_sampler,\n",
        "        batch_size=u_batch_size,\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "    )\n",
        "    logger.info(\"ImageDR unsupervised data loader created\")\n",
        "\n",
        "    supervised_sampler, supervised_loader = None, None\n",
        "    if classes_per_batch > 0 and s_batch_size > 0:\n",
        "        logger.info(\"Making supervised ImageDR data loader...\")\n",
        "\n",
        "        supervised_set = TransImageDR(\n",
        "            dataset=s_imagedr,\n",
        "            supervised=True,\n",
        "            supervised_views=supervised_views,\n",
        "            init_transform=init_transform,\n",
        "            seed=_GLOBAL_SEED,\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        supervised_sampler = ClassStratifiedSampler(\n",
        "            data_source=supervised_set,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=s_batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            unique_classes=unique_classes,\n",
        "            seed=_GLOBAL_SEED,\n",
        "        )\n",
        "        supervised_loader = torch.utils.data.DataLoader(\n",
        "            supervised_set,\n",
        "            batch_sampler=supervised_sampler,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "        \"\"\"\n",
        "        #supervised_set = datasets.ImageFolder(\n",
        "        #    root=root_path+s_image_folder\n",
        "        #)\n",
        "\n",
        "        supervised_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset=supervised_set, num_replicas=world_size, rank=rank\n",
        "        )\n",
        "\n",
        "        supervised_loader = torch.utils.data.DataLoader(\n",
        "            supervised_set,\n",
        "            sampler=supervised_sampler,\n",
        "            batch_size=s_batch_size*2, # YOO LE Agrege la Multiplicacion\n",
        "            drop_last=True,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "        \"\"\"\n",
        "        if len(supervised_loader) > 0:\n",
        "            tmp = ceil(len(unsupervised_loader) / len(supervised_loader))\n",
        "            supervised_sampler.set_inner_epochs(tmp)\n",
        "            logger.info(f\"supervised-reset-period {tmp}\")\n",
        "        \"\"\"\n",
        "        logger.info(\"ImageDR supervised data loader created\")\n",
        "\n",
        "    return (\n",
        "        unsupervised_loader,\n",
        "        unsupervised_sampler,\n",
        "        supervised_loader,\n",
        "        supervised_sampler,\n",
        "    )\n",
        "\n",
        "\n",
        "def _init_imgnt_ft_data(\n",
        "    transform,\n",
        "    init_transform,\n",
        "    batch_size,\n",
        "    world_size,\n",
        "    rank,\n",
        "    stratify=False,\n",
        "    classes_per_batch=1,\n",
        "    unique_classes=False,\n",
        "    root_path=\"/datasets/\",\n",
        "    image_folder=\"imagenet_full_size/061417/\",\n",
        "    training=True,\n",
        "    copy_data=False,\n",
        "    drop_last=True,\n",
        "    tar_folder=\"imagenet_full_size/\",\n",
        "    tar_file=\"imagenet_full_size-061417.tar\",\n",
        "):\n",
        "    imagenet = ImageDR(\n",
        "        root=root_path,\n",
        "        image_folder=image_folder,\n",
        "        tar_folder=tar_folder,\n",
        "        tar_file=tar_file,\n",
        "        transform=transform,\n",
        "        train=training,\n",
        "        copy_data=copy_data,\n",
        "    )\n",
        "    logger.info(\"ImageNet fine-tune dataset created\")\n",
        "    dataset = TransImageDR(\n",
        "        dataset=imagenet,\n",
        "        supervised=True,\n",
        "        init_transform=init_transform,\n",
        "        seed=_GLOBAL_SEED,\n",
        "    )\n",
        "\n",
        "    if not stratify:\n",
        "        dist_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "            dataset=dataset, num_replicas=world_size, rank=rank\n",
        "        )\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            sampler=dist_sampler,\n",
        "            batch_size=batch_size,\n",
        "            drop_last=drop_last,\n",
        "            pin_memory=True,\n",
        "            num_workers=8,\n",
        "        )\n",
        "    else:\n",
        "        dist_sampler = ClassStratifiedSampler(\n",
        "            data_source=dataset,\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            batch_size=batch_size,\n",
        "            classes_per_batch=classes_per_batch,\n",
        "            seed=_GLOBAL_SEED,\n",
        "            unique_classes=unique_classes,\n",
        "        )\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_sampler=dist_sampler, pin_memory=True, num_workers=8\n",
        "        )\n",
        "\n",
        "    return (data_loader, dist_sampler)\n",
        "\n",
        "\n",
        "def make_transforms(\n",
        "    dataset_name,\n",
        "    subset_path=None,\n",
        "    unlabeled_frac=1.0,\n",
        "    training=True,\n",
        "    basic_augmentations=False,\n",
        "    force_center_crop=False,\n",
        "    crop_scale=(0.08, 1.0),\n",
        "    color_jitter=1.0,\n",
        "    normalize=False,\n",
        "    split_seed=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    :param dataset_name: ['imagenet', 'cifar10']\n",
        "    :param subset_path: path to .txt file denoting subset of data to use\n",
        "    :param unlabeled_frac: fraction of data that is unlabeled\n",
        "    :param training: whether to load training data\n",
        "    :param basic_augmentations: whether to use simple data-augmentations\n",
        "    :param force_center_crop: whether to force use of a center-crop\n",
        "    :param color_jitter: strength of color-jitter\n",
        "    :param normalize: whether to normalize color channels\n",
        "    \"\"\"\n",
        "\n",
        "    if \"dr\" in dataset_name:\n",
        "        logger.info(\"making imagenet data transforms\")\n",
        "\n",
        "        # -- file identifying which imagenet labels to keep\n",
        "        keep_file = None\n",
        "        if subset_path is not None:\n",
        "            if unlabeled_frac >= 0:\n",
        "                keep_file = os.path.join(\n",
        "                    subset_path, f\"{int(unlabeled_frac* 100)}percent.txt\"\n",
        "                )\n",
        "            else:\n",
        "                keep_file = os.path.join(subset_path, \"val.txt\")\n",
        "            logger.info(f\"keep file: {keep_file}\")\n",
        "\n",
        "        return _make_imgnt_transforms(\n",
        "            unlabel_prob=unlabeled_frac,\n",
        "            training=training,\n",
        "            basic=basic_augmentations,\n",
        "            force_center_crop=force_center_crop,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_jitter,\n",
        "            scale=crop_scale,\n",
        "            keep_file=keep_file,\n",
        "        )\n",
        "\n",
        "\n",
        "def _make_imgnt_transforms(\n",
        "    unlabel_prob,\n",
        "    training=True,\n",
        "    basic=False,\n",
        "    force_center_crop=False,\n",
        "    normalize=False,\n",
        "    scale=(0.08, 1.0),\n",
        "    color_distortion=1.0,\n",
        "    keep_file=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Make data transformations\n",
        "\n",
        "    :param unlabel_prob: probability of sampling unlabeled data point\n",
        "    :param training: generate data transforms for train (alternativly test)\n",
        "    :param basic: whether train transforms include more sofisticated transforms\n",
        "    :param force_center_crop: whether to override settings and apply center crop to image\n",
        "    :param normalize: whether to normalize image means and stds\n",
        "    :param scale: random scaling range for image before resizing\n",
        "    :param color_distortion: strength of color distortion\n",
        "    :param keep_file: file containing names of images to use for semisupervised\n",
        "    \"\"\"\n",
        "\n",
        "    def get_color_distortion(s=1.0):\n",
        "        # s is the strength of color distortion.\n",
        "        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "        rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "        color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
        "        return color_distort\n",
        "\n",
        "    logger.debug(\n",
        "        f\"uprob: {unlabel_prob}\\t training: {training}\\t basic: {basic}\\t normalize: {normalize}\\t color_distortion: {color_distortion}\"\n",
        "    )\n",
        "    if training and (not force_center_crop):\n",
        "        if basic:\n",
        "            transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.RandomResizedCrop(size=224, scale=scale),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    transforms.ToTensor(),\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            logger.debug(\"making training (non-basic) transforms\")\n",
        "            transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.RandomResizedCrop(size=224, scale=scale),\n",
        "                    transforms.RandomHorizontalFlip(),\n",
        "                    get_color_distortion(s=color_distortion),\n",
        "                    GaussianBlur(p=0.5),\n",
        "                    transforms.ToTensor(),\n",
        "                ]\n",
        "            )\n",
        "    else:\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size=256),\n",
        "                transforms.CenterCrop(size=224),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transform,\n",
        "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def init_transform(\n",
        "        root, samples, class_to_idx, seed, keep_file=keep_file, training=training\n",
        "    ):\n",
        "        \"\"\"Transforms applied to dataset at the start of training\"\"\"\n",
        "\n",
        "        new_targets, new_samples = [], []\n",
        "        if training and (keep_file is not None) and os.path.exists(keep_file):\n",
        "            logger.info(f\"Using {keep_file}\")\n",
        "            with open(keep_file, \"r\") as rfile:\n",
        "                for line in rfile:\n",
        "                    class_name = line.split(\"_\")[0]\n",
        "                    target = class_to_idx[class_name]\n",
        "                    img = line.split(\"\\n\")[0]\n",
        "                    new_samples.append((os.path.join(root, class_name, img), target))\n",
        "                    new_targets.append(target)\n",
        "        else:\n",
        "            logger.info(\"flipping coin to keep labels\")\n",
        "            g = torch.Generator()\n",
        "            g.manual_seed(seed)\n",
        "            for sample in samples:\n",
        "                if torch.bernoulli(torch.tensor(unlabel_prob), generator=g) == 0:\n",
        "                    target = sample[1]\n",
        "                    new_samples.append((sample[0], target))\n",
        "                    new_targets.append(target)\n",
        "\n",
        "        return np.array(new_targets), np.array(new_samples)\n",
        "\n",
        "    return transform, init_transform\n",
        "\n",
        "\n",
        "def make_multicrop_transform(\n",
        "    dataset_name, num_crops, size, crop_scale, normalize, color_distortion\n",
        "):\n",
        "    if \"dr\" in dataset_name:\n",
        "        return _make_multicrop_imgnt_transforms(\n",
        "            num_crops=num_crops,\n",
        "            size=size,\n",
        "            scale=crop_scale,\n",
        "            normalize=normalize,\n",
        "            color_distortion=color_distortion,\n",
        "        )\n",
        "\n",
        "\n",
        "def _make_multicrop_imgnt_transforms(\n",
        "    num_crops, size=96, scale=(0.05, 0.14), normalize=False, color_distortion=1.0,\n",
        "):\n",
        "    def get_color_distortion(s=1.0):\n",
        "        color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "        rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "        rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "        color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
        "        return color_distort\n",
        "\n",
        "    logger.debug(\"making multicrop transforms\")\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomResizedCrop(size=size, scale=scale),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            get_color_distortion(s=color_distortion),\n",
        "            GaussianBlur(p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if normalize:\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transform,\n",
        "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    return (num_crops, transform)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP1JxultMMz0"
      },
      "source": [
        "class ImageNet(torchvision.datasets.ImageFolder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        image_folder=\"imagenet_full_size/061417/\",\n",
        "        tar_folder=\"imagenet_full_size/\",\n",
        "        tar_file=\"imagenet_full_size-061417.tar\",\n",
        "        train=True,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "        job_id=None,\n",
        "        local_rank=None,\n",
        "        copy_data=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ImageNet\n",
        "        Dataset wrapper (can copy data locally to machine)\n",
        "        :param root: root network directory for ImageNet data\n",
        "        :param image_folder: path to images inside root network directory\n",
        "        :param tar_file: zipped image_folder inside root network directory\n",
        "        :param train: whether to load train data (or validation)\n",
        "        :param transform: data-augmentations (applied in data-loader)\n",
        "        :param target_transform: target-transform to apply in data-loader\n",
        "        :param job_id: scheduler job-id used to create dir on local machine\n",
        "        :param copy_data: whether to copy data from network file locally\n",
        "        \"\"\"\n",
        "\n",
        "        suffix = \"train/\" if train else \"val/\"\n",
        "        data_path = None\n",
        "        if copy_data:\n",
        "            logger.info(\"copying data locally\")\n",
        "            data_path = copy_imgnt_locally(\n",
        "                root=root,\n",
        "                suffix=suffix,\n",
        "                image_folder=image_folder,\n",
        "                tar_folder=tar_folder,\n",
        "                tar_file=tar_file,\n",
        "                job_id=job_id,\n",
        "                local_rank=local_rank,\n",
        "            )\n",
        "        if (not copy_data) or (data_path is None):\n",
        "            data_path = os.path.join(root, image_folder, suffix)\n",
        "        logger.info(f\"data-path {data_path}\")\n",
        "\n",
        "        super(ImageNet, self).__init__(\n",
        "            root=data_path, transform=transform, target_transform=target_transform\n",
        "        )\n",
        "        logger.info(\"Initialized ImageNet\")\n",
        "\n",
        "class ImageDR(torchvision.datasets.ImageFolder):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        image_folder=\"imagenet_full_size/061417/\",\n",
        "        tar_folder=\"imagenet_full_size/\",\n",
        "        tar_file=\"imagenet_full_size-061417.tar\",\n",
        "        train=True,\n",
        "        transform=None,\n",
        "        target_transform=None,\n",
        "        job_id=None,\n",
        "        local_rank=None,\n",
        "        copy_data=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ImageNet\n",
        "\n",
        "        Dataset wrapper (can copy data locally to machine)\n",
        "\n",
        "        :param root: root network directory for DR data\n",
        "        :param image_folder: path to images inside root network directory\n",
        "        :param tar_file: zipped image_folder inside root network directory\n",
        "        :param train: whether to load train data (or validation)\n",
        "        :param transform: data-augmentations (applied in data-loader)\n",
        "        :param target_transform: target-transform to apply in data-loader\n",
        "        :param job_id: scheduler job-id used to create dir on local machine\n",
        "        :param copy_data: whether to copy data from network file locally\n",
        "        \"\"\"\n",
        "\n",
        "        suffix = \"train/\" if train else \"val/\"\n",
        "        data_path = None\n",
        "        if copy_data:\n",
        "            logger.info(\"copying data locally\")\n",
        "            data_path = copy_imgnt_locally(\n",
        "                root=root,\n",
        "                suffix=suffix,\n",
        "                image_folder=image_folder,\n",
        "                tar_folder=tar_folder,\n",
        "                tar_file=tar_file,\n",
        "                job_id=job_id,\n",
        "                local_rank=local_rank,\n",
        "            )\n",
        "        if (not copy_data) or (data_path is None):\n",
        "            data_path = os.path.join(root, image_folder, suffix)\n",
        "        logger.info(f\"data-path {data_path}\")\n",
        "\n",
        "        super(ImageDR, self).__init__(\n",
        "            root=data_path, transform=transform, target_transform=target_transform\n",
        "        )\n",
        "        logger.info(\"Initialized ImageDR\")\n",
        "\n",
        "\n",
        "class TransImageDR(ImageNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        supervised=False,\n",
        "        supervised_views=1,\n",
        "        init_transform=None,\n",
        "        multicrop_transform=(0, None),\n",
        "        seed=0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        TransImageDR\n",
        "\n",
        "        Dataset that can apply transforms to images on initialization and can\n",
        "        return multiple transformed copies of the same image in each call\n",
        "        to __getitem__\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.supervised = supervised\n",
        "        self.supervised_views = supervised_views\n",
        "        self.multicrop_transform = multicrop_transform\n",
        "\n",
        "        print(\"self.multicrop_transform {}\".format(self.multicrop_transform))\n",
        "\n",
        "        self.targets, self.samples = dataset.targets, dataset.samples\n",
        "        if self.supervised:\n",
        "            self.targets, self.samples = init_transform(\n",
        "                dataset.root, dataset.samples, dataset.class_to_idx, seed\n",
        "            )\n",
        "            logger.debug(f\"num-labeled {len(self.samples)}\")\n",
        "            mint = None\n",
        "            self.target_indices = []\n",
        "            for t in range(len(dataset.classes)):\n",
        "                indices = np.squeeze(np.argwhere(self.targets == t)).tolist()\n",
        "                self.target_indices.append(indices)\n",
        "                mint = len(indices) if mint is None else min(mint, len(indices))\n",
        "                logger.debug(f\"num-labeled target {t} {len(indices)}\")\n",
        "            logger.debug(f\"min. labeled indices {mint}\")\n",
        "\n",
        "    @property\n",
        "    def classes(self):\n",
        "        return self.dataset.classes\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        target = self.targets[index]\n",
        "        path = self.samples[index][0]\n",
        "        img = self.dataset.loader(path)\n",
        "\n",
        "        if self.dataset.target_transform is not None:\n",
        "            target = self.dataset.target_transform(target)\n",
        "\n",
        "        if self.dataset.transform is not None:\n",
        "            if self.supervised:\n",
        "                ans = (\n",
        "                    *[\n",
        "                        self.dataset.transform(img)\n",
        "                        for _ in range(self.supervised_views)\n",
        "                    ],\n",
        "                    target,\n",
        "                )\n",
        "                return ans\n",
        "            else:\n",
        "                img_1 = self.dataset.transform(img)\n",
        "                img_2 = self.dataset.transform(img)\n",
        "\n",
        "                multicrop, mc_transform = self.multicrop_transform\n",
        "                if multicrop > 0 and mc_transform is not None:\n",
        "                    mc_imgs = [mc_transform(img) for _ in range(int(multicrop))]\n",
        "                    ans = img_1, img_2, *mc_imgs, target\n",
        "                    return ans\n",
        "\n",
        "                return img_1, img_2, target\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def copy_imgnt_locally(\n",
        "    root,\n",
        "    suffix,\n",
        "    image_folder=\"imagenet_full_size/061417/\",\n",
        "    tar_folder=\"imagenet_full_size/\",\n",
        "    tar_file=\"imagenet_full_size-061417.tar\",\n",
        "    job_id=None,\n",
        "    local_rank=None,\n",
        "):\n",
        "    if job_id is None:\n",
        "        try:\n",
        "            job_id = os.environ[\"SLURM_JOBID\"]\n",
        "        except Exception:\n",
        "            logger.info(\"No job-id, will load directly from network file\")\n",
        "            return None\n",
        "\n",
        "    if local_rank is None:\n",
        "        try:\n",
        "            local_rank = int(os.environ[\"SLURM_LOCALID\"])\n",
        "        except Exception:\n",
        "            logger.info(\"No job-id, will load directly from network file\")\n",
        "            return None\n",
        "\n",
        "    source_file = os.path.join(root, tar_folder, tar_file)\n",
        "    target = f\"/scratch/slurm_tmpdir/{job_id}/\"\n",
        "    target_file = os.path.join(target, tar_file)\n",
        "    data_path = os.path.join(target, image_folder, suffix)\n",
        "    logger.info(f\"{source_file}\\n{target}\\n{target_file}\\n{data_path}\")\n",
        "\n",
        "    tmp_sgnl_file = os.path.join(target, \"copy_signal.txt\")\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        if local_rank == 0:\n",
        "            commands = [[\"tar\", \"-xf\", source_file, \"-C\", target]]\n",
        "            for cmnd in commands:\n",
        "                start_time = time.time()\n",
        "                logger.info(f\"Executing {cmnd}\")\n",
        "                subprocess.run(cmnd)\n",
        "                logger.info(f\"Cmnd took {(time.time()-start_time)/60.} min.\")\n",
        "            with open(tmp_sgnl_file, \"+w\") as f:\n",
        "                print(\"Done copying locally.\", file=f)\n",
        "        else:\n",
        "            while not os.path.exists(tmp_sgnl_file):\n",
        "                time.sleep(60)\n",
        "                logger.info(f\"{local_rank}: Checking {tmp_sgnl_file}\")\n",
        "\n",
        "    return data_path\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.0):\n",
        "        self.prob = p\n",
        "        self.radius_min = radius_min\n",
        "        self.radius_max = radius_max\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if torch.bernoulli(torch.tensor(self.prob)) == 0:\n",
        "            return img\n",
        "\n",
        "        radius = self.radius_min + torch.rand(1) * (self.radius_max - self.radius_min)\n",
        "        return img.filter(ImageFilter.GaussianBlur(radius=radius))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhgKoRE3OKlW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "72b76c5a-6bef-4dcf-846e-9a24bf35555b"
      },
      "source": [
        "\"\"\"\n",
        "def load_checkpoint(r_path, encoder, opt, scaler, use_fp16=False):\n",
        "    checkpoint = torch.load(r_path, map_location=\"cpu\")\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "\n",
        "    # -- loading encoder\n",
        "    encoder.load_state_dict(checkpoint[\"encoder\"])\n",
        "    logger.info(f\"loaded encoder from epoch {epoch}\")\n",
        "\n",
        "    # -- loading optimizer\n",
        "    opt.load_state_dict(checkpoint[\"opt\"])\n",
        "    if use_fp16:\n",
        "        scaler.load_state_dict(checkpoint[\"amp\"])\n",
        "    logger.info(f\"loaded optimizers from epoch {epoch}\")\n",
        "    logger.info(f\"read-path: {r_path}\")\n",
        "    del checkpoint\n",
        "    return encoder, opt, epoch\n",
        "\n",
        "\n",
        "def init_model(device, model_name=\"resnet50\", use_pred=False, output_dim=128):\n",
        "    if \"wide_resnet\" in model_name:\n",
        "        encoder = wide_resnet.__dict__[model_name](dropout_rate=0.0)\n",
        "        hidden_dim = 128\n",
        "    else:\n",
        "        encoder = resnet.__dict__[model_name]() \n",
        "\n",
        "        # Load pre-trained ResNetImagenNet\n",
        "        #logger.info(\"Load pre-trained ResNet ImagenNet weigths ...\")\n",
        "        #state_dict = load_state_dict_from_url('https://download.pytorch.org/models/resnet50-0676ba61.pth',progress=True)\n",
        "        #log = encoder.load_state_dict(state_dict, strict=False)\n",
        "        #logger.info(log)\n",
        "    \n",
        "        hidden_dim = 2048\n",
        "        if \"w2\" in model_name:\n",
        "            hidden_dim *= 2\n",
        "        elif \"w4\" in model_name:\n",
        "            hidden_dim *= 4\n",
        "\n",
        "    # -- projection head\n",
        "    encoder.fc = torch.nn.Sequential(\n",
        "        OrderedDict(\n",
        "            [\n",
        "                (\"fc1\", torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "                (\"bn1\", torch.nn.BatchNorm1d(hidden_dim)),\n",
        "                (\"relu1\", torch.nn.ReLU(inplace=True)),\n",
        "                (\"fc2\", torch.nn.Linear(hidden_dim, hidden_dim)),\n",
        "                (\"bn2\", torch.nn.BatchNorm1d(hidden_dim)),\n",
        "                (\"relu2\", torch.nn.ReLU(inplace=True)),\n",
        "                (\"fc3\", torch.nn.Linear(hidden_dim, output_dim)),\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # -- prediction head\n",
        "    encoder.pred = None\n",
        "    if use_pred:\n",
        "        mx = 4  # 4x bottleneck prediction head\n",
        "        pred_head = OrderedDict([])\n",
        "        pred_head[\"bn1\"] = torch.nn.BatchNorm1d(output_dim)\n",
        "        pred_head[\"fc1\"] = torch.nn.Linear(output_dim, output_dim // mx)\n",
        "        pred_head[\"bn2\"] = torch.nn.BatchNorm1d(output_dim // mx)\n",
        "        pred_head[\"relu\"] = torch.nn.ReLU(inplace=True)\n",
        "        pred_head[\"fc2\"] = torch.nn.Linear(output_dim // mx, output_dim)\n",
        "        encoder.pred = torch.nn.Sequential(pred_head)\n",
        "\n",
        "    encoder.to(device)\n",
        "    logger.info(encoder)\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def init_opt(\n",
        "    encoder,\n",
        "    iterations_per_epoch,\n",
        "    start_lr,\n",
        "    ref_lr,\n",
        "    ref_mom,\n",
        "    nesterov,\n",
        "    warmup,\n",
        "    num_epochs,\n",
        "    weight_decay=1e-6,\n",
        "    final_lr=0.0,\n",
        "):\n",
        "    param_groups = [\n",
        "        {\n",
        "            \"params\": (\n",
        "                p\n",
        "                for n, p in encoder.named_parameters()\n",
        "                if (\"bias\" not in n) and (\"bn\" not in n)\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"params\": (\n",
        "                p for n, p in encoder.named_parameters() if (\"bias\" in n) or (\"bn\" in n)\n",
        "            ),\n",
        "            \"LARS_exclude\": True,\n",
        "            \"weight_decay\": 0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = SGD(\n",
        "        param_groups,\n",
        "        weight_decay=weight_decay,\n",
        "        momentum=0.9,\n",
        "        nesterov=nesterov,\n",
        "        lr=ref_lr,\n",
        "    )\n",
        "    scheduler = WarmupCosineSchedule(\n",
        "        optimizer,\n",
        "        warmup_steps=warmup * iterations_per_epoch,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=ref_lr,\n",
        "        final_lr=final_lr,\n",
        "        T_max=num_epochs * iterations_per_epoch,\n",
        "    )\n",
        "    optimizer = LARS(optimizer, trust_coefficient=0.001)\n",
        "    return encoder, optimizer, scheduler\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef load_checkpoint(r_path, encoder, opt, scaler, use_fp16=False):\\n    checkpoint = torch.load(r_path, map_location=\"cpu\")\\n    epoch = checkpoint[\"epoch\"]\\n\\n    # -- loading encoder\\n    encoder.load_state_dict(checkpoint[\"encoder\"])\\n    logger.info(f\"loaded encoder from epoch {epoch}\")\\n\\n    # -- loading optimizer\\n    opt.load_state_dict(checkpoint[\"opt\"])\\n    if use_fp16:\\n        scaler.load_state_dict(checkpoint[\"amp\"])\\n    logger.info(f\"loaded optimizers from epoch {epoch}\")\\n    logger.info(f\"read-path: {r_path}\")\\n    del checkpoint\\n    return encoder, opt, epoch\\n\\n\\ndef init_model(device, model_name=\"resnet50\", use_pred=False, output_dim=128):\\n    if \"wide_resnet\" in model_name:\\n        encoder = wide_resnet.__dict__[model_name](dropout_rate=0.0)\\n        hidden_dim = 128\\n    else:\\n        encoder = resnet.__dict__[model_name]() \\n\\n        # Load pre-trained ResNetImagenNet\\n        #logger.info(\"Load pre-trained ResNet ImagenNet weigths ...\")\\n        #state_dict = load_state_dict_from_url(\\'https://download.pytorch.org/models/resnet50-0676ba61.pth\\',progress=True)\\n        #log = encoder.load_state_dict(state_dict, strict=False)\\n        #logger.info(log)\\n    \\n        hidden_dim = 2048\\n        if \"w2\" in model_name:\\n            hidden_dim *= 2\\n        elif \"w4\" in model_name:\\n            hidden_dim *= 4\\n\\n    # -- projection head\\n    encoder.fc = torch.nn.Sequential(\\n        OrderedDict(\\n            [\\n                (\"fc1\", torch.nn.Linear(hidden_dim, hidden_dim)),\\n                (\"bn1\", torch.nn.BatchNorm1d(hidden_dim)),\\n                (\"relu1\", torch.nn.ReLU(inplace=True)),\\n                (\"fc2\", torch.nn.Linear(hidden_dim, hidden_dim)),\\n                (\"bn2\", torch.nn.BatchNorm1d(hidden_dim)),\\n                (\"relu2\", torch.nn.ReLU(inplace=True)),\\n                (\"fc3\", torch.nn.Linear(hidden_dim, output_dim)),\\n            ]\\n        )\\n    )\\n\\n    # -- prediction head\\n    encoder.pred = None\\n    if use_pred:\\n        mx = 4  # 4x bottleneck prediction head\\n        pred_head = OrderedDict([])\\n        pred_head[\"bn1\"] = torch.nn.BatchNorm1d(output_dim)\\n        pred_head[\"fc1\"] = torch.nn.Linear(output_dim, output_dim // mx)\\n        pred_head[\"bn2\"] = torch.nn.BatchNorm1d(output_dim // mx)\\n        pred_head[\"relu\"] = torch.nn.ReLU(inplace=True)\\n        pred_head[\"fc2\"] = torch.nn.Linear(output_dim // mx, output_dim)\\n        encoder.pred = torch.nn.Sequential(pred_head)\\n\\n    encoder.to(device)\\n    logger.info(encoder)\\n    return encoder\\n\\n\\ndef init_opt(\\n    encoder,\\n    iterations_per_epoch,\\n    start_lr,\\n    ref_lr,\\n    ref_mom,\\n    nesterov,\\n    warmup,\\n    num_epochs,\\n    weight_decay=1e-6,\\n    final_lr=0.0,\\n):\\n    param_groups = [\\n        {\\n            \"params\": (\\n                p\\n                for n, p in encoder.named_parameters()\\n                if (\"bias\" not in n) and (\"bn\" not in n)\\n            )\\n        },\\n        {\\n            \"params\": (\\n                p for n, p in encoder.named_parameters() if (\"bias\" in n) or (\"bn\" in n)\\n            ),\\n            \"LARS_exclude\": True,\\n            \"weight_decay\": 0,\\n        },\\n    ]\\n    optimizer = SGD(\\n        param_groups,\\n        weight_decay=weight_decay,\\n        momentum=0.9,\\n        nesterov=nesterov,\\n        lr=ref_lr,\\n    )\\n    scheduler = WarmupCosineSchedule(\\n        optimizer,\\n        warmup_steps=warmup * iterations_per_epoch,\\n        start_lr=start_lr,\\n        ref_lr=ref_lr,\\n        final_lr=final_lr,\\n        T_max=num_epochs * iterations_per_epoch,\\n    )\\n    optimizer = LARS(optimizer, trust_coefficient=0.001)\\n    return encoder, optimizer, scheduler\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX1hxqUQn47M"
      },
      "source": [
        "## PyTorch/XLA ResNet18/CIFAR10 (GPU or TPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJSWXG7FKF5n"
      },
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import gdown\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from src.utils import (\n",
        "    init_distributed,\n",
        "    WarmupCosineSchedule,\n",
        "    CSVLogger,\n",
        "    AverageMeter,\n",
        ")\n",
        "from src.utils import (\n",
        "    AllGather,\n",
        "    AllReduce\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyHKXfPULVP1"
      },
      "source": [
        "def download(data, url):\n",
        "    # Download dataset\n",
        "    import zipfile\n",
        "    url = url\n",
        "    output = \"{}.zip\".format(data)\n",
        "    gdown.download(url, output, quiet=False)\n",
        "\n",
        "    # Uncompress dataset\n",
        "    local_zip = '{}.zip'.format(data)\n",
        "    zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "    zip_ref.extractall()\n",
        "    zip_ref.close()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDhpxrcKLX7W"
      },
      "source": [
        "data_samples = {\n",
        "    \"sample@200\": \"https://drive.google.com/uc?id=1FfV7YyDJvNUCDP5r3-8iQfZ2-xJp_pgb\",\n",
        "    \"sample@500\": \"https://drive.google.com/uc?id=1dHwUqpmSogEdjAB9rwDUL-OKFRUcVXte\",\n",
        "    \"sample@1000\": \"https://drive.google.com/uc?id=1DPZrHrj3Bdte5Dc6NCZ33CAqMG-Oipa2\",\n",
        "    \"sample@2000\": \"https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\",\n",
        "    \"sample@3000\": \"https://drive.google.com/uc?id=1_yre5K9YYvJgSrT4xvrI8eD_htucIywA\",\n",
        "    \"sample@4000_images\": \"https://drive.google.com/uc?id=1dqVB8EozEpwWzyuU80AauoQmsiw3Gtm2\",\n",
        "    \"sample@20000\": \"https://drive.google.com/uc?id=1MTDpLzpmhSiZq2jSdmHx2UDPn9FC8gzO\",\n",
        "    \"val-voets-tf\": \"https://drive.google.com/uc?id=1VzVgMGTkBBPG2qbzLunD9HvLzH6tcyrv\",\n",
        "    \"train_voets\": \"https://drive.google.com/uc?id=1AmcFh1MOOZ6aqKm2eO7XEdgmIEqHKTZ5\",\n",
        "    \"voets_test_images\": \"https://drive.google.com/uc?id=15S_V3B_Z3BOjCT3AbO2c887FyS5B0Lyd\"\n",
        "}"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1J6gqYLZC7"
      },
      "source": [
        "UNLABELED = 'sample@2000'"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy-BZSTOLaFm",
        "outputId": "c704f420-2330-42ef-a4e3-aadac142d912"
      },
      "source": [
        "URL_UNLABELED = data_samples[UNLABELED]\n",
        "download(UNLABELED, URL_UNLABELED)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PB7uGd-dUnZKnKZpZl-HvE1DVcWgX50F\n",
            "To: /content/suncet/sample@2000.zip\n",
            "214MB [00:03, 71.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IednejwkIW-K"
      },
      "source": [
        "Only run the below commented cell if you would like a nightly release"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFzLg5gy7l6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3a0eabf3-b0d2-45df-b373-aa74e39cb020"
      },
      "source": [
        "\"\"\"\n",
        "# PyTorch/XLA GPU Setup (only if GPU runtime)\n",
        "if os.environ.get('COLAB_GPU', '0') == '1':\n",
        "  os.environ['GPU_NUM_DEVICES'] = '1'\n",
        "  os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# PyTorch/XLA GPU Setup (only if GPU runtime)\\nif os.environ.get('COLAB_GPU', '0') == '1':\\n  os.environ['GPU_NUM_DEVICES'] = '1'\\n  os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/local/cuda/'\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rroH9yiAn-XE"
      },
      "source": [
        "### Define Parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-JwDHNgcNoY7",
        "outputId": "d7b44fb9-8770-4eea-ca5e-f1ca15349227"
      },
      "source": [
        "UNLABELED"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sample@2000'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umuD5GhXawwo"
      },
      "source": [
        "multicrop=multicrop\n",
        "tau=temperature\n",
        "T=sharpen\n",
        "me_max=reg\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Make semi-supervised PAWS loss\n",
        "\n",
        ":param multicrop: number of small multi-crop views\n",
        ":param tau: cosine similarity temperature\n",
        ":param T: target sharpenning temperature\n",
        ":param me_max: whether to perform me-max regularization\n",
        "\"\"\"\n",
        "\n",
        "def sharpen_func(p):\n",
        "    sharp_p = p**(1./T)\n",
        "    sharp_p /= torch.sum(sharp_p, dim=1, keepdim=True)\n",
        "    return sharp_p\n",
        "\n",
        "def snn(query, supports, labels, tau):\n",
        "    #device = xm.xla_device() # YOOOOO\n",
        "\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "    \"\"\" Soft Nearest Neighbours similarity classifier \"\"\"\n",
        "    # Step 1: normalize embeddings\n",
        "    query = torch.nn.functional.normalize(query)\n",
        "\n",
        "    #query = query.to(device) # YOOOOO\n",
        "\n",
        "    #print(\"Query is:::\")\n",
        "    #print(query)\n",
        "\n",
        "    supports = torch.nn.functional.normalize(supports)\n",
        "\n",
        "    #labels = labels.to(device) # YOOOOO\n",
        "    #supports = supports.to(device) # YOOOOO\n",
        "\n",
        "    #print(\"supports is:::::\")\n",
        "    #print(supports)\n",
        "    # Step 2: gather embeddings from all workers\n",
        "    supports = AllGather.apply(supports)\n",
        "\n",
        "    # Step 3: compute similarlity between local embeddings\n",
        "    #return softmax(query @ supports.T / tau) @ labels\n",
        "    #return softmax(query @ supports.T / tau) @ labels\n",
        "    ans = softmax(torch.matmul(query, supports.T / tau))\n",
        "\n",
        "    print(\"ans shape is {}\".format(ans.shape))\n",
        "    #print(ans)\n",
        "    print(\"labels shape is {}\".format(labels.shape))\n",
        "    #print(labels)\n",
        "\n",
        "    #result = softmax(query @ supports.T / tau) @ labels\n",
        "    result = torch.matmul(softmax(torch.matmul(query, supports.T / tau)), labels)\n",
        "    #result = ans\n",
        "    #print(\"result is:\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def my_loss_func(\n",
        "    anchor_views,\n",
        "    anchor_supports,\n",
        "    anchor_support_labels,\n",
        "    target_views,\n",
        "    target_supports,\n",
        "    target_support_labels,\n",
        "    sharpen=sharpen_func,\n",
        "    snn=snn\n",
        "):\n",
        "    # -- NOTE: num views of each unlabeled instance = 2+multicrop\n",
        "    batch_size = len(anchor_views) // (2+multicrop)\n",
        "\n",
        "    # Step 1: compute anchor predictions\n",
        "    probs = snn(anchor_views, anchor_supports, anchor_support_labels, tau)\n",
        "\n",
        "    # Step 2: compute targets for anchor predictions\n",
        "    with torch.no_grad():\n",
        "        targets = snn(target_views, target_supports, target_support_labels, tau)\n",
        "        targets = sharpen(targets)\n",
        "        if multicrop > 0:\n",
        "            mc_target = 0.5*(targets[:batch_size]+targets[batch_size:])\n",
        "            targets = torch.cat([targets, *[mc_target for _ in range(multicrop)]], dim=0)\n",
        "        targets[targets < 1e-4] *= 0  # numerical stability\n",
        "\n",
        "    # Step 3: compute cross-entropy loss H(targets, queries)\n",
        "    #loss = torch.mean(torch.sum(torch.log(probs**(-targets)), dim=1))\n",
        "\n",
        "    print(\"probs shape is {}\".format(probs.shape))\n",
        "    print(\"targets shape is {}\".format(targets.shape))\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    targets2 = targets.argmax(-1)\n",
        "    loss = criterion(probs, targets2)\n",
        "\n",
        "    # Step 4: compute me-max regularizer\n",
        "    rloss = 0.\n",
        "    if me_max:\n",
        "        avg_probs = AllReduce.apply(torch.mean(sharpen(probs), dim=0))\n",
        "        rloss -= torch.sum(torch.log(avg_probs**(-avg_probs)))\n",
        "\n",
        "    #return loss, rloss\n",
        "    return loss, rloss"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQlRwv7TzBN"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX90auWuTnDP"
      },
      "source": [
        "##########################################################\n",
        "##########################################################\n",
        "################# DEBUG DATA LOADERS #####################\n",
        "##########################################################\n",
        "##########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8zko5ZuTqyr"
      },
      "source": [
        "##########################################################\n",
        "##########################################################\n",
        "##########################################################\n",
        "##########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG4xTLJxT0WA"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vMl96KLoCq8"
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "# Only instantiate model weights once in memory.\n",
        "#WRAPPED_MODEL = xmp.MpModelWrapper(ResNet18())\n",
        "\n",
        "def train_resnet18():\n",
        "  torch.manual_seed(1)\n",
        "\n",
        "  ############# PAWS CODE ##################\n",
        "\n",
        "  device = xm.xla_device()\n",
        "\n",
        "  # -- init model\n",
        "  encoder = init_model(\n",
        "    device=device,\n",
        "    model_name=model_name,\n",
        "    use_pred=use_pred_head,\n",
        "    output_dim=output_dim,\n",
        "  )\n",
        "\n",
        "  # -- init losses\n",
        "  #paws = init_paws_loss(multicrop=multicrop, tau=temperature, T=sharpen, me_max=reg)\n",
        "  # -- assume support images are sampled with ClassStratifiedSampler\n",
        "  \"\"\"\n",
        "  labels_matrix = make_labels_matrix(\n",
        "    num_classes=classes_per_batch,\n",
        "    s_batch_size=s_batch_size,\n",
        "    world_size=xm.xrt_world_size(),\n",
        "    device=device,\n",
        "    unique_classes=unique_classes,\n",
        "    smoothing=label_smoothing,\n",
        "  )\n",
        "  \"\"\"\n",
        "  # TO DO: Hacer el smooth de los labels Matrix\n",
        "\n",
        "  print(\"normalize {}\".format(normalize))\n",
        "\n",
        "# -- make data transforms\n",
        "  transform, init_transform = make_transforms(\n",
        "    dataset_name=dataset_name,\n",
        "    subset_path=subset_path,\n",
        "    unlabeled_frac=unlabeled_frac,\n",
        "    training=True,\n",
        "    split_seed=data_seed,\n",
        "    crop_scale=crop_scale,\n",
        "    basic_augmentations=False,\n",
        "    color_jitter=color_jitter,\n",
        "    normalize=normalize,\n",
        "  )\n",
        "  multicrop_transform = (multicrop, None)\n",
        "  if multicrop > 0:\n",
        "    multicrop_transform = make_multicrop_transform(\n",
        "        dataset_name=dataset_name,\n",
        "        num_crops=multicrop,\n",
        "        size=mc_size,\n",
        "        crop_scale=mc_scale,\n",
        "        normalize=normalize,\n",
        "        color_distortion=color_jitter,\n",
        "    )\n",
        "\n",
        "# -- init data-loaders/samplers\n",
        "  (\n",
        "    unsupervised_loader,\n",
        "    unsupervised_sampler,\n",
        "    supervised_loader,\n",
        "    supervised_sampler,\n",
        ") = init_data(\n",
        "    dataset_name=dataset_name,\n",
        "    transform=transform,\n",
        "    init_transform=init_transform,\n",
        "    supervised_views=supervised_views,\n",
        "    u_batch_size=u_batch_size,\n",
        "    s_batch_size=s_batch_size,\n",
        "    unique_classes=unique_classes,\n",
        "    classes_per_batch=classes_per_batch,\n",
        "    multicrop_transform=multicrop_transform,\n",
        "    world_size=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    root_path=root_path,\n",
        "    s_image_folder=s_image_folder,\n",
        "    u_image_folder=u_image_folder,\n",
        "    training=True,\n",
        "    copy_data=copy_data,\n",
        "  )\n",
        "  #iter_supervised = None\n",
        "  ipe = len(unsupervised_loader)\n",
        "\n",
        "  print(\"len unsuupervised dataloader IS: {}\".format(ipe))\n",
        "\n",
        "\n",
        "  logger.info(f\"iterations per epoch: {ipe}\")\n",
        "\n",
        "    # -- init optimizer and scheduler\n",
        "  scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n",
        "\n",
        "  \"\"\"\n",
        "  encoder, optimizer, scheduler = init_opt(\n",
        "        encoder=encoder,\n",
        "        weight_decay=wd,\n",
        "        start_lr=start_lr,\n",
        "        ref_lr=lr,\n",
        "        final_lr=final_lr,\n",
        "        ref_mom=mom,\n",
        "        nesterov=nesterov,\n",
        "        iterations_per_epoch=ipe,\n",
        "        warmup=warmup,\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "  \"\"\"\n",
        "  \n",
        "  # YOOOO\n",
        "  optimizer = torch.optim.Adam(encoder.parameters(), 0.0001, weight_decay=5e-4)\n",
        "  # YOOOO\n",
        "  \n",
        "    #if xm.xrt_world_size() > 1:\n",
        "    #    encoder = DistributedDataParallel(encoder, broadcast_buffers=False)\n",
        "\n",
        "  def train_loop_fn(supervised_loader, unsupervised_loader, epoch):\n",
        "      print(\"Entre a `train_loop_fn`\")\n",
        "\n",
        "      # -- TRAINING LOOP\n",
        "      best_loss = None\n",
        "\n",
        "      logger.info(\"Epoch %d\" % (epoch + 1))\n",
        "\n",
        "      \"\"\"\n",
        "      TO DO: Hacer un Double CHECK EN este Sampler!!!\n",
        "      \"\"\"\n",
        "      # -- update distributed-data-loader epoch\n",
        "      unsupervised_sampler.set_epoch(epoch)\n",
        "      #if supervised_sampler is not None:\n",
        "      #    supervised_sampler.set_epoch(epoch)\n",
        "\n",
        "      loss_meter = AverageMeter()\n",
        "      ploss_meter = AverageMeter()\n",
        "      rloss_meter = AverageMeter()\n",
        "      time_meter = AverageMeter()\n",
        "      data_meter = AverageMeter()\n",
        "\n",
        "      print(\"len supervised loader dentro de 'train_loop_fn` is {}\".format(len(supervised_loader)))\n",
        "      print(\"len unsupervised_loader loader dentro de 'train_loop_fn` is {}\".format(len(unsupervised_loader)))\n",
        "\n",
        "      for itr, udata in enumerate(unsupervised_loader):\n",
        "          print(\"Entre a `for itr, udata`\")\n",
        "          def load_imgs(supervised_loader):\n",
        "              # -- unsupervised imgs\n",
        "              uimgs = [u.to(device, non_blocking=True) for u in udata[:-1]]\n",
        "              # -- supervised imgs\n",
        "              global iter_supervised\n",
        "              print(\"len supervised loader is {}\".format(len(supervised_loader)))\n",
        "              try:\n",
        "                  print(\"Entro al Try\")\n",
        "                  sdata = next(iter_supervised)\n",
        "              except Exception:\n",
        "                  print(\"Entro al Except!!\")\n",
        "                  iter_supervised = iter(supervised_loader)\n",
        "                  logger.info(f\"len.supervised_loader: {len(iter_supervised)}\")\n",
        "                  sdata = next(iter_supervised)\n",
        "              finally:\n",
        "                  idx = sdata[1].clone().detach()\n",
        "                  idx = idx.to(device)\n",
        "\n",
        "                  labels_matrix = torch.zeros(len(idx), idx.max()+1, device = device).scatter_(1, idx.unsqueeze(1), 1.)\n",
        "  \n",
        "                  labels_matrix = labels_matrix.to(device)\n",
        "                  labels = torch.cat([labels_matrix for _ in range(supervised_views)])\n",
        "                  simgs = [s.to(device, non_blocking=True) for s in sdata[:-1]]\n",
        "              # -- concatenate supervised imgs and unsupervised imgs\n",
        "              imgs = simgs + uimgs\n",
        "              return imgs, labels\n",
        "          \n",
        "          imgs, labels = load_imgs(supervised_loader)\n",
        "          \n",
        "          def train_step():\n",
        "              print(\"Entre a train_step()\")\n",
        "\n",
        "              #with torch.cuda.amp.autocast(enabled=use_fp16):\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              # --\n",
        "              # h: representations of 'imgs' before head\n",
        "              # z: representations of 'imgs' after head\n",
        "              # -- If use_pred_head=False, then encoder.pred (prediction\n",
        "              #    head) is None, and _forward_head just returns the\n",
        "              #    identity, z=h\n",
        "              h, z = encoder(imgs, return_before_head=True)\n",
        "\n",
        "              # Compute paws loss in full precision\n",
        "              #with torch.cuda.amp.autocast(enabled=False):\n",
        "\n",
        "              # Step 1. convert representations to fp32\n",
        "              h, z = h.float(), z.float()\n",
        "\n",
        "              # Step 2. determine anchor views/supports and their\n",
        "              #         corresponding target views/supports\n",
        "              # --\n",
        "              num_support = (\n",
        "                  supervised_views * s_batch_size * classes_per_batch\n",
        "              )\n",
        "\n",
        "              print(\"num_support is {}\".format(num_support))\n",
        "\n",
        "              # --\n",
        "              anchor_supports = z[:num_support]\n",
        "              anchor_views = z[num_support:]\n",
        "              # --\n",
        "              target_supports = h[:num_support].detach()\n",
        "              target_views = h[num_support:].detach()\n",
        "              target_views = torch.cat(\n",
        "                  [\n",
        "                      target_views[u_batch_size : 2 * u_batch_size],\n",
        "                      target_views[:u_batch_size],\n",
        "                  ],\n",
        "                  dim=0,\n",
        "              )\n",
        "\n",
        "              # Step 3. compute paws loss with me-max regularization\n",
        "              print(\"# Step 3. compute paws loss with me-max regularization\")\n",
        "\n",
        "              #ploss, me_max ,lr_stats= 0,0,0 # FAKE LOSS \n",
        "\n",
        "              print(\"`anchor_views` shape is {}\".format(anchor_views.shape))\n",
        "              print(\"`anchor_supports` shape is {}\".format(anchor_supports.shape))\n",
        "              print(\"`labels shape` is {}\".format(labels.shape))\n",
        "              print(\"`target_views` shape is {}\".format(target_views.shape))\n",
        "              print(\"`target_supports` shape is {}\".format(target_supports.shape))\n",
        "\n",
        "              ploss, me_max = my_loss_func(\n",
        "                  anchor_views=anchor_views,\n",
        "                  anchor_supports=anchor_supports,\n",
        "                  anchor_support_labels=labels,\n",
        "                  target_views=target_views,\n",
        "                  target_supports=target_supports,\n",
        "                  target_support_labels=labels,\n",
        "              )\n",
        "\n",
        "              print(\"Termine Calculo de PAWS loss\")\n",
        "\n",
        "              loss = ploss + me_max\n",
        "\n",
        "              print(\"loss is {}\".format(loss))\n",
        "\n",
        "              print(\"Doing loss backward\")\n",
        "              loss.backward()\n",
        "              print(\"Optimizing step optimizer\")\n",
        "              xm.optimizer_step(optimizer)\n",
        "              print(\"Finish Optimizer Step\")\n",
        " \n",
        "              return (float(loss), float(ploss), float(me_max))\n",
        "\n",
        "          (loss, ploss, rloss) = train_step()\n",
        "          #loss_meter.update(loss)\n",
        "          #ploss_meter.update(ploss)\n",
        "          #rloss_meter.update(rloss)\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  data, pred, target = None, None, None\n",
        "\n",
        "  start_epoch = 0\n",
        "  end_epoch = FLAGS.get(\"num_epochs\")\n",
        "\n",
        "  train_supervised_loader = pl.MpDeviceLoader(supervised_loader, device)\n",
        "  train_unsupervised_loader = pl.MpDeviceLoader(unsupervised_loader, device)\n",
        "\n",
        "  for epoch in range(start_epoch, end_epoch):\n",
        "      train_loop_fn(train_supervised_loader, train_unsupervised_loader, epoch)\n",
        "      xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "  #return accuracy, data, pred, target\n",
        "  return 0, 0, 0, 0"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wF3c4xmKQofR",
        "outputId": "a208b61e-3b73-465b-8860-ccc39f63bf21"
      },
      "source": [
        "\"\"\"\n",
        "To Do: \n",
        "  - Dont Donwload several Times ResNet model. How?. \n",
        "\"\"\""
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTo Do: \\n  - Dont Donwload several Times ResNet model. How?. \\n'"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2nL4HmloEyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936cdea5-5ec6-4d52-879a-906f1d0d90e8"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  accuracy, data, pred, target = train_resnet18()\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'], start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc1648f1d0>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc17c901d0>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc17c96190>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a train_step()\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc17c97190>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a train_step()\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc17c96190>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "Entre a train_step()\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc17c901d0>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "Entre a train_step()\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc17c991d0>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "normalize True\n",
            "self.multicrop_transform (6, Compose(\n",
            "    Compose(\n",
            "    RandomResizedCrop(size=(96, 96), scale=(0.05, 0.14), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    Compose(\n",
            "    RandomApply(\n",
            "    p=0.8\n",
            "    ColorJitter(brightness=[0.19999999999999996, 1.8], contrast=[0.19999999999999996, 1.8], saturation=[0.19999999999999996, 1.8], hue=[-0.2, 0.2])\n",
            ")\n",
            "    RandomGrayscale(p=0.2)\n",
            ")\n",
            "    <__main__.GaussianBlur object at 0x7fcc17c911d0>\n",
            "    ToTensor()\n",
            ")\n",
            "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
            "))\n",
            "self.multicrop_transform (0, None)\n",
            "len unsuupervised dataloader IS: 7\n",
            "Entre a `train_loop_fn`\n",
            "len supervised loader dentro de 'train_loop_fn` is 1\n",
            "len unsupervised_loader loader dentro de 'train_loop_fn` is 7\n",
            "loss is 0.2942256033420563\n",
            "loss is 0.04340720176696777\n",
            "loss is 0.0004494786262512207\n",
            "Doing loss backward\n",
            "Doing loss backward\n",
            "Doing loss backward\n",
            "loss is 0.0034177303314208984\n",
            "Doing loss backward\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Entre a train_step()\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "Entre a train_step()\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "loss is 0.1967729926109314\n",
            "Doing loss backward\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "loss is -0.012301385402679443\n",
            "Doing loss backward\n",
            "Optimizing step optimizer\n",
            "Optimizing step optimizer\n",
            "Optimizing step optimizer\n",
            "Optimizing step optimizer\n",
            "Optimizing step optimizer\n",
            "Entre a `for itr, udata`\n",
            "len supervised loader is 1\n",
            "Entro al Try\n",
            "Entro al Except!!\n",
            "Finish Optimizer Step\n",
            "Finish Optimizer Step\n",
            "Finish Optimizer Step\n",
            "Finish Optimizer Step\n",
            "Finish Optimizer Step\n",
            "Optimizing step optimizer\n",
            "Entre a train_step()\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "Finish Optimizer Step\n",
            "Entre a train_step()\n",
            "num_support is 16\n",
            "# Step 3. compute paws loss with me-max regularization\n",
            "`anchor_views` shape is torch.Size([256, 2048])\n",
            "`anchor_supports` shape is torch.Size([16, 2048])\n",
            "`labels shape` is torch.Size([16, 2])\n",
            "`target_views` shape is torch.Size([64, 2048])\n",
            "`target_supports` shape is torch.Size([16, 2048])\n",
            "ans shape is torch.Size([256, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "ans shape is torch.Size([64, 16])\n",
            "labels shape is torch.Size([16, 2])\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "loss is 0.0009788274765014648\n",
            "Doing loss backward\n",
            "probs shape is torch.Size([256, 2])\n",
            "targets shape is torch.Size([256, 2])\n",
            "Termine Calculo de PAWS loss\n",
            "Optimizing step optimizer\n",
            "loss is 0.022017180919647217\n",
            "Doing loss backward\n",
            "Finish Optimizer Step\n",
            "Optimizing step optimizer\n",
            "Finish Optimizer Step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2REoWOpRuFt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}